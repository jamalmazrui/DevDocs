

Contents
About	1
Chapter 1: Getting started with R Language	2
Section 1.1: Installing R	2
Section 1.2: Hello World!	3
Section 1.3: Getting Help	3
Section 1.4: Interactive mode and R scripts	3
Chapter 2: Variables	7
Section 2.1: Variables, data structures and basic Operations	7
Chapter 3: Arithmetic Operators	10
Section 3.1: Range and addition	10
Section 3.2: Addition and subtraction	10
Chapter 4: Matrices	13
Section 4.1: Creating matrices	13
Chapter 5: Formula	15
Section 5.1: The basics of formula	15
Chapter 6: Reading and writing strings	17
Section 6.1: Printing and displaying strings	17
Section 6.2: Capture output of operating system command	18
Section 6.3: Reading from or writing to a file connection	19
Chapter 7: String manipulation with stringi package	21
Section 7.1: Count pattern inside string	21
Section 7.2: Duplicating strings	21
Section 7.3: Paste vectors	22
Section 7.4: Splitting text by some fixed pattern	22
Chapter 8: Classes	23
Section 8.1: Inspect classes	23
Section 8.2: Vectors and lists	23
Section 8.3: Vectors	24
Chapter 9: Lists	25
Section 9.1: Introduction to lists	25
Section 9.2: Quick Introduction to Lists	25
Section 9.3: Serialization: using lists to pass information	27
Chapter 10: Hashmaps	29
Section 10.1: Environments as hash maps	29
Section 10.2: package:hash	32
Section 10.3: package:listenv	33
Chapter 11: Creating vectors	35
Section 11.1: Vectors from build in constants: Sequences of letters & month names	35
Section 11.2: Creating named vectors	35
Section 11.3: Sequence of numbers	37
Section 11.4: seq()	37
Section 11.5: Vectors	38
Section 11.6: Expanding a vector with the rep() function	39
Chapter 12: Date and Time	41
Section 12.1: Current Date and Time	41
Section 12.2: Go to the End of the Month	41
Section 12.3: Go to First Day of the Month	42
Section 12.4: Move a date a number of months consistently by months	42
Chapter 13: The Date class	44
Section 13.1: Formatting Dates	44
Section 13.2: Parsing Strings into Date Objects	44
Section 13.3: Dates	45
Chapter 14: Date-time classes (POSIXct and POSIXlt)	47
Section 14.1: Formatting and printing date-time objects	47
Section 14.2: Date-time arithmetic	47
Section 14.3: Parsing strings into date-time objects	48
Chapter 15: The character class	50
Section 15.1: Coercion	50
Chapter 16: Numeric classes and storage modes	51
Section 16.1: Numeric	51
Chapter 17: The logical class	53
Section 17.1: Logical operators	53
Section 17.2: Coercion	53
Section 17.3: Interpretation of NAs	53
Chapter 18: Data frames	55
Section 18.1: Create an empty data.frame	55
Section 18.2: Subsetting rows and columns from a data frame	56
Section 18.3: Convenience functions to manipulate data.frames	59
Section 18.4: Introduction	60
Section 18.5: Convert all columns of a data.frame to character class	61
Chapter 19: Split function	63
Section 19.1: Using split in the split-apply-combine paradigm	63
Section 19.2: Basic usage of split	64
Chapter 20: Reading and writing tabular data in plain-text files (CSV, TSV, etc.)	67
Section 20.1: Importing .csv files	67
Section 20.2: Importing with data.table	68
Section 20.3: Exporting .csv files	69
Section 20.4: Import multiple csv files	69
Section 20.5: Importing fixed-width files	69
Chapter 21: Pipe operators (%>% and others)	71
Section 21.1: Basic use and chaining	71
Section 21.2: Functional sequences	72
Section 21.3: Assignment with %<>%	73
Section 21.4: Exposing contents with %$%	73
Section 21.5: Creating side eects with %T>%	74
Section 21.6: Using the pipe with dplyr and ggplot2	75
Chapter 22: Linear Models (Regression)	76
Section 22.1: Linear regression on the mtcars dataset	76
Section 22.2: Using the 'predict' function	78
Section 22.3: Weighting	79
Section 22.4: Checking for nonlinearity with polynomial regression	81
Section 22.5: Plotting The Regression (base)	83
Section 22.6: Quality assessment	85
Chapter 23: data.table	87
Section 23.1: Creating a data.table	87
Section 23.2: Special symbols in data.table	88
Section 23.3: Adding and modifying columns	89
Section 23.4: Writing code compatible with both data.frame and data.table	91
Section 23.5: Setting keys in data.table	93
Chapter 24: Pivot and unpivot with data.table	95
Section 24.1: Pivot and unpivot tabular data with data.table - I	95
Section 24.2: Pivot and unpivot tabular data with data.table - II	96
Chapter 25: Bar Chart	98
Section 25.1: barplot() function	98
Chapter 26: Base Plotting	104
Section 26.1: Density plot	104
Section 26.2: Combining Plots	105
Section 26.3: Getting Started with R_Plots	107
Section 26.4: Basic Plot	108
Section 26.5: Histograms	111
Section 26.6: Matplot	113
Section 26.7: Empirical Cumulative Distribution Function	119
Chapter 27: boxplot	121
Section 27.1: Create a box-and-whisker plot with boxplot() {graphics}	121
Section 27.2: Additional boxplot style parameters	125
Chapter 28: ggplot2	128
Section 28.1: Displaying multiple plots	128
Section 28.2: Prepare your data for plotting	131
Section 28.3: Add horizontal and vertical lines to plot	133
Section 28.4: Scatter Plots	136
Section 28.5: Produce basic plots with qplot	136
Section 28.6: Vertical and Horizontal Bar Chart	138
Section 28.7: Violin plot	140
Chapter 29: Factors	143
Section 29.1: Consolidating Factor Levels with a List	143
Section 29.2: Basic creation of factors	144
Section 29.3: Changing and reordering factors	145
Section 29.4: Rebuilding factors from zero	150
Chapter 30: Pattern Matching and Replacement	152
Section 30.1: Finding Matches	152
Section 30.2: Single and Global match	153
Section 30.3: Making substitutions	154
Section 30.4: Find matches in big data sets	154
Chapter 31: Run-length encoding	156
Section 31.1: Run-length Encoding with `rle`	156
Section 31.2: Identifying and grouping by runs in base R	156
Section 31.3: Run-length encoding to compress and decompress vectors	157
Section 31.4: Identifying and grouping by runs in data.table	158
Chapter 32: Speeding up tough-to-vectorize code	159
Section 32.1: Speeding tough-to-vectorize for loops with Rcpp	159
Section 32.2: Speeding tough-to-vectorize for loops by byte compiling	159
Chapter 33: Introduction to Geographical Maps	161
Section 33.1: Basic map-making with map() from the package maps	161
Section 33.2: 50 State Maps and Advanced Choropleths with Google Viz	164
Section 33.3: Interactive plotly maps	165
Section 33.4: Making Dynamic HTML Maps with Leaflet	167
Section 33.5: Dynamic Leaflet maps in Shiny applications	168
Chapter 34: Set operations	171
Section 34.1: Set operators for pairs of vectors	171
Section 34.2: Cartesian or "cross" products of vectors	171
Section 34.3: Set membership for vectors	172
Section 34.4: Make unique / drop duplicates / select distinct elements from a vector	172
Section 34.5: Measuring set overlaps / Venn diagrams for vectors	173
Chapter 35: tidyverse	174
Section 35.1: tidyverse: an overview	174
Section 35.2: Creating tbl_df's	175
Chapter 36: Rcpp	176
Section 36.1: Extending Rcpp with Plugins	176
Section 36.2: Inline Code Compile	176
Section 36.3: Rcpp Attributes	177
Section 36.4: Specifying Additional Build Dependencies	178
Chapter 37: Random Numbers Generator	179
Section 37.1: Random permutations	179
Section 37.2: Generating random numbers using various density functions	179
Section 37.3: Random number generator's reproducibility	181
Chapter 38: Parallel processing	182
Section 38.1: Parallel processing with parallel package	182
Section 38.2: Parallel processing with foreach package	183
Section 38.3: Random Number Generation	184
Section 38.4: mcparallelDo	184
Chapter 39: Subsetting	186
Section 39.1: Data frames	186
Section 39.2: Atomic vectors	187
Section 39.3: Matrices	188
Section 39.4: Lists	190
Section 39.5: Vector indexing	191
Section 39.6: Other objects	192
Section 39.7: Elementwise Matrix Operations	192
Chapter 40: Debugging	194
Section 40.1: Using debug	194
Section 40.2: Using browser	194
Chapter 41: Installing packages	196
Section 41.1: Install packages from GitHub	196
Section 41.2: Download and install packages from repositories	197
Section 41.3: Install package from local source	198
Section 41.4: Install local development version of a package	198
Section 41.5: Using a CLI package manager -- basic pacman usage	199
Chapter 42: Inspecting packages	200
Section 42.1: View Package Version	200
Section 42.2: View Loaded packages in Current Session	200
Section 42.3: View package information	200
Section 42.4: View package's built-in data sets	200
Section 42.5: List a package's exported functions	200
Chapter 43: Creating packages with devtools	201
Section 43.1: Creating and distributing packages	201
Section 43.2: Creating vignettes	203
Chapter 44: Using pipe assignment in your own package %<>%: How to ?	204
Section 44.1: Putting the pipe in a utility-functions file	204
Chapter 45: Arima Models	205
Section 45.1: Modeling an AR1 Process with Arima	205
Chapter 46: Distribution Functions	210
Section 46.1: Normal distribution	210
Section 46.2: Binomial Distribution	210
Chapter 47: Shiny	214
Section 47.1: Create an app	214
Section 47.2: Checkbox Group	214
Section 47.3: Radio Button	215
Section 47.4: Debugging	216
Section 47.5: Select box	216
Section 47.6: Launch a Shiny app	217
Section 47.7: Control widgets	218
Chapter 48: spatial analysis	220
Section 48.1: Create spatial points from XY data set	220
Section 48.2: Importing a shape file (.shp)	221
Chapter 49: sqldf	222
Section 49.1: Basic Usage Examples	222
Chapter 50: Code profiling	224
Section 50.1: Benchmarking using microbenchmark	224
Section 50.2: proc.time()	225
Section 50.3: Microbenchmark	226
Section 50.4: System.time	227
Section 50.5: Line Profiling	227
Chapter 51: Control flow structures	229
Section 51.1: Optimal Construction of a For Loop	229
Section 51.2: Basic For Loop Construction	230
Section 51.3: The Other Looping Constructs: while and repeat	230
Chapter 52: Column wise operation	234
Section 52.1: sum of each column	234
Chapter 53: JSON	236
Section 53.1: JSON to / from R objects	236
Chapter 54: RODBC	238
Section 54.1: Connecting to Excel Files via RODBC	238
Section 54.2: SQL Server Management Database connection to get individual table	238
Section 54.3: Connecting to relational databases	238
Chapter 55: lubridate	239
Section 55.1: Parsing dates and datetimes from strings with lubridate	239
Section 55.2: Dierence between period and duration	240
Section 55.3: Instants	240
Section 55.4: Intervals, Durations and Periods	241
Section 55.5: Manipulating date and time in lubridate	242
Section 55.6: Time Zones	243
Section 55.7: Parsing date and time in lubridate	243
Section 55.8: Rounding dates	243
Chapter 56: Time Series and Forecasting	245
Section 56.1: Creating a ts object	245
Section 56.2: Exploratory Data Analysis with time-series data	245
Chapter 57: strsplit function	247
Section 57.1: Introduction	247
Chapter 58: Web scraping and parsing	248
Section 58.1: Basic scraping with rvest	248
Section 58.2: Using rvest when login is required	248
Chapter 59: Generalized linear models	250
Section 59.1: Logistic regression on Titanic dataset	250
Chapter 60: Reshaping data between long and wide forms	253
Section 60.1: Reshaping data	253
Section 60.2: The reshape function	254
Chapter 61: RMarkdown and knitr presentation	256
Section 61.1: Adding a footer to an ioslides presentation	256
Section 61.2: Rstudio example	257
Chapter 62: Scope of variables	259
Section 62.1: Environments and Functions	259
Section 62.2: Function Exit	259
Section 62.3: Sub functions	260
Section 62.4: Global Assignment	260
Section 62.5: Explicit Assignment of Environments and Variables	261
Chapter 63: Performing a Permutation Test	262
Section 63.1: A fairly general function	262
Chapter 64: xgboost	265
Section 64.1: Cross Validation and Tuning with xgboost	265
Chapter 65: R code vectorization best practices	267
Section 65.1: By row operations	267
Chapter 66: Missing values	270
Section 66.1: Examining missing data	270
Section 66.2: Reading and writing data with NA values	270
Section 66.3: Using NAs of dierent classes	270
Section 66.4: TRUE/FALSE and/or NA	271
Chapter 67: Hierarchical Linear Modeling	272
Section 67.1: basic model fitting	272
Chapter 68: *apply family of functions (functionals)	273
Section 68.1: Using built-in functionals	273
Section 68.2: Combining multiple `data.frames` (`lapply`, `mapply`)	273
Section 68.3: Bulk File Loading	275
Section 68.4: Using user-defined functionals	275
Chapter 69: Text mining	277
Section 69.1: Scraping Data to build N-gram Word Clouds	277
Chapter 70: ANOVA	281
Section 70.1: Basic usage of aov()	281
Section 70.2: Basic usage of Anova()	281
Chapter 71: Raster and Image Analysis	283
Section 71.1: Calculating GLCM Texture	283
Section 71.2: Mathematical Morphologies	285
Chapter 72: Survival analysis	287
Section 72.1: Random Forest Survival Analysis with randomForestSRC	287
Section 72.2: Introduction - basic fitting and plotting of parametric survival models with the survivalpackage	288
Section 72.3: Kaplan Meier estimates of survival curves and risk set tables with survminer	289
Chapter 73: Fault-tolerant/resilient code	292
Section 73.1: Using tryCatch()	292
Chapter 74: Reproducible R	295
Section 74.1: Data reproducibility	295
Section 74.2: Package reproducibility	295
Chapter 75: Fourier Series and Transformations	296
Section 75.1: Fourier Series	297
Chapter 76: .Rprofile	302
Section 76.1: .Rprofile - the first chunk of code executed	302
Section 76.2: .Rprofile example	303
Chapter 77: dplyr	304
Section 77.1: dplyr's single table verbs	304
Section 77.2: Aggregating with %>% (pipe) operator	311
Section 77.3: Subset Observation (Rows)	312
Section 77.4: Examples of NSE and string variables in dpylr	313
Chapter 78: caret	314
Section 78.1: Preprocessing	314
Chapter 79: Extracting and Listing Files in Compressed Archives	315
Section 79.1: Extracting files from a .zip archive	315
Chapter 80: Probability Distributions with R	316
Section 80.1: PDF and PMF for dierent distributions in R	316
Chapter 81: R in LaTeX with knitr	317
Section 81.1: R in LaTeX with Knitr and Code Externalization	317
Section 81.2: R in LaTeX with Knitr and Inline Code Chunks	317
Section 81.3: R in LaTex with Knitr and Internal Code Chunks	318
Chapter 82: Web Crawling in R	319
Section 82.1: Standard scraping approach using the RCurl package	319
Chapter 83: Creating reports with RMarkdown	320
Section 83.1: Including bibliographies	320
Section 83.2: Including LaTeX Preample Commands	320
Section 83.3: Printing tables	321
Section 83.4: Basic R-markdown document structure	323
Chapter 84: GPU-accelerated computing	326
Section 84.1: gpuR gpuMatrix objects	326
Section 84.2: gpuR vclMatrix objects	326
Chapter 85: heatmap and heatmap.2	327
Section 85.1: Examples from the ocial documentation	327
Section 85.2: Tuning parameters in heatmap.2	335
Chapter 86: Network analysis with the igraph package	341
Section 86.1: Simple Directed and Non-directed Network Graphing	341
Chapter 87: Functional programming	343
Section 87.1: Built-in Higher Order Functions	343
Chapter 88: Get user input	344
Section 88.1: User input in R	344
Chapter 89: Spark API (SparkR)	345
Section 89.1: Setup Spark context	345
Section 89.2: Cache data	345
Section 89.3: Create RDDs (Resilient Distributed Datasets)	346
Chapter 90: Meta: Documentation Guidelines	347
Section 90.1: Style	347
Section 90.2: Making good examples	347
Chapter 91: Input and output	348
Section 91.1: Reading and writing data frames	348
Chapter 92: I/O for foreign tables (Excel, SAS, SPSS, Stata)	350
Section 92.1: Importing data with rio	350
Section 92.2: Read and write Stata, SPSS and SAS files	350
Section 92.3: Importing Excel files	351
Section 92.4: Import or Export of Feather file	354
Chapter 93: I/O for database tables	356
Section 93.1: Reading Data from MySQL Databases	356
Section 93.2: Reading Data from MongoDB Databases	356
Chapter 94: I/O for geographic data (shapefiles, etc.)	357
Section 94.1: Import and Export Shapefiles	357
Chapter 95: I/O for raster images	358
Section 95.1: Load a multilayer raster	358
Chapter 96: I/O for R's binary format	360
Section 96.1: Rds and RData (Rda) files	360
Section 96.2: Enviromments	360
Chapter 97: Recycling	361
Section 97.1: Recycling use in subsetting	361
Chapter 98: Expression: parse + eval	362
Section 98.1: Execute code in string format	362
Chapter 99: Regular Expression Syntax in R	363
Section 99.1: Use `grep` to find a string in a character vector	363
Chapter 100: Regular Expressions (regex)	365
Section 100.1: Dierences between Perl and POSIX regex	365
Section 100.2: Validate a date in a "YYYYMMDD" format	365
Section 100.3: Escaping characters in R regex patterns	366
Section 100.4: Validate US States postal abbreviations	366
Section 100.5: Validate US phone numbers	366
Chapter 101: Combinatorics	368
Section 101.1: Enumerating combinations of a specified length	368
Section 101.2: Counting combinations of a specified length	369
Chapter 102: Solving ODEs in R	370
Section 102.1: The Lorenz model	370
Section 102.2: Lotka-Volterra or: Prey vs. predator	371
Section 102.3: ODEs in compiled languages - definition in R	373
Section 102.4: ODEs in compiled languages - definition in C	373
Section 102.5: ODEs in compiled languages - definition in fortran	375
Section 102.6: ODEs in compiled languages - a benchmark test	376
Chapter 103: Feature Selection in R -- Removing Extraneous Features	378
Section 103.1: Removing features with zero or near-zero variance	378
Section 103.2: Removing features with high numbers of NA	378
Section 103.3: Removing closely correlated features	378
Chapter 104: Bibliography in RMD	380
Section 104.1: Specifying a bibliography and cite authors	380
Section 104.2: Inline references	381
Section 104.3: Citation styles	382
Chapter 105: Writing functions in R	385
Section 105.1: Anonymous functions	385
Section 105.2: RStudio code snippets	385
Section 105.3: Named functions	386
Chapter 106: Color schemes for graphics	388
Section 106.1: viridis - print and colorblind friendly palettes	388
Section 106.2: A handy function to glimse a vector of colors	389
Section 106.3: colorspace - click&drag interface for colors	390
Section 106.4: Colorblind-friendly palettes	391
Section 106.5: RColorBrewer	392
Section 106.6: basic R color functions	393
Chapter 107: Hierarchical clustering with hclust	394
Section 107.1: Example 1 - Basic use of hclust, display of dendrogram, plot clusters	394
Section 107.2: Example 2 - hclust and outliers	397
Chapter 108: Random Forest Algorithm	400
Section 108.1: Basic examples - Classification and Regression	400
Chapter 109: RESTful R Services	402
Section 109.1: opencpu Apps	402
Chapter 110: Machine learning	403
Section 110.1: Creating a Random Forest model	403
Chapter 111: Using texreg to export models in a paper-ready way	404
Section 111.1: Printing linear regression results	404
Chapter 112: Publishing	406
Section 112.1: Formatting tables	406
Section 112.2: Formatting entire documents	406
Chapter 113: Implement State Machine Pattern using S4 Class	407
Section 113.1: Parsing Lines using State Machine	407
Chapter 114: Reshape using tidyr	419
Section 114.1: Reshape from long to wide format with spread()	419
Section 114.2: Reshape from wide to long format with gather()	419
Chapter 115: Modifying strings by substitution	421
Section 115.1: Rearrange character strings using capture groups	421
Section 115.2: Eliminate duplicated consecutive elements	421
Chapter 116: Non-standard evaluation and standard evaluation	423
Section 116.1: Examples with standard dplyr verbs	423
Chapter 117: Randomization	425
Section 117.1: Random draws and permutations	425
Section 117.2: Setting the seed	427
Chapter 118: Object-Oriented Programming in R	428
Section 118.1: S3	428
Chapter 119: Coercion	429
Section 119.1: Implicit Coercion	429
Chapter 120: Standardize analyses by writing standalone R scripts	430
Section 120.1: The basic structure of standalone R program and how to call it	430
Section 120.2: Using littler to execute R scripts	431
Chapter 121: Analyze tweets with R	433
Section 121.1: Download Tweets	433
Section 121.2: Get text of tweets	433
Chapter 122: Natural language processing	435
Section 122.1: Create a term frequency matrix	435
Chapter 123: R Markdown Notebooks (from RStudio)	437
Section 123.1: Creating a Notebook	437
Section 123.2: Inserting Chunks	437
Section 123.3: Executing Chunk Code	438
Section 123.4: Execution Progress	439
Section 123.5: Preview Output	440
Section 123.6: Saving and Sharing	440
Chapter 124: Aggregating data frames	442
Section 124.1: Aggregating with data.table	442
Section 124.2: Aggregating with base R	443
Section 124.3: Aggregating with dplyr	444
Chapter 125: Data acquisition	446
Section 125.1: Built-in datasets	446
Section 125.2: Packages to access open databases	446
Section 125.3: Packages to access restricted data	448
Section 125.4: Datasets within packages	452
Chapter 126: R memento by examples	454
Section 126.1: Plotting (using plot)	454
Section 126.2: Commonly used functions	454
Section 126.3: Data types	455
Chapter 127: Updating R version	457
Section 127.1: Installing from R Website	457
Section 127.2: Updating from within R using installr Package	457
Section 127.3: Deciding on the old packages	457
Section 127.4: Updating Packages	459
Section 127.5: Check R Version	459
Credits	460
You may also like	464


About
Please feel free to share this PDF with anyone for free, latest version of this book can be downloaded from: https://goalkicker.com/RBook
This R Notes for Professionals book is compiled from Stack Overflow
Documentation, the content is written by the beautiful people at Stack Overflow.
Text content is released under Creative Commons BY-SA, see credits at the end of this book whom contributed to the various chapters. Images may be copyright of their respective owners unless otherwise specified
This is an unofficial free book created for educational purposes and is not affiliated with official R group(s) or company(s) nor Stack Overflow. All trademarks and registered trademarks are the property of their respective company owners
The information presented in this book is not guaranteed to be correct nor accurate, use at your own risk
Please send feedback and corrections to web@petercv.com
Chapter 1: Getting started with R Language
Section 1.1: Installing R
You might wish to install RStudio after you have installed R. RStudio is a development environment for R that simplifies many programming tasks.
Windows only:
Visual Studio (starting from version 2015 Update 3) now features a development environment for R called R Tools, that includes a live interpreter, IntelliSense, and a debugging module. If you choose this method, you won't have to install R as specified in the following section.
For Windows
1. Go to the CRAN website, click on download R for Windows, and download the latest version of R.
2. Right-click the installer file and RUN as administrator.
3. Select the operational language for installation.
4. Follow the instructions for installation.
For OSX / macOS Alternative 1
(0. Ensure XQuartz is installed )
1. Go to the CRAN website and download the latest version of R.
2. Open the disk image and run the installer.
3. Follow the instructions for installation.
This will install both R and the R-MacGUI. It will put the GUI in the /Applications/ Folder as R.app where it can either be double-clicked or dragged to the Doc. When a new version is released, the (re)-installation process will overwrite
R.app but prior major versions of R will be maintained. The actual R code will be in the
/Library/Frameworks/R.Framework/Versions/ directory. Using R within RStudio is also possible and would be using the same R code with a different GUI.
Alternative 2
1. Install homebrew (the missing package manager for macOS) by following the instructions on https://brew.sh/
2. brew install R
Those choosing the second method should be aware that the maintainer of the Mac fork advises against it, and will not respond to questions about difficulties on the R-SIG-Mac Mailing List.
For Debian, Ubuntu and derivatives
apt-getYou can get the version of R corresponding to your distro via . However, this version will frequently be quite far behind the most recent version available on CRAN. You can add CRAN to your list of recognized "sources".

You can get a more recent version directly from CRAN by adding CRAN to your sources list. Follow the directions from CRAN for more details. Note in particular the need to also execute this so that you can use
install.packages(). Linux packages are usually distributed as source files and need compilation:
sudo apt-get install r-base-dev
For Red Hat and Fedora sudo dnf install R For Archlinux
R is directly available in the Extra package repo.
sudo pacman -S r
More info on using R under Archlinux can be found on the ArchWiki R page.
Section 1.2: Hello World!

Also, check out the detailed discussion of how, when, whether and why to print a string.
Section 1.3: Getting Help
helphelp.searchYou can use function () or ? to access documentations and search for help in R. For even more general searches, you can use () or ??.

Visit https://www.r-project.org/help.html for additional information
Section 1.4: Interactive mode and R scripts
The interactive mode
The most basic way to use R is the interactive mode. You type commands and immediately get the result from R.
Using R as a calculator
Start R by typing R at the command prompt of your operating system or by executing RGui on Windows. Below you can see a screenshot of an interactive R session on Linux:

This is RGui on Windows, the most basic working environment for R under Windows:

After the > sign, expressions can be typed in. Once an expression is typed, the result is shown by R. In the screenshot above, R is used as a calculator: Type

to immediately see the result, 2. The leading [1] indicates that R returns a vector. In this case, the vector contains only one number (2).
The first plot
R can be used to generate plots. The following example uses the data set PlantGrowth, which comes as an example data set along with R
Type int the following all lines into the R prompt which do not start with ##. Lines starting with ## are meant to document the result which R will return.

The following plot is created:

data(PlantGrowth) loads the example data set PlantGrowth, which is records of dry masses of plants which were
subject to two different treatment conditions or no treatment at all (control group). The data set is made available under the name PlantGrowth. Such a name is also called a Variable.
To load your own data, the following two documentation pages might be helpful:
Reading and writing tabular data in plain-text files (CSV, TSV, etc.)
I/O for foreign tables (Excel, SAS, SPSS, Stata)
str(PlantGrowth) shows information about the data set which was loaded. The output indicates that PlantGrowth
data.frame, which is R's name for a table. The data.frameis a  contains of two columns and 30 rows. In this case,
each row corresponds to one plant. Details of the two columns are shown in the lines starting with $: The first column is called weight and contains numbers (num, the dry weight of the respective plant). The second column, group, contains the treatment that the plant was subjected to. This is categorial data, which is called factor in R. Read more information about data frames.
anova(lm( ... data = ...To compare the dry masses of the three different groups, a one-way ANOVA is performed using )). weight ~ group means "Compare the values of the column weight, grouping by the values of the column group". This is called a Formula in R.  specifies the name of the table where the data can be found.
Pr(>F)), p = 0.01591The result shows, among others, that there exists a significant difference (Column ) between some of the three groups. Post-hoc tests, like Tukey's Test, must be performed to determine which groups' means differ significantly.
boxplot(...ylab = ...) creates a box plot of the data. where the values to be plotted come from. weight ~ group means: "Plot the values of the column weight versus the values of the column group.  specifies the label of the y axis. More information: Base plotting
Type q() or  Ctrl - D  to exit from the R session.
R scripts
To document your research, it is favourable to save the commands you use for calculation in a file. For that effect, you can create R scripts. An R script is a simple text file, containing R commands.
plants.Create a text file with the name R, and fill it with the following text, where some commands are familiar from the code block above:

Execute the script by typing into your terminal (The terminal of your operating system, not an interactive R session like in the previous section!)

plant_result.txtThe file  contains the results of your calculation, as if you had typed them into the interactive R prompt. Thereby, your calculations are documented.
dev.offThe new commands png and  are used for saving the boxplot to disk. The two commands must enclose the
png("FILENAME", width = ..., height = ...plotting command, as shown in the example above. ) opens a new
dev.offdev.offPNG file with the specified file name, width and height in pixels. () will finish plotting and saves the plot to disk. No output is saved until () is called.
Chapter 2: Variables
Section 2.1: Variables, data structures and basic Operations
In R, data objects are manipulated using named data structures. The names of the objects might be called "variables" although that term does not have a specific meaning in the official R documentation. R names are case sensitive and may contain alphanumeric characters(a-z,A-z,0-9), the dot/period(.) and underscore(_). To create names for the data structures, we have to follow the following rules:
 Names that start with a digit or an underscore (e.g. 1a), or names that are valid numerical expressions (e.g. .11), or names with dashes ('-') or spaces can only be used when they are quoted: `1a` and `.11`. The names will be printed with backticks:
All other combinations of alphanumeric characters, dots and underscores can be used freely, where reference with or without backticks points to the same object.
ls Names that begin with . are considered system names and are not always visible using the ()-function.
There is no restriction on the number of characters in a variable name.
foo.bar, foo_bar, .foobarSome examples of valid object names are: foobar, 
<-In R, variables are assigned values using the infix-assignment operator <-. The operator = can also be used for assigning values to variables, however its proper use is for associating values with parameter names in function calls. Note that omitting spaces around operators may create confusion for users. The expression a1 is parsed as assignment (
So foo is assigned the value of 42. Typing foo within the console will output 42, while typing fooEquals will output 43.

The following command assigns a value to the variable named x and prints the value simultaneously:

It is also possible to make assignments to variables using ->.


Types of data structures
There are no scalar data types in R. Vectors of length-one act like scalars.
v <- c(2, 3, 7, 10), v2 <- c("a", "b", "c"a <- matrix(data , 2, 3, 4, 5, 6, 7, 8, Vectors: Atomic vectors must be sequence of same-class objects.: a sequence of numbers, or a sequence of logicals or a sequence of characters. ) are both vectors.
Matrices: A matrix of numbers, logical or characters. = c(19,
, byrow , ncol 10, 11, 12), nrow = 4= 3= F). Like vectors, matrix must be made of same-class
elements. To extract elements from a matrix rows and columns must be specified: a[1,2] returns [1] 5 that is the element on the first row, second column.
mylist <- list (course = 'stat', date = '04/07/2009'	 Lists: concatenation of different elements 	,
, num_cons num_isc , num_mat = as.character(c(45020, 45679, 46789, 43126, 42345, 47568= 7= 6,
30, 19, 29, NA, 25, 26 ,2745674)), results = c() ). Extracting elements from a list can be done by
mylistmylistname (if the list is named) or by index. In the given example mylist$results and [[6]] obtains the same element. Warning: if you try [6], R won't give you an error, but it extract the result as a list.
mylist[[6]][2] is permitted (it gives you 19), mylistWhile [6][2] gives you an error.
exam <- data.frame(matr = as.character(c(45020, 45679, 46789, 43126, 42345, 47568 data.frame: object with columns that are vectors of equal length, but (possibly) different types. They are not matrices. ,
45674)), res_S 30, 19, 29, NA, 25, 26, 27), res_O , 3, 1, NA, 3, 2, NA), res_TOT 30,22,30,NA,28,28,27)). Columns can be read by name exam$matr, exam[, 'matr'] or by index exam               = c(= c(3= c([1],
exam[,1]. Rows can also be read by name exam['rowname', ] or index exam[1,]. Dataframes are actually just lists with a particular structure (rownames-attribute and equal length components)
Common operations and some cautionary advice
SyntaxDefault operations are done element by element. See ? for the rules of operator precedence. Most operators (and may other functions in base R) have recycling rules that allow arguments of unequal length. Given these objects:
Example objects


 [1]  7.389056 20.085537 54.598150 Some vector operation Warnings!

R sums what it can and then reuses the shorter vector to fill in the blanks... The warning was given only because the two vectors have lengths that are not exactly multiples. c+f # no warning whatsoever.

To use a matrix multiply: V %*% W

"Private" variables
A leading dot in a name of a variable or function in R is commonly used to denote that the variable or function is meant to be hidden.
So, declaring the following variables

And then using the ls function to list objects will only show the first object.

all.names = TRUEHowever, passing  to the function will show the 'private' variable

Chapter 3: Arithmetic Operators
Section 3.1: Range and addition
Let's take an example of adding a value to a range (as it could be done in a loop for example):

Gives:

This is because the range operator : has higher precedence than addition operator +.
What happens during evaluation is as follows:
, 2, 3, 4, , 5, 6, 7, 3+1:5
3+c(15) expansion of the range operator to make a vector of integers.
c(48) Addition of 3 to each member of the vector.

Now R will compute what is inside the parentheses before expanding the range and gives:

Section 3.2: Addition and subtraction
The basic math operations are performed mainly on numbers or on vectors (lists of numbers).
1. Using single numbers
We can simple enter the numbers concatenated with + for adding and - for subtracting:

We can assign the numbers to variables (constants in this case) and do the same operations:

2. Using vectors
In this case we create vectors of numbers and do the operations using those vectors, or combinations with single numbers. In this case the operation is done considering each element of the vector:

We can also use the function sum to add all elements of a vector:

We must take care with recycling, which is one of the characteristics of R, a behavior that happens when doing math operations where the length of vectors is different. Shorter vectors in the expression are recycled as often as need be (perhaps fractionally) until they match the length of the longest vector. In particular a constant is simply repeated. In this case a Warning is show.


In this case the correct procedure will be to consider only the elements of the shorter vector:

When using the sum function, again all the elements inside the function are added.


Chapter 4: Matrices
Matrices store data
Section 4.1: Creating matrices
Under the hood, a matrix is a special kind of vector with two dimensions. Like a vector, a matrix can only have one data class. You can create matrices using the matrix function as shown below.

As you can see this gives us a matrix of all numbers from 1 to 6 with two rows and three columns. The data parameter takes a vector of values, nrow specifies the number of rows in the matrix, and ncol specifies the number of columns. By convention the matrix is filled by column. The default behavior can be changed with the byrow parameter as shown below:

Matrices do not have to be numeric - any vector can be transformed into a matrix. For example:

Like vectors matrices can be stored as variables and then called later. The rows and columns of a matrix can have names. You can look at these using the functions rownames and colnames. As shown below, the rows and columns don't initially have names, which is denoted by NULL. However, you can assign values to them.

It is important to note that similarly to vectors, matrices can only have one data type. If you try to specify a matrix with multiple data types the data will be coerced to the higher order data class.
The class, is, and as functions can be used to check and coerce data structures in the same way they were used on the vectors in class 1.


Chapter 5: Formula
Section 5.1: The basics of formula
Statistical functions in R make heavy use of the so-called Wilkinson-Rogers formula notation1 .
When running model functions like lm for the Linear Regressions, they need a formula. This formula specifies which regression coefficients shall be estimated.

On the left side of the ~ (LHS) the dependent variable is specified, while the right hand side (RHS) contains the independent variables. Technically the formula call above is redundant because the tilde-operator is an infix function that returns an object with formula class:

The advantage of the formula function over ~ is that it also allows an environment for evaluation to be specified:

In this case, the output shows that a regression coefficient for wt is estimated, as well as (per default) an intercept parameter. The intercept can be excluded / forced to be 0 by including 0 or -1 in the formula:

Interactions between variables a and b can added by included a:b to the formula:

a + b + aAs it is (from a statistical point of view) generally advisable not have interactions in the model without the main effects, the naive approach would be to expand the formula to :b. This works but can be simplified by writing a*b, where the * operator indicates factor crossing (when between two factor columns) or multiplication when one or both of the columns are 'numeric':

Using the * notation expands a term to include all lower order effects, such that:

will give, in addition to the intercept, 7 regression coefficients. One for the three-way interaction, three for the twoway interactions and three for the main effects.
If one wants, for example, to exclude the three-way interaction, but retain all two-way interactions there are two shorthands. First, using - we can subtract any particular term:

Or, we can use the ^ notation to specify which level of interaction we require:

Those two formula specifications should create the same model matrix.
Finally, . is shorthand to use all available variables as main effects. In this case, the data argument is used to obtain the available variables (which are not on the LHS). Therefore:

update.formulagives coefficients for the intercept and 10 independent variables. This notation is frequently used in machine learning packages, where one would like to use all variables for prediction or classification. Note that the meaning of . depends on context (see e.g. ? for a different meaning).
1. G. N. Wilkinson and C. E. Rogers. Journal of the Royal Statistical Society. Series C (Applied Statistics) Vol. 22, No. 3 (1973), pp. 392-399

Chapter 6: Reading and writing strings
Section 6.1: Printing and displaying strings
R has several built-in functions that can be used to print or display information, but print and cat are the most basic. As R is an interpreted language, you can try these out directly in the R console:

x <- "Hello World"Note the difference in both input and output for the two functions. (Note: there are no quote-characters in the value of x created with . They are added by print at the output stage.)
cat takes one or more character vectors as arguments and prints them to the console. If the character vector has a length greater than 1, arguments are separated by a space (by default):

Without the new-line character (\n) the output would be:

The prompt for the next command appears immediately after the output. (Some consoles such as RStudio's may automatically append a newline to strings that do not end with a newline.)
print is an example of a "generic" function, which means the class of the first argument passed is detected and a class-specific method is used to output. For a character vector like "Hello World", the result is similar to the output of cat. However, the character string is quoted and a number [1] is output to indicate the first element of a character vector (In this case, the first and only element):

print(s) or print("Hello World"This default print method is also what we see when we simply ask R to print a variable. Note how the output of typing s is the same as calling ):

Or even without assigning it to anything:

printIf we add another character string as a second element of the vector (using the c() function to concatenate the elements together), then the behavior of () looks quite a bit different from that of cat:


Observe that the c() function does not do string-concatenation. (One needs to use paste for that purpose.) R shows that the character vector has two elements by quoting them separately. If we have a vector long enough to span multiple lines, R will print the index of the element starting each line, just as it prints [1] at the start of the first line.
c("Hello World", "Here I am!", "This next string is really long.") #[1] "Hello World"                      "Here I am!"                      
#[3] "This next string is really long."
The particular behavior of print depends on the class of the object passed to the function.
If we call print an object with a different class, such as "numeric" or "logical", the quotes are omitted from the output to indicate we are dealing with an object that is not character class:

printFactor objects get printed in the same fashion as character variables which often creates ambiguity when console output is used to display objects in SO question bodies. It is rare to use cat or print except in an interactive context. Explicitly calling () is particularly rare (unless you wanted to suppress the appearance of the quotes or view an object that is returned as invisible by a function), as entering foo at the console is a shortcut for
print(foo). The interactive console of R is known as a REPL, a "read-eval-print-loop". The cat function is best saved
for special purposes (like writing output to an open file connection). Sometimes it is used inside functions (where
print() are suppressed), however using catcalls to () inside a function to generate output to the console is
message() or warningbad practice. The preferred method is to () for intermediate messages; they behave
similarly to cat but can be optionally suppressed by the end user. The final result should simply returned so that the user can assign it to store it if necessary.

Section 6.2: Capture output of operating system command
Functions which return a character vector
Base R has two functions for invoking a system command. Both require an additional parameter to capture the output of the system command.

Both return a character vector.
[1] "top - 08:52:03 up 70 days, 15:09,  0 users,  load average: 0.00, 0.00, 0.00"    
[2] "Tasks: 125 total,   1 running, 124 sleeping,   0 stopped,   0 zombie"            
[3] "Cpu(s):  0.9%us,  0.3%sy,  0.0%ni, 98.7%id,  0.1%wa,  0.0%hi,  0.0%si,  0.0%st"    [4] "Mem:  12194312k total,  3613292k used,  8581020k free,   216940k buffers"          [5] "Swap: 12582908k total,  2334156k used, 10248752k free,  1682340k cached"        
[6] ""                                                                                
[7] "  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND            "
[8] "11300 root      20   0 1278m 375m 3696 S  0.0  3.2 124:40.92 trala              "
[9] " 6093 user1     20   0 1817m 269m 1888 S  0.0  2.3  12:17.96 R                  "
[10] " 4949 user2     20   0 1917m 214m 1888 S  0.0  1.8  11:16.73 R                  "
top -a -b -n For illustration, the UNIX command 1 is used. This is OS specific and may need to be amended to run the examples on your computer.
Package devtools has a function to run a system command and capture the output without an additional parameter. It also returns a character vector. devtools::system_output("top", "-a -b -n 1")
Functions which return a data frame
data.tableThe fread function in package  allows to execute a shell command and to read the output like
read.table. It returns a data.table or a data.frame.
fread("top -a -b -n 1", check.names = TRUE)
       PID     USER PR NI  VIRT  RES  SHR S X.CPU X.MEM     TIME.         COMMAND
  1: 11300     root 20  0 1278m 375m 3696 S     0   3.2 124:40.92           trala
  2:  6093    user1 20  0 1817m 269m 1888 S     0   2.3  12:18.56               R
  3:  4949    user2 20  0 1917m 214m 1888 S     0   1.8  11:17.33               R
   4:  7922    user3 20  0 3094m 131m 1892 S     0   1.1  21:04.95               R Note, that fread automatically has skipped the top 6 header lines.
check.names = TRUE was added to convert %CPU, %MEN, and TIMEHere the parameter + to syntactically valid column names.
Section 6.3: Reading from or writing to a file connection
Not always we have liberty to read from or write to a local system path. For example if R code streaming mapreduce must need to read and write to file connection. There can be other scenarios as well where one is going beyond local system and with advent of cloud and big data, this is becoming increasingly common. One of the way to do this is in logical sequence.
fileEstablish a file connection to read with () command ("r" is for read mode):
conn <- file("/path/example.data", "r") #when file is in local system
conn1 <- file("stdin", "r") #when just standard input/output for files are available
As this will establish just file connection, one can read the data from these file connections as follows:

=-Here we are reading the data from file connection conn line by line as n=1. one can change value of n (say 10, 20 etc.) for reading data blocks for faster reading (10 or 20 lines block read in one go). To read complete file in one go set n1.
writeLines(),catfileAfter data processing or say model execution; one can write the results back to file connection using many different commands like () etc. which are capable of writing to a file connection. However all of these commands will leverage file connection established for writing. This could be done using () command as:
conn2 <- file("/path/result.data", "w") #when file is in local system
conn3 <- file("stdout", "w") #when just standard input/output for files are available
Then write the data as follows:


Chapter 7: String manipulation with stringi package
Section 7.1: Count pattern inside string
With fixed pattern

Natively:

function is vectorized over string and pattern:

A base R solution:

With regex
First example - find a and any character after
Second example - find a and any digit after

Section 7.2: Duplicating strings

A base R solution that does the same would look like this:

Section 7.3: Paste vectors
stri_paste(LETTERS,"-", 1:13)
# [1] "A-1"  "B-2"  "C-3"  "D-4"  "E-5"  "F-6"  "G-7"  "H-8"  "I-9"  "J-10" "K-11" "L-12" "M-13"
# [14] "N-1"  "O-2"  "P-3"  "Q-4"  "R-5"  "S-6"  "T-7"  "U-8"  "V-9"  "W-10" "X-11" "Y-12" "Z-13"
Natively, we could do this in R via:
> paste(LETTERS,1:13,sep="-")
 #[1] "A-1"  "B-2"  "C-3"  "D-4"  "E-5"  "F-6"  "G-7"  "H-8"  "I-9"  "J-10" "K-11" "L-12" "M-13"
 #[14] "N-1"  "O-2" "P-3"  "Q-4"  "R-5"  "S-6"  "T-7"  "U-8"  "V-9"  "W-10" "X-11" "Y-12" "Z-13"
Section 7.4: Splitting text by some fixed pattern
Split vector of texts using one pattern:

Split one text using many patterns:

Chapter 8: Classes
class<-The class of a data-object determines which functions will process its contents. The class-attribute is a character vector, and objects can have zero, one or more classes. If there is no class-attribute, there will still be an implicit class determined by an object's mode. The class can be inspected with the function class and it can be set or modified by the  function. The S3 class system was established early in S's history. The more complex S4 class system was established later
Section 8.1: Inspect classes
class() to find the object's class and strEvery object in R is assigned a class. You can use () to see its structure, including the classes it contains. For example:

data.frame and using strWe see that iris has the class () allows us to examine the data inside. The variable
strclassSpecies in the iris data frame is of class factor, in contrast to the other variables which are of class numeric. The () function also provides the length of the variables and shows the first couple of observations, while the () function only provides the object's class.
Section 8.2: Vectors and lists
atomic"logical", "integer", "numeric" (synonym "double"), "complex", "character"Data in R are stored in vectors. A typical vector is a sequence of values all having the same storage mode (e.g., characters vectors, numeric vectors). See ? for details on the atomic implicit classes and their corresponding storage modes:  and "raw".
Many classes are simply an atomic vector with a class attribute on top:

Lists are a special type of vector where each element can be anything, even another list, hence the R term for lists:
"recursive vectors":
mylist <- list( A = c(5,6,7,8), B = letters[1:10], CC = list( 5, "Z") )

vectors all having the same length:

The other class of recursive vectors is R expressions, which are "language"- objects
Section 8.3: Vectors
The most simple data structure available in R is a vector. You can make vectors of numeric values, logical values, and character strings using the c() function. For example:

You can also join to vectors using the c() function.

A more elaborate treatment of how to create vectors can be found in the "Creating vectors" topic
Chapter 9: Lists
Section 9.1: Introduction to lists
Lists allow users to store multiple elements (like vectors and matrices) under a single object. You can use the list function to create a list:

Notice the vectors that make up the above list are different classes. Lists allow users to group elements of different classes. Each element in a list can also have a name. List names are accessed by the names function, and are assigned in the same manner row and column names are assigned in a matrix.

It is often easier and safer to declare the list names when creating the list object.

Above the list has two elements, named "vec" and "mat," a vector and matrix, resepcively.
Section 9.2: Quick Introduction to Lists
In general, most of the objects you would interact with as a user would tend to be a vector; e.g numeric vector, logical vector. These objects can only take in a single type of variable (a numeric vector can only have numbers inside it).
A list would be able to store any type variable in it, making it to the generic object that can store any type of variables we would need.
Example of initializing a list

In order to understand the data that was defined in the list, we can use the str function.

Subsetting of lists distinguishes between extracting a slice of the list, i.e. obtaining a list containing a subset of the elements in the original list, and extracting a single element. Using the [ operator commonly used for vectors produces a new list.

To obtain a single element use [[ instead.

List entries may be named:

The entries in named lists can be accessed by their name instead of their index.

Alternatively the $ operator can be used to access named elements.

This has the advantage that it is faster to type and may be easier to read but it is important to be aware of a potential pitfall. The $ operator uses partial matching to identify matching list elements and may produce unexpected results.

Lists can be particularly useful because they can store objects of different lengths and of various classes.

Section 9.3: Serialization: using lists to pass information
There exist cases in which it is necessary to put data of different types together. In Azure ML for example, it is necessary to pass information from a R script module to another one exclusively throught dataframes. Suppose we have a dataframe and a number:
> df        name height        team fun_index title age         desc Y 1    Andrea    195       Lazio        97     6  33   eccellente 1
2 Paja    165  Fiorentina        87     6  31       deciso 1
3 Roro    190       Lazio        65     6  28       strano 0
4 Gioele     70       Lazio       100     0   2    simpatico 1
5 Cacio    170    Juventus        81     3  33         duro 0
6 Edola    171       Lazio        72     5  32     svampito 1
7 Salami    175       Inter        75     3  30  doppiopasso 1
8 Braugo    180       Inter        79     5  32          gjn 0
9 Benna    158    Juventus        80     6  28     esaurito 0
10 Riggio    182       Lazio        92     5  31     certezza 1
11 Giordano    185        Roma        79     5  29        buono 1

We can access to this information:

In order to put different types of data in a dataframe we have to use the list object and the serialization. In particular we have to put the data in a generic list and then put the list in a particular dataframe:

Once we have stored the information in the dataframe, we need to deserialize it in order to use it:

Then, we can verify that the data are transfered correctly:

Chapter 10: Hashmaps
Section 10.1: Environments as hash maps
Note: in the subsequent passages, the terms hash map and hash table are used interchangeably and refer to the same concept, namely, a data structure providing efficient key lookup through use of an internal hash function.
Introduction
new.envAlthough R does not provide a native hash table structure, similar functionality can be achieved by leveraging the fact that the environment object returned from  (by default) provides hashed key lookups. The following two statements are equivalent, as the hash parameter defaults to TRUE:

Additionally, one may specify that the internal hash table is pre-allocated with a particular size via the size parameter, which has a default value of 29. Like all other R objects, environments manage their own memory and will grow in capacity as needed, so while it is not necessary to request a non-default value for size, there may be a slight performance advantage in doing so if the object will (eventually) contain a very large number of elements. It is worth noting that allocating extra space via size does not, in itself, result in an object with a larger memory footprint:

Insertion
<- or $<-<-Insertion of elements may be done using either of the [[ methods provided for the environment class, but not by using "single bracket" assignment ([):

object[[key]] <- valueLike other facets of R, the first method () is generally preferred to the second (object$key
<- value) because in the former case, a variable maybe be used instead of a literal value (e.g key2 in the example
above).
As is generally the case with hash map implementations, the environment object will not store duplicate keys. Attempting to insert a key-value pair for an existing key will replace the previously stored value:

Key Lookup
Likewise, elements may be accessed with [[ or $, but not with [:

Inspecting the Hash Map
Being just an ordinary environment, the hash map can be inspected by typical means:



Flexibility
One of the major benefits of using environment objects as hash tables is their ability to store virtually any type of object as a value, even other environments:


Limitations
One of the major limitations of using environment objects as hash maps is that, unlike many aspects of R, vectorization is not supported for element lookup / insertion:

Depending on the nature of the data being stored in the object, it may be possible to use vapply or list2env for assigning many elements at once:

Neither of the above are particularly concise, but may be preferable to using a for loop, etc. when the number of key-value pairs is large.
Section 10.2: package:hash
The hash package offers a hash structure in R. However, it terms of timing for both inserts and reads it compares unfavorably to using environments as a hash. This documentation simply acknowledges its existence and provides sample timing code below for the above stated reasons. There is no identified case where hash is an appropriate solution in R code today.
Consider:

Section 10.3: package:listenv
package:listenvpackage:futureAlthough  implements a list-like interface to environments, its performance relative to environments for hash-like purposes is poor on hash retrieval. However, if the indexes are numeric, it can be quite fast on retrieval. However, they have other advantages, e.g. compatibility with . Covering this package for that purpose goes beyond the scope of the current topic. However, the timing code provided here can be used in conjunction with the example for package:hash for write timings.


Chapter 11: Creating vectors
Section 11.1: Vectors from build in constants: Sequences of letters & month names
R has a number of build in constants. The following constants are available:
month.abbmonth.nameLETTERS: the 26 upper-case letters of the Roman alphabet letters: the 26 lower-case letters of the Roman alphabet
: the three-letter abbreviations for the English month names
: the English names for the months of the year
pi: the ratio of the circumference of a circle to its diameter
From the letters and month constants, vectors can be created.
1) Sequences of letters:

2) Sequences of month abbreviations or month names:

Section 11.2: Creating named vectors
Named vector can be created in several ways. With c:

which results in:

with list:

which results in:

With the setNames function, two vectors of the same length can be used to create a named vector:

which results in a named integer vector:

As can be seen, this gives the same result as the c method.
You may also use the names function to get the same result:

With such a vector it is also possibly to select elements by name:

This feature makes it possible to use such a named vector as a look-up vector/table to match the values to values of another vector or column in dataframe. Considering the following dataframe:

Suppose you want to create a new variable in the mydf dataframe called num with the correct values from xy in the rows. Using the match function the appropriate values from xy can be selected:

which results in:

Section 11.3: Sequence of numbers
Use the : operator to create sequences of numbers, such as for use in vectorizing larger chunks of your code:

This works both ways

and even with floating point numbers

or negatives

Section 11.4: seq()
seq is a more flexible function than the : operator allowing to specify steps other than 1.
The function creates a sequence from the start (default is 1) to the end including that number.
You can supply only the end (to) parameter

As well as the start

And finally the step (by)

length.outseq can optionally infer the (evenly spaced) steps when alternatively the desired length of the output () is supplied


length.out = lengthIf the sequence needs to have the same length as another vector we can use the along.with as a shorthand for (x)

seq.intThere are two useful simplified functions in the seq family: seq_along, seq_len, and . seq_along and seq_len functions construct the natural (counting) numbers from 1 through N where N is determined by the function argument, the length of a vector or list with seq_along, and the integer argument with seq_len.

Note that seq_along returns the indices of an existing object.

seq.intis the same as seq maintained for ancient compatibility.
There is also an old function sequencethat creates a vector of sequences from a non negative argument.

Section 11.5: Vectors
vectorVectors in R can have different types (e.g. integer, logical, character). The most general way of defining a vector is by using the function ().
vector('integer',2) # creates a vector of integers of size 2.
vector('character',2) # creates a vector of characters of size 2. vector('logical',2) # creates a vector of logicals of size 2.
However, in R, the shorthand functions are generally more popular.
integer(2) # is the same as vector('integer',2) and creates an integer vector with two elements character(2) # is the same as vector('integer',2) and creates an character vector with two elements logical(2) # is the same as vector('logical',2) and creates an logical vector with two elements
Creating vectors with values, other than the default values, is also possible. Often the function c() is used for this. The c is short for combine or concatenate.
c(1, 2) # creates a integer vector of two elements: 1 and 2. c('a', 'b') # creates a character vector of two elements: a and b. c(T,F) # creates a logical vector of two elements: TRUE and FALSE.
Important to note here is that R interprets any integer (e.g. 1) as an integer vector of size one. The same holds for numerics (e.g. 1.1), logicals (e.g. T or F), or characters (e.g. 'a'). Therefore, you are in essence combining vectors, which in turn are vectors.
Pay attention that you always have to combine similar vectors. Otherwise, R will try to convert the vectors in vectors of the same type.
c(1,1.1,'a',T) # all types (integer, numeric, character and logical) are converted to the 'lowest' type which is character.
Finding elements in vectors can be done with the [ operator.

This can also be used to change values

seqFinally, the : operator (short for the function ()) can be used to quickly create a vector of numbers.

This can also be used to subset vectors (from easy to more complex subsets)

Section 11.6: Expanding a vector with the rep() function
The rep function can be used to repeat a vector in a fairly flexible manner.

The each argument is especially useful for expanding a vector of statistics of observational/experimental units into a vector of data.frame with repeated observations of these units.

A nice feature of rep regarding involving expansion to such a data structure is that expansion of a vector to an unbalanced panel can be accomplished by replacing the length argument with a vector that dictates the number of times to repeat each element in the vector:

This should expose the possibility of allowing an external function to feed the second argument of rep in order to dynamically construct a vector that expands according to the data.
rep.intAs with seq, faster, simplified versions of rep are rep_len and . These drop some attributes that rep maintains and so may be most useful in situations where speed is a concern and additional aspects of the repeated vector are unnecessary.


Chapter 12: Date and Time
?Dates, ?DateTimeClasses, ?difftimeR comes with classes for dates, date-times and time differences; see  and follow the "See Also" section of those docs for further documentation. Related Docs: Dates and Date-Time Classes.
Section 12.1: Current Date and Time
R is able to access the current date, time and time zone:

OlsonNames()Use  to view the time zone names in Olson/IANA database on the current system:
str(OlsonNames())
## chr [1:589] "Africa/Abidjan" "Africa/Accra" "Africa/Addis_Ababa" "Africa/Algiers" "Africa/Asmara" "Africa/Asmera" "Africa/Bamako" ...
Section 12.2: Go to the End of the Month
Let's say we want to go to the last day of the month, this function will help on it: eom <- function(x, p=as.POSIXlt(x)) as.Date(modifyList(p, list(mon=p$mon + 1, mday=0))) Test:

Using a date in a string format:

Section 12.3: Go to First Day of the Month
Let's say we want to go to the first day of a given month:

Section 12.4: Move a date a number of months consistently by months
Let's say we want to move a given date a numof months. We can define the following function, that uses the mondate package:

It moves consistently the month part of the date and adjusting the day, in case the date refers to the last day of the month.
For example:
Back one month:

Back two months:

Forward two months:

It moves two months from the last day of February, therefore the last day of April.
Let's se how it works for backward and forward operations when it is the last day of the month:

Because November has 30 days, we get the same date in the backward operation, but:

Because January has 31 days, then moving two months from last day of November will get the last day of January.

Chapter 13: The Date class
Section 13.1: Formatting Dates
format(date, format="%Y-%m-%d")To format Dates we use the  function with either the POSIXct (given from
as.POSIXct()) or POSIXlt (given from as.POSIXlt())

?strptimeFor more, see .
Section 13.2: Parsing Strings into Date Objects
as.Date()YYYY-MM-DDR contains a Date class, which is created with , which takes a string or vector of strings, and if the date is not in ISO 8601 date format , a formatting string of strptime-style tokens.

Section 13.3: Dates

as.Date() function allows you to provide a format argument. The default is %Y-%m-%The d, which is Year-monthday.

The format string can be placed either within a pair of single quotes or double quotes. Dates are usually expressed in a variety of forms such as: "d-m-yy" or "d-m-YYYY" or "m-d-yy" or "m-d-YYYY" or "YYYY-m-d" or "YYYY-d-m". These formats can also be expressed by replacing "-" by "/". Furher, dates are also expressed in the forms, say, "Nov 6, 1986" or "November 6, 1986" or "6 Nov, 1986" or "6 November, 1986" and so on. The as.Date() function accepts all such character strings and when we mention the appropriate format of the string, it always outputs the date in the form "YYYY-m-d".
Suppose we have a date string "9-6-1962" in the format "%d-%m-%Y".

By specifying the correct format of the input string, we can get the desired results. We use the following codes for specifying the formats to the as.Date() function.
Format Code	Meaning
%d	day
%m	month
%y	year in 2-digits
%Y	year in 4-digits
%b	abbreviated month in 3 chars
%B	full name of the month
Consider the following example specifying the format parameter:

The parameter name format can be omitted.

Some times, names of the months abbreviated to the first three characters are used in the writing the dates. In which case we use the format specifier %b.

Note that, there are no either '-' or '/' or white spaces between the members in the date string. The format string should exactly match that input string. Consider the following example:

Note that, there is a comma in the date string and hence a comma in the format specification too. If comma is omitted in the format string, it results in an NA. An example usage of %B format specifier is as follows:

%y format is system specific and hence, should be used with caution. Other parameters used with this function are origin and tz( time zone).
Chapter 14: Date-time classes (POSIXct and POSIXlt)
?DateTimeClassesR includes two date-time classes -- POSIXct and POSIXlt -- see .
Section 14.1: Formatting and printing date-time objects

?strptimeSee  for details on the format strings here, as well as other formats.
Section 14.2: Date-time arithmetic
To add/subtract time, use POSIXct, since it stores times in seconds

as.difftimeMore formally,  can be used to specify time periods to add to a date or datetime object. E.g.:

difftimeTo find the difference between dates/times use () for differences in seconds, minutes, hours, days or weeks.

seq.POSIXtTo generate sequences of date-times use () or simply seq.
Section 14.3: Parsing strings into date-time objects
The functions for parsing a string into POSIXct and POSIXlt take similar parameters and return a similar-looking result, but there are differences in how that date-time is stored; see "Remarks."

Note that date and timezone are imputed.

Notes Missing elements
If a date element is not supplied, then that from the current date is used.
If a time element is not supplied, then that from midnight is used, i.e. 0s.
If no timezone is supplied in either the string or the tz parameter, the local timezone is used.
Time zones
The accepted values of tz depend on the location.
     CST is given with "CST6CDT" or "America/Chicago" For supported locations and time zones use:
OlsonNamesIn R: ()
system("cat $R_HOME/share/zoneinfo/zone.tab"Alternatively, try in R: )
These locations are given by Internet Assigned Numbers Authority (IANA)
List of tz database time zones (Wikipedia)
 IANA TZ Data (2016e)

Chapter 15: The character class
Characters are what other languages call 'string vectors.'
Section 15.1: Coercion
is.character()as.character()To check whether a value is a character use the  function. To coerce a variable to a character use the  function.

Note that numerics can be coerced to characters, but attempting to coerce a character to numeric may result in NA.


Chapter 16: Numeric classes and storage modes
Section 16.1: Numeric
Numeric represents integers and doubles and is the default mode assigned to vectors of numbers. The function
is.numericis.numeric(), the function as.numeric() will evaluate whether a vector is numeric. It is important to note that although integers and doubles will pass () will always attempt to convert to type double.

Doubles are R's default numeric value. They are double precision vectors, meaning that they take up 8 bytes of memory for each value in the vector. R has no single precision data type and so all real numbers are stored in the double precision format.

Integers are whole numbers that can be written without a fractional component. Integers are represented by a number with an L after it. Any number without an L after it will be considered a double.

Though in most cases using an integer or double will not matter, sometimes replacing doubles with integers will consume less memory and operational time. A double vector uses 8 bytes per element while an integer vector uses only 4 bytes per element. As the size of vectors increases, using proper types can dramatically speed up processes.


Chapter 17: The logical class
Logical is a mode (and an implicit class) for vectors.
Section 17.1: Logical operators
xorThere are two sorts of logical operators: those that accept and return vectors of any length (elementwise operators: !, |, &, ()) and those that only evaluate the first element in each argument (&&, ||). The second sort is primarily used as the cond argument to the if function.
Logical Operator	MeaningSyntax!	Not!x&	element-wise (vectorized) andx & y&&	and (single element only)x && y|	element-wise (vectorized) orx | y||	or (single element only)x || yxor	element-wise (vectorized) exclusive OR xor(x,y)
Note that the || operator evaluates the left condition and if the left condition is TRUE the right side is never evaluated. This can save time if the first is the result of a complex operation. The && operator will likewise return FALSE without evaluation of the second argument when the first element of the first argument is FALSE.

is.logicalTo check whether a value is a logical you can use the () function.
Section 17.2: Coercion
as.logicalTo coerce a variable to a logical use the () function.

as.numericWhen applying () to a logical, a double will be returned. NA is a logical value and a logical operator with an NA will return NA if the outcome is ambiguous.
Section 17.3: Interpretation of NAs
See Missing values for details.



Chapter 18: Data frames
Section 18.1: Create an empty data.frame
A data.frame is a special kind of list: it is rectangular. Each element (column) of the list has same length, and where each row has a "row name". Each column has its own class, but the class of one column can be different from the class of another column (unlike a matrix, where all elements must have the same class).
In principle, a data.frame could have no rows and no columns:

But this is unusual. It is more common for a data.frame to have many columns and many rows. Here is a data.frame with three rows and two columns (a is numeric class and b is character class):

In order for the data.frame to print, we will need to supply some row names. Here we use just the numbers 1:3:

nrowNow it becomes obvious that we have a data.frame with 3 rows and 2 columns. You can check this using (),

structuredata.framedata.frameR provides two other functions (besides ()) that can be used to create a data.frame. The first is called, intuitively, (). It checks to make sure that the column names you supplied are valid, that the list elements are all the same length, and supplies some automatically generated row names. This means that the output of () might now always be exactly what you expect:

as.data.framedata.frameThe other function is called (). This can be used to coerce an object that is not a data.frame into being a data.frame by running it through (). As an example, consider a matrix:


And the result:

Section 18.2: Subsetting rows and columns from a data frame
Syntax for accessing rows and columns: [, [[, and $
This topic covers the most common syntax to access specific rows and columns of a data frame. These are
data[rows, columnsLike a matrix with single brackets ]
Using row and column numbers
Using column (and row) names
Like a list:
data[columnsWith single brackets ] to get a data frame
data[[one_columnWith double brackets ]] to get a vector
data$column_nameWith $ for a single column 
We will use the built-in mtcars data frame to illustrate.
data[rows, columnsLike a matrix: ] With numeric indexes
Using the built in data frame mtcars, we can extract rows and columns using [] brackets with a comma included. Indices before the comma are rows:

Similarly, after the comma are columns:

mtcars[1, As shown above, if either rows or columns are left blank, all will be selected. ] indicates the first row with all the columns.
With column (and row) names
data.frameSo far, this is identical to how rows and columns of matrices are accessed. With s, most of the time it is preferable to use a column name to a column index. This is done by using a character with the column name instead of numeric with a column number:

Though less common, row names can also be used: mtcars["Mazda Rx4", ]
Rows and columns together
The row and column arguments can be used together:

A warning about dimensions:
When using these methods, if you extract multiple columns, you will get a data frame back. However, if you extract a single column, you will get a vector, not a data frame under the default options.

drop There are two ways around this. One is to treat the data frame as a list (see below), the other is to add a = FALSE argument. This tells R to not "drop the unused dimensions":

drop Note that matrices work the same way - by default a single column or row will be a vector, but if you specify = FALSE you can keep it as a one-column or one-row matrix.
Like a list
Data frames are essentially lists, i.e., they are a list of column vectors (that all must have the same length). Lists can be subset using single brackets [ for a sub-list, or double brackets [[ for a single element.
data[columnsWith single brackets ]
When you use single brackets and no commas, you will get column back because data frames are lists of columns.

Single brackets like a list vs. single brackets like a matrix
data[columns] and data[, columns] is that when treating the data.frameThe difference between  as a list (no
data.frame. If you use a comma to treat the data.framedata.framecomma in the brackets) the object returned will be a  like a matrix then selecting a single column will return a vector but selecting multiple columns will return a .

data[[one_columnWith double brackets ]]
data.frameTo extract a single column as a vector when treating your  as a list, you can use double brackets [[. This will only work for a single column at a time.

Using $ to access columns
A single column can be extracted using the magical shortcut $ without using a quoted column name:

Columns accessed by $ will always be vectors, not data frames.
Drawbacks of $ for accessing columns
The $ can be a convenient shortcut, especially if you are working in an environment (such as RStudio) that will autocomplete the column name in this case. However, $ has drawbacks as well: it uses non-standard evaluation to avoid the need for quotes, which means it will not work if your column name is stored in a variable.

Due to these concerns, $ is best used in interactive R sessions when your column names are constant. For programmatic use, for example in writing a generalizable function that will be used on different data sets with different column names, $ should be avoided.
Also note that the default behaviour is to use partial matching only when extracting from recursive objects (except environments) by $


Advanced indexing: negative and logical indices
Whenever we have the option to use numbers for a index, we can also use negative numbers to omit certain indices or a boolean (logical) vector to indicate exactly which items to keep.
Negative indices omit elements mtcars[1, ]   # first row
Logical vectors indicate specific elements to keep
We can use a condition such as < to generate a logical vector, and extract only the rows that meet the condition:

We can also bypass the step of saving the intermediate variable
# extract all columns for rows where the value of cyl is 4. mtcars[mtcars$cyl == 4, ]
# extract the cyl, mpg, and hp columns where the value of cyl is 4 mtcars[mtcars$cyl == 4, c("cyl", "mpg", "hp")]
Section 18.3: Convenience functions to manipulate data.frames
data.frames are subset(), transform(), with() and withinSome convenience functions to manipulate ().
subset
subset() function allows you to subset a data.frameThe  in a more convenient way (subset also works with other classes):

cyl == In the code above we asking only for the lines in which 6 and for the columns mpg and hp. You could achieve the same result using [] with the following code:

transform
transform() function is a convenience function to change columns inside a data.framempg^2 to the mtcars data.frameThe . For instance the following code adds another column named mpg2 with the result of :

with and within
with() and within() let you to evaluate expressions inside the data.frameBoth  environment, allowing a somewhat cleaner syntax, saving you the use of some $ or [].
data.frameFor example, if you want to create, change and/or remove multiple columns in the airquality :

Section 18.4: Introduction
data.frameData frames are likely the data structure you will used most in your analyses. A data frame is a special kind of list that stores same-length vectors of different classes. You create data frames using the  function. The example below shows this by combining a numeric and a character vector into a data frame. It uses the : operator, which will create a vector containing all integers from 1 to 3.

Data frame objects do not print with quotation marks, so the class of the columns is not always obvious.

Without further investigation, the "x" columns in df1 and df2 cannot be differentiated. The str function can be used to describe objects with more detail than class.


data.frameHere you see that df1 is a  and has 3 observations of 2 variables, "x" and "y." Then you are told that "x" has the data type integer (not important for this class, but for our purposes it behaves like a numeric) and "y" is a factor with three levels (another data class we are not discussing). It is important to note that, by default, data frames coerce characters to factors. The default behavior can be changed with the stringsAsFactors parameter:

Now the "y" column is a character. As mentioned above, each "column" of a data frame must have the same length.
data.frame, y Trying to create a data.frame from vectors with different lengths will result in an error. (Try running (x = 1:3= 1:4) to see the resulting error.)
As test-cases for data frames, some data is provided by R by default. One of them is iris, loaded as follows:

Section 18.5: Convert all columns of a data.frame to character class
A common task is to convert all columns of a data.frame to character class for ease of manipulation, such as in the cases of sending data.frames to a RDBMS or merging data.frames containing factors where levels may differ between input data.frames.
The best time to do this is when the data is read in - almost all input methods that create data frames have an options stringsAsFactors which can be set to FALSE.
If the data has already been created, factor columns can be converted to character columns as shown below.



Chapter 19: Split function
Section 19.1: Using split in the split-apply-combine paradigm
A popular form of data analysis is split-apply-combine, in which you split your data into groups, apply some sort of processing on each group, and then combine the results.
Let's consider a data analysis where we want to obtain the two cars with the best miles per gallon (mpg) for each cylinder count (cyl) in the built-in mtcars dataset. First, we split the mtcars data frame by the cylinder count:
(spl <- split(mtcars, mtcars$cyl))
# $`4`
#                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb
# Datsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
# Merc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2
# Merc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2 # Fiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 # ...
#
# $`6`
#                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb
# Mazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
# Mazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
# Hornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1 # Valiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1 # ...
#
# $`8`
#                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb
# Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
# Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4
# Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3 # Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3 # ...
This has returned a list of data frames, one for each cylinder count. As indicated by the output, we could obtain the
spl$`4`, spl$`6`, and spl$`relevant data frames with 8` (some might find it more visually appealing to use
spl$"4" or spl[["4"]] instead).
Now, we can use lapply to loop through this list, applying our function that extracts the cars with the best 2 mpg values from each of the list elements:
(best2 <- lapply(spl, function(x) tail(x[order(x$mpg),], 2)))
# $`4`
#                 mpg cyl disp hp drat    wt  qsec vs am gear carb
# Fiat 128       32.4   4 78.7 66 4.08 2.200 19.47  1  1    4    1 # Toyota Corolla 33.9   4 71.1 65 4.22 1.835 19.90  1  1    4    1
#
# $`6`
#                 mpg cyl disp  hp drat    wt  qsec vs am gear carb
# Mazda RX4 Wag  21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 # Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
#
# $`8`
#                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
# Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
# Pontiac Firebird  19.2   8  400 175 3.08 3.845 17.05  0  0    3    2
rbind(best2[["4"]], best2[["6"Finally, we can combine everything together using rbind. We want to call ]],
best2[["8"]]), but this would be tedious if we had a huge list. As a result, we use:
do.call(rbind, best2) #                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb
# 4.Fiat 128          32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1
# 4.Toyota Corolla    33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1
# 6.Mazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
# 6.Hornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1
# 8.Hornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
# 8.Pontiac Firebird  19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2
This returns the result of rbind (argument 1, a function) with all the elements of best2 (argument 2, a list) passed as arguments.
With simple analyses like this one, it can be more compact (and possibly much less readable!) to do the whole splitapply-combine in a single line of code: do.call(rbind, lapply(split(mtcars, mtcars$cyl), function(x) tail(x[order(x$mpg),], 2)))
lapply(split(x,f), FUN) combination can be alternatively framed using the ?byIt is also worth noting that the function:
by(mtcars, mtcars$cyl, function(x) tail(x[order(x$mpg),], 2))
do.call(rbind, by(mtcars, mtcars$cyl, function(x) tail(x[order(x$mpg),], 2)))
Section 19.2: Basic usage of split
split allows to divide a vector or a data.frame into buckets with regards to a factor/group variables. This ventilation into buckets takes the form of a list, that can then be used to apply group-wise computation (for loops or lapply/sapply).
First example shows the usage of split on a vector:
Consider following vector of letters: testdata <- c("e", "o", "r", "g", "a", "y", "w", "q", "i", "s", "b", "v", "x", "h", "u")
Objective is to separate those letters into voyels and consonants, ie split it accordingly to letter type.
Let's first create a grouping vector:

Note that letter_type has the same length that our vector testdata. Now we can split this test data in the two

Hence, the result is a list which names are coming from our grouping vector/factor letter_type. split has also a method to deal with data.frames.
Consider for instance iris data:

By using split, one can create a list containing one data.frame per iris specie (variable: Species):
> liris <- split(iris, iris$Species)
> names(liris)
[1] "setosa"     "versicolor" "virginica"
> head(liris$setosa)
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1 5.1         3.5          1.4         0.2  setosa
2 4.9         3.0          1.4         0.2  setosa
3 4.7         3.2          1.3         0.2  setosa
4 4.6         3.1          1.5         0.2  setosa
5 5.0         3.6          1.4         0.2  setosa
6 5.4         3.9          1.7         0.4  setosa
(contains only data for setosa group).
One example operation would be to compute correlation matrix per iris specie; one would then use lapply:
> (lcor <- lapply(liris, FUN=function(df) cor(df[,1:4])))
    $setosa
             Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length    1.0000000   0.7425467    0.2671758   0.2780984
Sepal.Width     0.7425467   1.0000000    0.1777000   0.2327520
Petal.Length    0.2671758   0.1777000    1.0000000   0.3316300
Petal.Width     0.2780984   0.2327520    0.3316300   1.0000000
$versicolor
             Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length    1.0000000   0.5259107    0.7540490   0.5464611
Sepal.Width     0.5259107   1.0000000    0.5605221   0.6639987
Petal.Length    0.7540490   0.5605221    1.0000000   0.7866681
Petal.Width     0.5464611   0.6639987    0.7866681   1.0000000
$virginica
             Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length    1.0000000   0.4572278    0.8642247   0.2811077
Sepal.Width     0.4572278   1.0000000    0.4010446   0.5377280
Petal.Length    0.8642247   0.4010446    1.0000000   0.3221082
Petal.Width     0.2811077   0.5377280    0.3221082   1.0000000
Then we can retrieve per group the best pair of correlated variables: (correlation matrix is reshaped/melted, diagonal is filtered out and selecting best record is performed)


Note that one computations are performed on such groupwise level, one may be interested in stacking the results, which can be done with:


Chapter 20: Reading and writing tabular data in plain-text files (CSV, TSV, etc.)
Parameter	Details
file	name of the CSV file to read header	logical: does the .csv file contain a header row with column names?
sep	character: symbol that separates the cells on each row quote	character: symbol used to quote character strings dec	character: symbol used as decimal separator fill	logical: when TRUE, rows with unequal length are filled with blank fields. comment.char character: character used as comment in the csv file. Lines preceded by this character are ignored.
read.table...	extra arguments to be passed to 
Section 20.1: Importing .csv files
Importing using base R
read.csv, which wraps read.table, but uses sep = ","Comma separated value files (CSVs) can be imported using to set the delimiter to a comma.

file.chooseA user friendly option, , allows to browse through the directories:

Notes
read.table, read.csv defaults to header = TRUEas.is = TRUEstringsAsFactors = FALSEread.csv2 variant defaults to sep = ";" and dec = ","Unlike , and uses the first row as column names.
All these functions will convert strings to factor class by default unless either  or
.
The  for use on data from countries where the comma is used as a decimal point and the semicolon as a field separator.
Importing using packages
read.csv, including stringsAsFactors = FALSEThe readr package's read_csv function offers much faster performance, a progress bar for large files, and more popular default options than standard .

Section 20.2: Importing with data.table
data.table package introduces the function fread. While it is similar to read.tableThe , fread is usually faster and more flexible, guessing the file's delimiter automatically.

"input1, input2 \n A, B \n C, D"the filename (e.g. "filename.csv"), a shell command that acts on a file (e.g. "grep 'word' filename"), or the input itself (e.g. ).
data.table that inherits from class data.framedata.tablefread returns an object of class , suitable for use with the data.table's usage of []. To return an ordinary data.frame, set the  parameter to FALSE:

Notes
read.table. One missing argument is na.comment	 fread does not have all same options as 	, which may lead
in unwanted behaviors if the source file contains #.
 fread uses only " for quote parameter. fread uses few (5) lines to guess variables types.
 Section 20.3: Exporting .csv files
Exporting using base R
write.csvData can be written to a CSV file using ():

row.names = FALSE and na = ""Commonly-specified parameters include .
Exporting using packages
readr::write_csv is significantly faster than write.csv and does not write row names.

Section 20.4: Import multiple csv files

This read every file and adds it to a list. Afterwards, if all data.frame have the same structure they can be combined into one big data.frame:

Section 20.5: Importing fixed-width files
Fixed-width files are text files in which columns are not separated by any character delimiter, like , or ;, but rather have a fixed character length (width). Data is usually padded with white spaces.
An example:

constants.txtLet's assume this data table exists in the local file  in the working directory.
Importing with base R

#> 1 1647     pi         'important'   3.14159   6.28318
#> 2 1731  euler   'quite important'   2.71828   5.43656
#> 3 1979 answer       'The Answer.'   42        42.0000
Note:
read.fwfColumn titles don't need to be separated by a character (Column4Column5)
The widths parameter defines the width of each column
Non-separated headers are not readable with ()
Importing with readr

Note:
fwf_readr's * helper functions offer alternative ways of specifying column lengths, including automatic guessing (fwf_empty) readr is faster than base R
Column titles cannot be automatically imported from data file
Chapter 21: Pipe operators (%>% and others)
	lhs	rhs
A value or the magrittr placeholder. A function call using the magrittr semantics
Pipe operators, available in magrittr, dplyr, and other R packages, process a data-object using a sequence of operations by passing the result of one step as input for the next step using infix-operators rather than the more typical R method of nested function calls.
Note that the intended aim of pipe operators is to increase human readability of written code. See Remarks section for performance considerations.
Section 21.1: Basic use and chaining
The pipe operator, %>%, is used to insert an argument into a function. It is not a base feature of the language and can only be used after attaching a package that provides it, such as magrittr. The pipe operator takes the left-hand side (LHS) of the pipe and uses it as the first argument of the function on the right-hand side (RHS) of the pipe. For example:

The pipe can be used to replace a sequence of function calls. Multiple pipes allow us to read and write the sequence from left to right, rather than from inside to out. For example, suppose we have years defined as a factor but want to convert it to a numeric. To prevent possible information loss, we first convert to character and then to numeric:

If we don't want the LHS (Left Hand Side) used as the first argument on the RHS (Right Hand Side), there are workarounds, such as naming the arguments or using . to indicate where the piped input goes.


Section 21.2: Functional sequences
Given a sequence of steps we use repeatedly, it's often handy to store it in a function. Pipes allow for saving such functions in a readable format by starting a sequence with a dot as in:

As an example, suppose we have factor dates and want to extract the year:

We can review the composition of the function by typing its name or using functions:

We can also access each function by its position in the sequence:


Generally, this approach may be useful when clarity is more important than speed.
Section 21.3: Assignment with %<>%
The magrittr package contains a compound assignment infix-operator, %<>%, that updates a value by first piping it into one or more rhs expressions and then assigning the result. This eliminates the need to type an object name twice (once on each side of the assignment operator <-). %<>% must be the first infix-operator in a chain:

Instead of writing

or

The compound assignment operator will both pipe and reassign df:

Section 21.4: Exposing contents with %$%
The exposition pipe operator, %$%, exposes the column names as R symbols within the left-hand side object to the right-hand side expression. This operator is handy when piping into functions that do not have a data argument (unlike, say, lm) and that don't take a data.frame and column names as arguments (most of the main dplyr functions).
The exposition pipe operator %$% allows a user to avoid breaking a pipeline when needing to refer to column names. For instance, say you want to filter a data.frame and then run a correlation test on two columns with
cor.test:


filtercor.testHere the standard %>% pipe passes the data.frame through to (), while the %$% pipe exposes the column names to ().
withThe exposition pipe works like a pipe-able version of the base R () functions, and the same left-hand side objects are accepted as inputs.
Section 21.5: Creating side eects with %T>%
Some functions in R produce a side effect (i.e. saving, printing, plotting, etc) and do not always return a meaningful or desired value.
>%%T (tee operator) allows you to forward a value into a side-effect-producing function while keeping the original lhs value intact. In other words: the tee operator works like %>%, except the return values is lhs itself, and not the result of the rhs function/expression.
>%Example: Create, pipe, write, and return an object. If %>% were used in place of %T in this example, then the variable all_letters would contain NULL rather than the value of the sorted object.

saveloadWarning: Piping an unnamed object to () will produce an object named . when loaded into the workspace with (). However, a workaround using a helper function is possible (which can also be written inline as an anonymous function).


Section 21.6: Using the pipe with dplyr and ggplot2
The %>% operator can also be used to pipe the dplyr output into ggplot. This creates a unified exploratory data analysis (EDA) pipeline that is easily customizable. This method is faster than doing the aggregations internally in ggplot and has the added benefit of avoiding unnecessary intermediate variables.


Chapter 22: Linear Models (Regression)
Parameter	Meaning
a formula in Wilkinson-Rogers notation; response ~ ... where ... contains terms corresponding to
formulavariables in the environment or in the data frame specified by the data argumentdatadata frame containing the response and predictor variablessubseta vector specifying a subset of observations to be used: may be expressed as a logical statement in terms of the variables in dataweightsanalytical weights (see Weights section above)na.actionmodel=TRUEna.action	how to handle missing (NA) values: see ? how to perform the fitting. Only choices are "qr" or "model.frame" (the latter returns the model frame method without fitting the model, identical to specifying )
model	whether to store the model frame in the fitted object x	whether to store the model matrix in the fitted object y	whether to store the model response in the fitted object qr	whether to store the QR decomposition in the fitted object whether to allow singular fits, models with collinear predictors (a subset of the coefficients will singular.ok automatically be set to NA in this case
contrasts.arga list of contrasts to be used for particular factors in the model; see the  argument of
model.matrix.default. Contrasts can also be set with optionscontrasts ?() (see the contrasts argument) or by
contrastsmodel.offset           assigning the contrast attributes of a factor (see ?) used to specify an a priori known component in the model. May also be specified as part of the offset formula. See ?
lm.fit() or lm.wfit...	additional arguments to be passed to lower-level fitting functions (())
Section 22.1: Linear regression on the mtcars dataset
help(mtcarsThe built-in mtcars data frame contains information about 32 cars, including their weight, fuel efficiency (in milesper-gallon), speed, etc. (To find out more about the dataset, use )).
If we are interested in the relationship between fuel efficiency (mpg) and weight (wt) we may start plotting those variables with:

The plots shows a (linear) relationship!. Then if we want to perform linear regression to determine the coefficients of a linear model, we would use the lm function:

The ~ here means "explained by", so the formula mpg ~ wt means we are predicting mpg as explained by wt. The most helpful way to view the output is with:

Which gives the output:


This provides information about:
37.2851 5.3445 wtthe estimated slope of each coefficient (wt and the y-intercept), which suggests the best-fit prediction of mpg is + (-) *
The p-value of each coefficient, which suggests that the intercept and weight are probably not due to chance Overall estimates of fit such as R^2 and adjusted R^2, which show how much of the variation in mpg is explained by the model
We could add a line to our first plot to show the predicted mpg:

It is also possible to add the equation to that plot. First, get the coefficients with coef. Then using paste0 we collapse the coefficients with appropriate variables and +/-, to built the equation. Finally, we add it to the plot using


Section 22.2: Using the 'predict' function
Once a model is built predict is the main function to test with new data. Our example will use the mtcars built-in dataset to regress miles per gallon against displacement:

If I had a new data source with displacement I could see the estimated miles per gallon.

The most important part of the process is to create a new data frame with the same column names as the original data. In this case, the original data had a column labeled disp, I was sure to call the new data that same name.
Caution
Let's look at a few common pitfalls:
1. not using a data.frame in the new object:

2. not using same names in new data frame:

Accuracy
To check the accuracy of the prediction you will need the actual y values of the new data. In this example, newdf will need a column for 'mpg' and 'disp'.

Section 22.3: Weighting
Sometimes we want the model to give more weight to some data points or examples than others. This is possible by specifying the weight for the input data while learning the model. There are generally two kinds of scenarios where we might use non-uniform weights over the examples:
 Analytic Weights: Reflect the different levels of precision of different observations. For example, if analyzing data where each observation is the average results from a geographic area, the analytic weight is proportional to the inverse of the estimated variance. Useful when dealing with averages in data by providing a proportional weight given the number of observations. Source
 Sampling Weights (Inverse Probability Weights - IPW): a statistical technique for calculating statistics standardized to a population different from that in which the data was collected. Study designs with a disparate sampling population and population of target inference (target population) are common in application. Useful when dealing with data that have missing values. Source
lmThe () function does analytic weighting. For sampling weights the survey package is used to build a survey
svyglm(). By default, the survey package uses sampling weights. (NOTE: lm(), and svyglmdesign object and run ()
gaussianwith family () will all produce the same point estimates, because they both solve for the coefficients by minimizing the weighted least squares. They differ in how standard errors are calculated.)
Test Data
data <- structure(list(lexptot = c(9.1595012302023, 9.86330744180814,
8.92372556833205, 8.58202430280175, 10.1133857229336), progvillm = c(1L, 1L, 1L, 1L, 0L), sexhead = c(1L, 1L, 0L, 1L, 1L), agehead = c(79L,
43L, 52L, 48L, 35L), weight = c(1.04273509979248, 1.01139605045319,
1.01139605045319, 1.01139605045319, 0.76305216550827)), .Names = c("lexptot",
"progvillm", "sexhead", "agehead", "weight"), class = c("tbl_df",
"tbl", "data.frame"), row.names = c(NA, -5L))
Analytic Weights

Output

Sampling Weights (IPW)

Output


Section 22.4: Checking for nonlinearity with polynomial regression
Sometimes when working with linear regression we need to check for non-linearity in the data. One way to do this is to fit a polynomial model and check whether it fits the data better than a linear model. There are other reasons, such as theoretical, that indicate to fit a quadratic or higher order model because it is believed that the variables relationship is inherently polynomial in nature.
Let's fit a quadratic model for the mtcars dataset. For a linear model see Linear regression on the mtcars dataset.
First we make a scatter plot of the variables mpg (Miles/gallon), disp (Displacement (cu.in.)), and wt (Weight (1000 lbs)). The relationship among mpg and disp appears non-linear.



dispThen, to get the result of a quadratic model, we added I(^2). The new model appears better when looking at R^2 and all variables are significant.

#Multiple R-squared:  0.8578,    Adjusted R-squared:  0.8426
As we have three variables, the fitted model is a surface represented by:
mpg = 41.4020-3.4179*wt-0.0824*disp+0.0001277*disp^2
raw=TRUEhelp(ployAnother way to specify polynomial regression is using poly with parameter , otherwise orthogonal polynomials will be considered (see the ) for more information). We get the same result using:

Finally, what if we need to show a plot of the estimated surface? Well there are many options to make 3D plots in R.
Here we use Fit3d from p3dpackage.

Section 22.5: Plotting The Regression (base)
Continuing on the mtcars example, here is a simple way to produce a plot of your linear regression that is potentially suitable for publication.
First fit the linear model and

Then plot the two variables of interest and add the regression line within the definition domain:
plot(mtcars$wt,mtcars$mpg,pch=18, xlab = 'wt',ylab = 'mpg') lines(c(min(mtcars$wt),max(mtcars$wt)),
as.numeric(predict(fit, data.frame(wt=c(min(mtcars$wt),max(mtcars$wt))))))
Almost there! The last step is to add to the plot, the regression equation, the rsquare as well as the correlation coefficient. This is done using the vector function:
rp = vector('expression',3) rp[1] = substitute(expression(italic(y) == MYOTHERVALUE3 + MYOTHERVALUE4 %*% x),           list(MYOTHERVALUE3 = format(fit$coefficients[1], digits = 2),
                        MYOTHERVALUE4 = format(fit$coefficients[2], digits = 2)))[2] rp[2] = substitute(expression(italic(R)^2 == MYVALUE),              list(MYVALUE = format(summary(fit)$adj.r.squared,dig=3)))[2] rp[3] = substitute(expression(Pearson-R == MYOTHERVALUE2),              list(MYOTHERVALUE2 = format(cor(mtcars$wt,mtcars$mpg), digits = 2)))[2] legend("topright", legend = rp, bty = 'n')
Note that you can add any other parameter such as the RMSE by adapting the vector function. Imagine you want a legend with 10 elements. The vector definition would be the following:

10and you will need to defined r[1].... to r[]
Here is the output:

Section 22.6: Quality assessment
After building a regression model it is important to check the result and decide if the model is appropriate and works well with the data at hand. This can be done by examining the residuals plot as well as other diagnostic plots.


These plots check for two assumptions that were made while building the model:
1. That the expected value of the predicted variable (in this case mpg) is given by a linear combination of the predictors (in this case wt). We expect this estimate to be unbiased. So the residuals should be centered around the mean for all values of the predictors. In this case we see that the residuals tend to be positive at the ends and negative in the middle, suggesting a non-linear relationship between the variables.
2. That the actual predicted variable is normally distributed around its estimate. Thus, the residuals should be normally distributed. For normally distributed data, the points in a normal Q-Q plot should lie on or close to the diagonal. There is some amount of skew at the ends here.

Chapter 23: data.table
Data.table is a package that extends the functionality of data frames from base R, particularly improving on their performance and syntax. See the package's Docs area at Getting started with data.table for details.
Section 23.1: Creating a data.table
class"data.table" "data.frame"A data.table is an enhanced version of the data.frame class from base R. As such, its () attribute is the vector  and functions that work on a data.frame will also work with a data.table. There are
many ways to create, load or coerce to a data.table.
Build
data.tableDon't forget to install and activate the  package

There is a constructor of the same name:

data.frame, data.tableUnlike  will not coerce strings to factors:

Read in
We can read from a text file:

read.csvUnlike , fread will read strings as strings, not as factors.
Modify a data.frame
For efficiency, data.table offers a way of altering a data.frame or list to make a data.table in-place (without making a copy or changing its memory location):

Note that we do not <- assign the result, since the object DF has been modified in-place. The class attributes of the

data.frame, or data.table, you should use the setDT function to convert to a data.tableas.data.tableIf you have a list, because it does the conversion by reference instead of making a copy (which  does). This is important if you are working with large datasets.
as.data.table to coerce it to a data.tableIf you have another R object (such as a matrix), you must use .

Section 23.2: Special symbols in data.table
.SD
SD refers to the subset of the data.table. for each group, excluding all columns used in by.
SD along with lapply can be used to apply any function to multiple columns by group in a data.table.
We will continue using the same built-in dataset, mtcars: mtcars = data.table(mtcars) # Let's not include rownames to keep things simpler Mean of all columns in the dataset by number of cylinders, cyl:

SDcolsApart from cyl, there are other categorical columns in the dataset such as vs, am, gear and carb. It doesn't really make sense to take the mean of these columns. So let's exclude these columns. This is where . comes into the picture.
.SDcols
SDcols specifies the columns of the data.table that are included in .SD..
Mean of all columns (continuous columns) in the dataset by number of gears gear, and number of cylinders, cyl, arranged by gear and cyl:

cols_chosen <- c("mpg", "disp", "hp", "drat", "wt", "qsec") mtcars[order(gear, cyl), lapply(.SD, mean), by = .(gear, cyl), .SDcols = cols_chosen]
#   gear cyl    mpg     disp       hp     drat       wt    qsec
#1:    3   4 21.500 120.1000  97.0000 3.700000 2.465000 20.0100
#2:    3   6 19.750 241.5000 107.5000 2.920000 3.337500 19.8300
#3:    3   8 15.050 357.6167 194.1667 3.120833 4.104083 17.1425
#4:    4   4 26.925 102.6250  76.0000 4.110000 2.378125 19.6125
#5:    4   6 19.750 163.8000 116.5000 3.910000 3.093750 17.6700
#6:    5   4 28.200 107.7000 102.0000 4.100000 1.826500 16.8000
#7:    5   6 19.700 145.0000 175.0000 3.620000 2.770000 15.5000
#8:    5   8 15.400 326.0000 299.5000 3.880000 3.370000 14.5500
Maybe we don't want to calculate the mean by groups. To calculate the mean for all the cars in the dataset, we don't specify the by variable.

Note:
SDcolsSDcols can also directly take a vector of columnnumbers. In the above example this would be mtcars[ ,It is not necessary to define cols_chosen beforehand. . can directly take column names
.
lapply(.SD, mean), .SDcols = c(1,3:7)]
.N

Section 23.3: Adding and modifying columns
DT[where, select|update|do, by] syntax is used to work with columns of a data.table.
The "where" part is the i argument
The "select|update|do" part is the j argument
These two arguments are usually passed by position instead of by name.
Our example data below is

Editing entire columns
Use the := operator inside j to assign new columns:

Remove columns by setting to NULL:

Add multiple columns by using the := operator's multivariate format:

If the columns are dependent and must be defined in sequence, one way is:

LHS := RHSThe .() syntax is used when the right-hand side of  is a list of columns.
For dynamically-determined column names, use parentheses:

Columns can also be modified with set, though this is rarely necessary:

Editing subsets of columns
Use the i argument to subset to rows "where" edits should be made:

As in a data.frame, we can subset using row numbers or logical tests. It is also possible to use a "join" in i, but that more complicated task is covered in another example.
Editing column attributes
levels<- or names<-Functions that edit attributes, such as , actually replace an object with a modified copy. Even if only used on one column in a data.table, the entire object is copied and replaced.
To modify an object without copies, use setnames to change the column names of a data.table or data.frame and setattr to change an attribute for any object.

Be aware that these changes are made by reference, so they are global. Changing them within one environment affects the object in all environments.
# This function also changes the levels in the global environment edit_levels <- function(x) setattr(x, "levels", c("low", "med", "high")) edit_levels(mtcars$cyl_factor)
Section 23.4: Writing code compatible with both data.frame and data.table
Differences in subsetting syntax
data.table is one of several two-dimensional data structures available in R, besides data.framerows, colsA , matrix and (2D) array. All of these classes use a very similar but not identical syntax for subsetting, the A[] schema.
data.frame and a data.tableConsider the following data stored in a matrix, a :

If you want to be sure of what will be returned, it is better to be explicit.
To get specific rows, just add a comma after the range:

But, if you want to subset columns, some cases are interpreted differently. All three can be subset the same way with integer or character indices not stored in a variable.

However, they differ for unquoted variable names

In the last case, mycols is evaluated as the name of a column. Because dt cannot find a column named mycols, an error is raised.
data.tableNote: For versions of the  package priorto 1.9.8, this behavior was slightly different. Anything in the
dt[, 2:3] and dt[, mycolscolumn index would have been evaluated using dt as an environment. So both ] would return the vector 2:3. No error would be raised for the second case, because the variable mycols does exist in the parent environment.
Strategies for maintaining compatibility with data.frame and data.table
data.frame and data.tabledata.frameThere are many reasons to write code that is guaranteed to work with . Maybe you are forced to use , or you may need to share some code that you don't know how will be used. So, there are some main strategies for achieving this, in order of convenience:
1. Use syntax that behaves the same for both classes.
2. Use a common function that does the same thing as the shortest syntax.
data.table to behave as data.frame (ex.: call the specific method print.data.frame3. Force ).
4. Treat them as list, which they ultimately are.
data.frame5. Convert the table to a  before doing anything (bad idea if it is a huge table).
data.table6. Convert the table to , if dependencies are not a concern.
, Subset rows. Its simple, just use the [] selector, with the comma:

If you want a uniform way to grab more than one column, it's necessary to appeal a bit:

data.frame has row.names, data.tableSubset 'indexed' rows. While  has its unique key feature. The best thing is
row.names entirely and take advantage of the existing optimizations in the case of data.tableto avoid  when possible.

Get a 1-column table, get a row as a vector. These are easy with what we have seen until now:
B <- select(A, 2)    #---> a table with just the second column
C <- unlist(A[1, ])  #---> the first row as a vector (coerced if necessary)
Section 23.5: Setting keys in data.table
Yes, you need to SETKEY pre 1.9.6
data.tableIn the past (pre 1.9.6), your  was sped up by setting columns as keys to the table, particularly for large tables. [See intro vignette page 5 of September 2015 version, where speed of search was 544 times better.] You may find older code making use of this setting keys with 'setkey' or setting a 'key=' column when setting up the table.

Set your key with the setkey command. You can have a key with multiple columns.

Check your table's key in tables()

Note this will re-sort your data.

Now it is unnecessary
Prior to v1.9.6 you had to have set a key for certain operations especially joining tables. The developers of data.table have sped up and introduced a "on=" feature that can replace the dependency on keys. See SO answer here for a detailed discussion.
In Jan 2017, the developers have written a vignette around secondary indices which explains the "on" syntax and allows for other columns to be identified for fast indexing.
Creating secondary indices?
setindex(DT, key.col) or setindexv(DT, "key.col.string"setindex(DT, NULLIn a manner similar to key, you can ), where DT is your data.table. Remove all indices with ).
indices(DTSee your secondary indices with ).
Why secondary indices?
This does not sort the table (unlike key), but does allow for quick indexing using the "on" syntax. Note there can be only one key, but you can use multiple secondary indices, which saves having to rekey and resort the table. This will speed up your subsetting when changing the columns you want to subset on.
Recall, in example above y was the key for table DT:

Chapter 24: Pivot and unpivot with data.table
Parameter	Details
id.vars	tell melt which columns to retain variable.name tell melt what to call the column with category labels
value.nametell melt what to call the column that has values associated with category labelsvalue.vartell dcast where to find the values to cast in columnsformulatell dcast which columns to retain to form a unique record identifier (LHS) and which one holds thecategory labels (RHS)
fun.aggregate specify the function to use when the casting operation generates a list of values in each cell
Section 24.1: Pivot and unpivot tabular data with data.table - I
Convert from wide form to long form

USArrests to find out more. First, convert to data.tableUse ?. The names of states are row names in the original
data.frame.

This is data in the wide form. It has a column for each variable. The data can also be stored in long form without loss of information. The long form has one column that stores the variable names. Then, it has another column for the variable values. The long form of USArrests looks like so.

We use the melt function to switch from wide form to long form.

id.varsBy default, melt treats all columns with numeric data as variables with values. In USArrests, the variable UrbanPop represents the percentage urban population of a state. It is different from the other variables, Murder, Assault and Rape, which are violent crimes reported per 100,000 people. Suppose we want to retain UrbanPop column. We achieve this by setting  as follows.

variable.name and the column containing the values with value.nameNote that we have specified the names of the column containing category names (Murder, Assault, etc.) with . Our data looks like so.

Generating summaries with with split-apply-combine style approach is a breeze. For example, to summarize violent crimes by state?

This gives:

Section 24.2: Pivot and unpivot tabular data with data.table II
Convert from long form to wide form
To recover data from the previous example, use dcast like so.

This gives the data in the original wide form.
             State UrbanPop Murder Assault Rape
 1:        Alabama       58   13.2     236 21.2
 2:         Alaska       48   10.0     263 44.5
 3:        Arizona       80    8.1     294 31.0
 4:       Arkansas       50    8.8     190 19.5  5:     California       91    9.0     276 40.6
value.varHere, the formula notation is used to specify the columns that form a unique record identifier (LHS) and the column containing category labels for new column names (RHS). Which column to use for the numeric values? By default, dcast uses the first column with numerical values left over when from the formula specification. To make explicit, use the parameter  with column name.
fun.aggregateWhen the operation produces a list of values in each cell, dcast provides a  method to handle the situation. Say I am interested in states with similar urban population when investigating crime rates. I add a column Decile with computed information.
DTmu[, Decile := cut(UrbanPop, quantile(UrbanPop, probs = seq(0, 1, by=0.1)))] levels(DTmu$Decile) <- paste0(1:10, "D")
fun.aggregateNow, casting Decile ~ Crime produces multiple values per cell. I can use  to determine how these are handled. Both text and numerical values can be handle this way.

This gives:

This gives:
             State UrbanPop  Crime Rate Decile
  1:       Alabama       58 Murder 13.2     4D
  2:        Alaska       48 Murder 10.0     2D
  3:       Arizona       80 Murder  8.1     8D
  4:      Arkansas       50 Murder  8.8     2D   5:    California       91 Murder  9.0    10D
fun.aggregateThere are multiple states in each decile of the urban population. Use  to specify how these should be handled.

This sums over the data for like states, giving the following.

Chapter 25: Bar Chart
The purpose of the bar plot is to display the frequencies (or proportions) of levels of a factor variable. For example, a bar plot is used to pictorially display the frequencies (or proportions) of individuals in various socio-economic (factor) groups(levels-high, middle, low). Such a plot will help to provide a visual comparison among the various factor levels.
Section 25.1: barplot() function
In barplot, factor-levels are placed on the x-axis and frequencies (or proportions) of various factor-levels are considered on the y-axis. For each factor-level one bar of uniform width with heights being proportional to factor level frequency (or proportion) is constructed.
barplot() function is in the graphics package of the R's System Library. The barplotThe () function must be
supplied at least one argument. The R help calls this as heights, which must be either vector or a matrix. If it is vector, its members are the various factor-levels.
barplotTo illustrate (), consider the following data preparation:

A bar chart of the Marks vector is obtained from


lexicographical ordernames.argNotice that, the barplot() function places the factor levels on the x-axis in the  of the levels. Using the parameter , the bars in plot can be placed in the order as stated in the vector, grades.
# plot to the desired horizontal axis labels
> barplot(table(Marks),names.arg=grades ,main="Mid-Marks in Algorithms")

colColored bars can be drawn using the = parameter.
> barplot(table(Marks),names.arg=grades,col = c("lightblue",         "lightcyan", "lavender", "mistyrose",  "cornsilk"),          main="Mid-Marks in Algorithms")

A bar chart with horizontal bars can be obtained as follows:



A bar chart with proportions on the y-axis can be obtained as follows:
> barplot(prop.table(table(Marks)),names.arg=grades,col = c("lightblue",            "lightcyan", "lavender", "mistyrose",  "cornsilk"),             main="Mid-Marks in Algorithms")

cex.namesThe sizes of the factor-level names on the x-axis can be increased using  parameter.
> barplot(prop.table(table(Marks)),names.arg=grades,col = c("lightblue",           "lightcyan", "lavender", "mistyrose",  "cornsilk"),            main="Mid-Marks in Algorithms",cex.names=2)

barplotThe heights parameter of the () could be a matrix. For example it could be matrix, where the columns are the various subjects taken in a course, the rows could be the labels of the grades. Consider the following matrix:

To draw a stacked bar, simply use the command:
> barplot(gradTab,col = c("lightblue","lightcyan",        "lavender", "mistyrose",  "cornsilk"),legend.text = grades,         main="Mid-Marks in Algorithms")

To draw a juxtaposed bars, use the besides parameter, as given under:
 > barplot(gradTab,beside = T,col = c("lightblue","lightcyan",        "lavender", "mistyrose",  "cornsilk"),legend.text = grades,         main="Mid-Marks in Algorithms")

horizA horizontal bar chart can be obtained using =T parameter:
> barplot(gradTab,beside = T,horiz=T,col = c("lightblue","lightcyan",        "lavender", "mistyrose",  "cornsilk"),legend.text = grades,         cex.names=.75,main="Mid-Marks in Algorithms")


Chapter 26: Base Plotting
Parameter	Details
data$variablex or data[,xdata$variabley or data[,yx	x-axis variable. May supply either ] y	y-axis variable. May supply either ] main	Main title of plot sub	Optional subtitle of plot xlab	Label for x-axis ylab	Label for y-axis pch	Integer or character indicating plotting symbol col	Integer or string indicating color
Type of plot. "p" for points, "l" for lines, "b" for both, "c" for the lines part alone of "b", "o" for both
type	'overplotted', "h" for 'histogram'-like (or 'high-density') vertical lines, "s" for stair steps, "S" for other steps, "n" for no plotting
Section 26.1: Density plot
A very useful and logical follow-up to histograms would be to plot the smoothed density function of a random variable. A basic plot produced by the command

would look like

You can overlay a histogram and a density curve with

which gives

Section 26.2: Combining Plots
par() and layoutIt's often useful to combine multiple plot types in one graph (for example a Barplot next to a Scatterplot.) R makes this easy with the help of the functions ().
parnrows, ncols() par uses the arguments mfrow or mfcol to create a matrix of nrows and ncols c() which will serve as a grid for your plots. The following example shows how to combine four plots in one graph:


layout()
layoutThe () is more flexible and allows you to specify the location and the extent of each plot within the final combined graph. This function expects a matrix object as an input:


Section 26.3: Getting Started with R_Plots
 Scatterplot
You have two vectors and you want to plot them.
x_values <- rnorm(n = 20 , mean = 5 , sd = 8) #20 values generated from Normal(5,8) y_values <- rbeta(n = 20 , shape1 = 500 , shape2 = 10) #20 values generated from Beta(500,10)
If you want to make a plot which has the y_values in vertical axis and the x_valuesin horizontal axis, you can use the following commands:

plot(x = x_values, y = y_values, type = "n") # empty plot
plotYou can type ?() in the console to read about more options.
 Boxplot
You have some variables and you want to examine their Distributions

Easy way to draw histograms

If you want to visualize the frequencies of a variable just draw pie First we have to generate data with frequencies, for example :
P <- c(rep('A' , 3) , rep('B' , 10) , rep('C' , 7) ) t <- table(P) # this is a frequency matrix of variable P pie(t) # And this is a visual version of the matrix above
Section 26.4: Basic Plot
plotA basic plot is created by calling (). Here we use the built-in cars data frame that contains the speed of cars and the distances taken to stop in the 1920s. (To find out more about the dataset, use help(cars)).


We can use many other variations in the code to get the same result. We can also change the parameters to obtain different results.


points(), text(), mtext(), lines(), gridAdditional features can be added to this plot by calling (), etc.


Section 26.5: Histograms
Histograms allow for a pseudo-plot of the underlying distribution of the data.



Section 26.6: Matplot
matplot is useful for quickly plotting multiple sets of observations from the same object, particularly from a matrix, on the same graph.
Here is an example of a matrix containing four sets of random draws, each with a different mean.
xmat <- cbind(rnorm(100, -3), rnorm(100, -1), rnorm(100, 1), rnorm(100, 3)) head(xmat) #          [,1]        [,2]       [,3]     [,4]
# [1,] -3.072793 -2.53111494  0.6168063 3.780465 # [2,] -3.702545 -1.42789347 -0.2197196 2.478416 # [3,] -2.890698 -1.88476126  1.9586467 5.268474
# [4,] -3.431133 -2.02626870  1.1153643 3.170689 # [5,] -4.532925  0.02164187  0.9783948 3.162121 # [6,] -2.169391 -1.42699116  0.3214854 4.480305
One way to plot all of these observations on the same graph is to do one plot call followed by three more points or lines calls.


However, this is both tedious, and causes problems because, among other things, by default the axis limits are fixed by plot to fit only the first column.
Much more convenient in this situation is to use the matplot function, which only requires one call and automatically takes care of axis limits and changing the aesthetics for each column to make them distinguishable.


Note that, by default, matplot varies both color (col) and linetype (lty) because this increases the number of possible combinations before they get repeated. However, any (or both) of these aesthetics can be fixed to a single value...


...or a custom vector (which will recycle to the number of columns, following standard R vector recycling rules). matplot(xmat, type = 'l', col = c('red', 'green', 'blue', 'orange'))

parStandard graphical parameters, including main, xlab, xmin, work exactly the same way as for plot. For more on those, see ?.
Like plot, if given only one object, matplot assumes it's the y variable and uses the indices for x. However, x and y can be specified explicitly.




Section 26.7: Empirical Cumulative Distribution Function
ecdf()A very useful and logical follow-up to histograms and density plots would be the Empirical Cumulative Distribution Function. We can use the function  for this purpose. A basic plot produced by the command

would look like


Chapter 27: boxplot
Parameters	Details (source R Documentation)
formulato the grouping variable grp (usually a factor).dataa data.frame (or list) from which the variables in formula should be taken.subsetan optional vector specifying a subset of observations to be used for plotting.na.actiona function which indicates what should happen when the data contain NAs. The default is to ignore missing values in either the response or the group.boxwexa scale factor to be applied to all boxes. When there are only a few groups, the appearance of the plot can be improved by making the boxes narrower.plotif TRUE (the default) then a boxplot is produced. If not, the summaries which the boxplots are based on are returned.colif col is non-null it is assumed to contain colors to be used to colour the bodies of the box plots. Bya formula, such as y ~ grp, where y is a numeric vector of data values to be split into groups according default they are in the background colour.
Section 27.1: Create a box-and-whisker plot with boxplot() {graphics}
boxplotThis example use the default () function and the irisdata frame.
> head(iris)
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1 5.1         3.5          1.4         0.2  setosa
2 4.9         3.0          1.4         0.2  setosa
3 4.7         3.2          1.3         0.2  setosa
4 4.6         3.1          1.5         0.2  setosa
5 5.0         3.6          1.4         0.2  setosa
 6          5.4         3.9          1.7         0.4  setosa Simple boxplot (Sepal.Length)
Create a box-and-whisker graph of a numerical variable
boxplot(iris[,1],xlab="Sepal.Length",ylab="Length(in centemeters)",            main="Summary Charateristics of Sepal.Length(Iris Data)")

Boxplot of sepal length grouped by species
Create a boxplot of a numerical variable grouped by a categorical variable


Bring order
To change order of the box in the plot you have to change the order of the categorical variable's levels.
virginica - versicolor - setosaFor example if we want to have the order 
newSpeciesOrder <- factor(iris$Species, levels=c("virginica","versicolor","setosa")) boxplot(Sepal.Length~newSpeciesOrder,data = iris)

Change groups names
If you want to specifie a better name to your groups you can use the Names parameter. It take a vector of the size of the levels of categorical variable
boxplot(Sepal.Length~newSpeciesOrder,data = iris,names= c("name1","name2","name3"))

Small improvements Color
col : add a vector of the size of the levels of categorical variable
boxplot(Sepal.Length~Species,data = iris,col=c("green","yellow","orange"))

Proximity of the box
boxwex: set the margin between boxes.
boxplot(Sepal.Length~Species,data = iris,boxwex = 0.1Left )
Right 
plot=FALSESee the summaries which the boxplots are based 
To see a summary you have to put the paramater plot to FALSE.
Various results are given

Section 27.2: Additional boxplot style parameters
Box
boxlty - box line type boxlwd - box line width boxcol - box line color boxfill - box fill colors
Median
medlty - median line type ("blank" for no line) medlwd - median line widht medcol - median line color medpch - median point (NA for no symbol) medcex - median point size medbg - median point background color
Whisker
whisklty - whisker line type whisklwd - whisker line width whiskcol - whisker line color
Staple
staplelty - staple line type staplelwd - staple line width staplecol - staple line color
Outliers
outlty - outlier line type ("blank" for no line) outlwd - outlier line width outcol - outlier line color outpch - outlier point type (NA for no symbol) outcex - outlier point size outbg - outlier point background color
Example
Default and heavily modified plots side by side



Chapter 28: ggplot2
Section 28.1: Displaying multiple plots
Display multiple plots in one image with the different facet functions. An advantage of this method is that all axes share the same scale across charts, making it easy to compare them at a glance. We'll use the mpg dataset included in ggplot2.
Wrap charts line by line (attempts to create a square layout):


Display multiple charts on one row, multiple columns:



Display multiple charts on one column, multiple rows:



Display multiple charts in a grid by 2 variables:


Section 28.2: Prepare your data for plotting
ggplot2 works best with a long data frame. The following sample data which represents the prices for sweets on 20 different days, in a format described as wide, because each category has a column.

data.tableTo convert sweetsWide to long format for use with ggplot2, several useful functions from base R, and the packages reshape2,  and tidyr (in chronological order) can be used:


The all give a similar result:

See also Reshaping data between long and wide forms for details on converting data between long and wide format.
The resulting sweetsLong has one column of prices and one column describing the type of sweet. Now plotting is much simpler:


Section 28.3: Add horizontal and vertical lines to plot
Add one common horizontal line for all categorical variables

Add one horizontal line for each categorical variable


Add horizontal line over grouped bars


Add vertical line

Section 28.4: Scatter Plots
We plot a simple scatter plot using the builtin iris data set as follows:

This gives:

Section 28.5: Produce basic plots with qplot
plot()qplot is intended to be similar to base r  function, trying to always plot out your data without requiring too much specifications.
basic qplot


adding colors

adding a smoother
qplot(x = disp, y = mpg, geom = c("point", "smooth"), data = mtcars)

Section 28.6: Vertical and Horizontal Bar Chart



it is possible to obtain an horizontal bar chart simply adding coord_flip() aesthetic to the ggplot object:


Section 28.7: Violin plot
Violin plots are kernel density estimates mirrored in the vertical plane. They can be used to visualize several distributions side-by-side, with the mirroring helping to highlight any differences.


Violin plots are named for their resemblance to the musical instrument, this is particularly visible when they are coupled with an overlaid boxplot. This visualisation then describes the underlying distributions both in terms of Tukey's 5 number summary (as boxplots) and full continuous density estimates (violins).


Chapter 29: Factors
Section 29.1: Consolidating Factor Levels with a List
There are times in which it is desirable to consolidate factor levels into fewer groups, perhaps because of sparse data in one of the categories. It may also occur when you have varying spellings or capitalization of the category names. Consider as an example the factor

Since R is case-sensitive, a frequency table of this vector would appear as below.

This table, however, doesn't represent the true distribution of the data, and the categories may effectively be reduced to three types: Blue, Green, and Red. Three examples are provided. The first illustrates what seems like an obvious solution, but won't actually provide a solution. The second gives a working solution, but is verbose and computationally expensive. The third is not an obvious solution, but is relatively compact and computationally efficient.
Consolidating levels using factor (factor_approach)
factor(as.character(colorful),        levels = c("blue", "Blue", "BLUE", "green", "gren", "red", "Red", "RED"),        labels = c("Blue", "Blue", "Blue", "Green", "Green", "Red", "Red", "Red"))
 [1] Green Blue  Red   Red   Blue  Red   Red   Red   Blue  Red   Green Green Green Blue
 Red   Green
[17] Red   Green Green Red  
Levels: Blue Blue Blue Green Green Red Red Red Warning message:
In `levels<-`(`*tmp*`, value = if (nl == nL) as.character(labels) else paste0(labels,  :   duplicated levels in factors are deprecated
Notice that there are duplicated levels. We still have three categories for "Blue", which doesn't complete our task of consolidating levels. Additionally, there is a warning that duplicated levels are deprecated, meaning that this code may generate an error in the future.


This code generates the desired result, but requires the use of nested ifelse statements. While there is nothing wrong with this approach, managing nested ifelse statements can be a tedious task and must be done carefully.
Consolidating Factors Levels with a List (list_approach)
A less obvious way of consolidating levels is to use a list where the name of each element is the desired category name, and the element is a character vector of the levels in the factor that should map to the desired category. This has the added advantage of working directly on the levels attribute of the factor, without having to assign new objects.

Benchmarking each approach
The time required to execute each of these approaches is summarized below. (For the sake of space, the code to generate this summary is not shown)
Unit: microseconds           expr     min      lq      mean   median      uq     max neval cld         factor  78.725  83.256  93.26023  87.5030  97.131 218.899   100  b         ifelse 104.494 107.609 123.53793 113.4145 128.281 254.580   100   c  list_approach  49.557  52.955  60.50756  54.9370  65.132 138.193   100 a
The list approach runs about twice as fast as the ifelse approach. However, except in times of very, very large amounts of data, the differences in execution time will likely be measured in either microseconds or milliseconds. With such small time differences, efficiency need not guide the decision of which approach to use. Instead, use an approach that is familiar and comfortable, and which you and your collaborators will understand on future review.
Section 29.2: Basic creation of factors
Factors are one way to represent categorical variables in R. A factor is stored internally as a vector of integers. The unique elements of the supplied character vector are known as the levels of the factor. By default, if the levels are not supplied by the user, then R will generate the set of unique values in the vector, sort these values alphanumerically, and use them as the levels.


If you want to change the ordering of the levels, then one option to to specify the levels manually:

Factors have a number of properties. For example, levels can be given labels:

Another property that can be assigned is whether the factor is ordered:
> Weekdays <- factor(c("Monday", "Wednesday", "Thursday", "Tuesday", "Friday", "Sunday",
"Saturday"))
> Weekdays
[1] Monday    Wednesday Thursday  Tuesday   Friday    Sunday    Saturday
Levels: Friday Monday Saturday Sunday Thursday Tuesday Wednesday
> Weekdays <- factor(Weekdays, levels=c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday",
"Saturday", "Sunday"), ordered=TRUE)
> Weekdays
[1] Monday    Wednesday Thursday  Tuesday   Friday    Sunday    Saturday
Levels: Monday < Tuesday < Wednesday < Thursday < Friday < Saturday < Sunday
droplevelsWhen a level of the factor is no longer used, you can drop it using the () function:

Section 29.3: Changing and reordering factors
as.characterWhen factors are created with defaults, levels are formed by  applied to the inputs and are ordered alphabetically.

In some situations the treatment of the default ordering of levels (alphabetic/lexical order) will be acceptable. For example, if one justs want to plot the frequencies, this will be the result:


But if we want a different ordering of levels, we need to specify this in the levels or labels parameter (taking care that the meaning of "order" here is different from ordered factors, see below). There are many alternatives to accomplish that task depending on the situation.
1. Redefine the factor
When it is possible, we can recreate the factor using the levels parameter with the order we want.

When the input levels are different than the desired output levels, we use the labels parameter which causes the levels parameter to become a "filter" for acceptable input values, but leaves the final values of "levels" for the factor vector as the argument to labels:


2. Use relevel function
When there is one specific level that needs to be the first we can use relevel. This happens, for example, in the context of statistical analysis, when a base category is necessary for testing hypothesis.

As can be verified f and g are the same

3. Reordering factors
There are cases when we need to reorder the levels based on a number, a partial result, a computed statistic, or previous calculations. Let's reorder based on the frequencies of the levels

help(reorderThe reorder function is generic (see )), but in this context needs: x, in this case the factor; X, a numeric value of the same length as x; and FUN, a function to be applied to X and computed by level of the x, which determines the levels order, by default increasing. The result is the same factor with its levels reordered.

To get de decreasing order we consider negative values (-1)

Again the factor is the same as the others.


help("iris"Sepal.WidthWhen there is a quantitative variable related to the factor variable, we could use other functions to reorder the levels. Lets take the iris data () for more information), for reordering the Species factor by using its mean .

with(miris, boxplot(Petal.Width~SpeciesSepal.WidthThe usual boxplot (say: )) will show the especies in this order: setosa, versicolor, and virginica. But using the ordered factor we get the species ordered by its mean :



Additionally, it is also possible to change the names of levels, combine them into groups, or add new levels. For that we use the function of the same name levels.


- Ordered factors
Finally, we know that ordered factors are different from factors, the first one are used to represent ordinal data, and the second one to work with nominal data. At first, it does not make sense to change the order of levels for ordered factors, but we can change its labels.

Section 29.4: Rebuilding factors from zero
Problem
Factors are used to represent variables that take values from a set of categories, known as Levels in R. For example, some experiment could be characterized by the energy level of a battery, with four levels: empty, low, normal, and full. Then, for 5 different sampling sites, those levels could be identified, in those terms, as follows:
		full, full, normal, empty, low
Typically, in databases or other information sources, the handling of these data is by arbitrary integer indices associated with the categories or levels. If we assume that, for the given example, we would assign, the indices as follows: 1 = empty, 2 = low, 3 = normal, 4 = full, then the 5 samples could be coded as:
		4, 4, 3, 1, 2
It could happen that, from your source of information, e.g. a database, you only have the encoded list of integers, and the catalog associating each integer with each level-keyword. How can a factor of R be reconstructed from that information? Solution
We will simulate a vector of 20 integers that represents the samples, each of which may have one of four different values:

The first step is to make a factor, from the previous sequence, in which the levels or categories are exactly the numbers from 1 to 4.

Now simply, you have to dress the factor already created with the index tags:

Chapter 30: Pattern Matching and Replacement
This topic covers matching string patterns, as well as extracting or replacing them. For details on defining complicated patterns see Regular Expressions.
Section 30.1: Finding Matches

Is there a match?
grepl() is used to check whether a word or regular expression exists in a string or character vector. The function returns a TRUE/FALSE (or "Boolean") vector.
Notice that we can check each string for the word "fox" and receive a Boolean vector in return.

Match locations
grep takes in a character string and a regular expression. It returns a numeric vector of indexes.This will return which sentence contains the word "fox" in it.

Matched values
To select sentences that match a pattern:

Details
fixed = TRUESince the "fox" pattern is just a word, rather than a regular expression, we could improve performance (with either grep or grepl) by specifying .

invert = TRUEgrep(...) or !grepl(...To select sentences that don't match a pattern, one can use grep with ; or follow subsetting rules with -).
grepl(pattern, x) and grep(pattern, xpattern[1] against x[1], patternIn both ), the x parameter is vectorized, the pattern parameter is not. As a result, you cannot use these directly to match [2] against x[2], and so on.
Summary of matches
After performing the e.g. the grepl command, maybe you want to get an overview about how many matches where TRUE or FALSE. This is useful e.g. in case of big data sets. In order to do so run the summary command:

Section 30.2: Single and Global match
When working with regular expressions one modifier for PCRE is g for global match.
In R matching and replacement functions have two version: first match and global match:
sub(pattern,replacement,textgsub(pattern,replacement,textregexpr(pattern,textgregexpr(pattern,text) will replace the first occurrence of pattern by replacement in text
) will do the same as sub but for each occurrence of pattern
) will return the position of match for the first instance of pattern ) will return all matches.
Some random data:

Let's see how this works if we want to replace vowels by something else:

Now let's see how we can find a consonant immediately followed by one or more vowel:

We have a match on position 3 of the string of length 2, i.e: ju
Now if we want to get all matches:


All this is really great, but this only give use positions of match and that's not so easy to get what is matched, and here comes regmatches it's sole purpose is to extract the string matched from regexpr, but it has a different syntax.
Let's save our matches in a variable and then extract them from original string:

This may sound strange to not have a shortcut, but this allow extraction from another string by the matches of our first one (think comparing two long vector where you know there's is a common pattern for the first but not for the second, this allow an easy comparison):

perl=TRUEAttention note: by default the pattern is not Perl Compatible Regular Expression, some things like lookarounds are not supported, but each function presented here allow for  argument to enable them.
Section 30.3: Making substitutions

Let's make the brown fox red:

Now, let's make the "fast" fox act "fastly". This won't do it:

sub only makes the first available replacement, we need gsub for global replacement:

See Modifying strings by substitution for more examples.
Section 30.4: Find matches in big data sets
grepl("fox", test_sentencesIn case of big data sets, the call of ) does not perform well. Big data sets are e.g. crawled websites or million of Tweets, etc.
perl = TRUE option. Even faster is the option fixed = TRUEThe first acceleration is the usage of the . A complete example would be:

In case of text mining, often a corpus gets used. A corpus cannot be used directly with grepl. Therefore, consider this function:


Chapter 31: Run-length encoding
Section 31.1: Run-length Encoding with `rle`
Run-length encoding captures the lengths of runs of consecutive elements in a vector. Consider an example vector:

The rle function extracts each run and its length:

The values for each run are captured in r$values:

This captures that we first saw a run of 1's, then a run of 2's, then a run of 3's, then a run of 1's, and so on.
The lengths of each run are captured in r$lengths:

We see that the initial run of 1's was of length 1, the run of 2's that followed was of length 3, and so on.
Section 31.2: Identifying and grouping by runs in base R
One might want to group their data by the runs of a variable and perform some sort of analysis. Consider the following simple dataset:

The variable x has three runs: a run of length 2 with value 1, a run of length 3 with value 2, and a run of length 1 with value 1. We might want to compute the mean value of variable y in each of the runs of variable x (these mean values are 1.5, 4, and 6).
In base R, we would first compute the run-length encoding of the x variable using rle:

The next step is to compute the run number of each row of our dataset. We know that the total number of runs is
length(r$lengths), and the length of each run is r$lengths, so we can compute the run number of each of our
runs with rep:

Now we can use tapply to compute the mean y value for each run by grouping on the run id:

Section 31.3: Run-length encoding to compress and decompress vectors
Long vectors with long runs of the same value can be significantly compressed by storing them in their run-length encoding (the value of each run and the number of times that value is repeated). As an example, consider a vector of length 10 million with a huge number of 1's and only a small number of 0's:

Storing 10 million entries will require significant space, but we can instead create a data frame with the run-length encoding of this vector:

rle.dfwrite.csvFrom the run-length encoding, we see that the first 52,818 values in the vector are 1's, followed by a single 0, followed by 219,329 consecutive 1's, followed by a 0, and so on. The run-length encoding only has 207 entries, requiring us to store only 414 values instead of 10 million values. As  is a data frame, it can be stored using standard functions like .
Decompressing a vector in run-length encoding can be accomplished in two ways. The first method is to simply call rep, passing the values element of the run-length encoding as the first argument and the lengths element of the run-length encoding as the second argument:

We can confirm that our decompressed data is identical to our original data:

inverse.rleThe second method is to use R's built-in  function on the rle object, for instance:

We can confirm again that this produces exactly the original dat:

Section 31.4: Identifying and grouping by runs in data.table
The data.table package provides a convenient way to group by runs in data. Consider the following example data:

The variable x has three runs: a run of length 2 with value 1, a run of length 3 with value 2, and a run of length 1 with value 1. We might want to compute the mean value of variable y in each of the runs of variable x (these mean values are 1.5, 4, and 6).
The data.table rleid function provides an id indicating the run id of each element of a vector:

One can then easily group on this run ID and summarize the y data:

Chapter 32: Speeding up tough-tovectorize code
Section 32.1: Speeding tough-to-vectorize for loops with Rcpp
cos(x_{i-1} + 1)Consider the following tough-to-vectorize for loop, which creates a vector of length len where the first element is specified (first) and each element x_i is equal to :

cos(x[i-1]+1)This code involves a for loop with a fast operation (), which often benefit from vectorization.
However, it is not trivial to vectorize this operation with base R, since R does not have a "cumulative cosine of x+1" function.
One possible approach to speeding this function would be to implement it in C++, using the Rcpp package:

This often provides significant speedups for large computations while yielding the exact same results:

In this case, the Rcpp code generates a vector of length 1 million in 0.03 seconds instead of 1.31 seconds with the base R approach.
Section 32.2: Speeding tough-to-vectorize for loops by byte compiling
Following the Rcpp example in this documentation entry, consider the following tough-to-vectorize function, which creates a vector of length len where the first element is specified (first) and each element x_i is equal to
cos(x_{i-1} + 1):


One simple approach to speeding up such a function without rewriting a single line of code is byte compiling the code using the R compile package:

The resulting function will often be significantly faster while still returning the same results:

In this case, byte compiling sped up the tough-to-vectorize operation on a vector of length 1 million from 1.20 seconds to 0.34 seconds.
Remark
The essence of repeatedCosPlusOne, as the cumulative application of a single function, can be expressed more transparently with Reduce:

repeatedCosPlusOne_vec may be regarded as a "vectorization" of repeatedCosPlusOne. However, it can be expected to be slower by a factor of 2:

Chapter 33: Introduction to Geographical Maps
See also I/O for geographic data
Section 33.1: Basic map-making with map() from the package maps
map()The function  from the package maps provides a simple starting point for creating maps with R.
A basic world map can be drawn as follows:


The color of the outline can be changed by setting the color parameter, col, to either the character name or hex value of a color:


fill = TRUETo fill land masses with the color in col we can set :


fill = TRUEA vector of any length may be supplied to col when  is also set:


In the example above colors from col are assigned arbitrarily to polygons in the map representing regions and colors are recycled if there are fewer colors than polygons.
We can also use color coding to represent a statistical variable, which may optionally be described in a legend. A map created as such is known as a "choropleth".
mapcounty.fipsThe following choropleth example sets the first argument of (), which is database to "county" and "state" to color code unemployment using data from the built-in datasets unemp and  while overlaying state lines in white:

  leg.txt <- c("<2%", "2-4%", "4-6%", "6-8%", "8-10%", ">10%")  
  # align data with map definitions by (partial) matching state,county
  # names, which include multiple polygons for some counties   cnty.fips <- county.fips$fips[match(map("county", plot=FALSE)$names,                                       county.fips$polyname)]   colorsmatched <- unemp$colorBuckets[match(cnty.fips, unemp$fips)]    # draw map   par(mar=c(1, 1, 2, 1) + 0.1)   map("county", col = colors[colorsmatched], fill = TRUE, resolution = 0,       lty = 0, projection = "polyconic")   map("state", col = "white", fill = FALSE, add = TRUE, lty = 1, lwd = 0.1,       projection="polyconic")   title("unemployment by county, 2009")   legend("topright", leg.txt, horiz = TRUE, fill = colors, cex=0.6) }

Section 33.2: 50 State Maps and Advanced Choropleths with Google Viz
A common question is how to juxtapose (combine) physically separate geographical regions on the same map, such as in the case of a choropleth describing all 50 American states (The mainland with Alaska and Hawaii juxtaposed). Creating an attractive 50 state map is simple when leveraging Google Maps. Interfaces to Google's API include the


gvisGeoChartmapThe function () requires far less coding to create a choropleth compared to older mapping methods, such as () from the package maps. The colorvar parameter allows easy coloring of a statistical variable, at a level specified by the locationvar parameter. The various options passed to options as a list allow customization of the map's details such as size (height), shape (markers), and color coding (colorAxis and colors).
Section 33.3: Interactive plotly maps
plot_ly() or ggplotlyplot_geo() or plot_mapboxThe plotly package allows many kind of interactive plots, including maps. There are a few ways to create a map in plotly. Either supply the map data yourself (via ()), use plotly's "native" mapping capabilities (via ()), or even a combination of both. An example of supplying the map yourself would be:


plot_ly() for plot_geo() or plot_mapboxFor a combination of both approaches, swap () in the above example.
See the plotly book for more examples.
world.citiesThe next example is a "strictly native" approach that leverages the layout.geo attribute to set the aesthetics and zoom level of the map. It also uses the database  from maps to filter the Brazilian cities and plot them on top of the "native" map.
The main variables: pophis a text with the city and its population (which is shown upon mouse hover); qis a ordered factor from the population's quantile. ge has information for the layout of the maps. See the package documentation for more information.


Section 33.4: Making Dynamic HTML Maps with Leaflet
Leaflet is an open-source JavaScript library for making dynamic maps for the web. RStudio wrote R bindings for Leaflet, available through its leaflet package, built with htmlwidgets. Leaflet maps integrate well with the RMarkdown and Shiny ecosystems.
leafletleafletThe interface is piped, using a () function to initialize a map and subsequent functions adding (or removing) map layers. Many kinds of layers are available, from markers with popups to polygons for creating choropleth maps. Variables in the data.frame passed to () are accessed via function-style ~ quotation.
state.name and state.centerTo map the  datasets:


(Screenshot; click for dynamic version.)
Section 33.5: Dynamic Leaflet maps in Shiny applications
The Leaflet package is designed to integerate with Shiny
leafletOutput() and in the server you call renderLeafletIn the ui you call ()

However, reactive inputs that affect the renderLeaflet expression will cause the entire map to be redrawn each time the reactive element is updated.
leafletProxyTherefore, to modify a map that's already running you should use the () function.
Normally you use leaflet to create the static aspects of the map, and leafletProxy to manage the dynamic elements, for example:



Chapter 34: Set operations
Section 34.1: Set operators for pairs of vectors
Comparing sets
In R, a vector may contain duplicated elements:

However, a set contains only one copy of each element. R treats a vector like a set by taking only its distinct elements, so the two vectors above are regarded as the same:

Combining sets
The key functions have natural names:

?unionThese are all documented on the same page, .
Section 34.2: Cartesian or "cross" products of vectors
expand.gridTo find every vector of the form (x, y) where x is drawn from vector X and y from Y, we use :

do.callThe result is a data.frame with one column for each vector passed to it. Often, we want to take the Cartesian product of sets rather than to expand a "grid" of vectors. We can use unique, lapply and :


Applying functions to combinations
x,yIf you then want to apply a function to each resulting combination f(), it can be added as another column:

This approach works for as many vectors as we need, but in the special case of two, it is sometimes a better fit to have the result in a matrix, which can be achieved with outer:

For related concepts and tools, see the combinatorics topic.
Section 34.3: Set membership for vectors

Each element on the left is treated individually and tested for membership in the set associated with the vector on the right (consisting of all its distinct elements).
inUnlike equality tests, %% always returns TRUE or FALSE:

inThe documentation is at ?`%%`.
Section 34.4: Make unique / drop duplicates / select distinct elements from a vector
unique drops duplicates so that each element in the result is unique (only appears once):

Values are returned in the order they first appeared. duplicated tags each duplicated element:

anyDuplicated(x) > 0L is a quick way of checking whether a vector contains any duplicates.
Section 34.5: Measuring set overlaps / Venn diagrams for vectors
To count how many elements of two sets overlap, one could write a custom function:

A Venn diagram, offered by various packages, can be used to visualize overlap counts across multiple sets.
Chapter 35: tidyverse
Section 35.1: tidyverse: an overview
What is tidyverse? tidyverse is the fast and elegant way to turn basic R into an enhanced tool, redesigned by Hadley/Rstudio. The development of all packages included in tidyverse follow the principle rules of The tidy tools manifesto. But first, let the authors describe their masterpiece:
The tidyverse is a set of packages that work in harmony because they share common data representations and API design. The tidyverse package is designed to make it easy to install and load core packages from the tidyverse in a single command.
The best place to learn about all the packages in the tidyverse and how they fit together is R for Data Science. Expect to hear more about the tidyverse in the coming months as I work on improved package websites, making citation easier, and providing a common home for discussions about data analysis with the tidyverse.
(source))
How to use it?
Just with the ordinary R packages, you need to install and load the package.

The difference is, on a single command a couple of dozens of packages are installed/loaded. As a bonus, one may rest assured that all the installed/loaded packages are of compatible versions.
What are those packages?
The commonly known and widely used packages:
ggplot2: advanced data visualisation SO_doc dplyr: fast (Rcpp) and coherent approach to data manipulation SO_doc tidyr: tools for data tidying SO_doc readr: for data import.
purrr: makes your pure functions purr by completing R's functional programming tools with important features from other languages, in the style of the JS packages underscore.js, lodash and lazy.js. tibble: a modern re-imagining of data frames.
magrittr: piping to make code more readable SO_doc
Packages for manipulating specific data formats:
hms: easily read times stringr: provide a cohesive set of functions designed to make working with strings as easy as posssible lubridate: advanced date/times manipulations SO_doc forcats: advanced work with factors.
Data import:
DBI: defines a common interface between the R and database management systems (DBMS) haven: easily import SPSS, SAS and Stata files SO_doc
httr: the aim of httr is to provide a wrapper for the curl package, customised to the demands of modern web APIs jsonlite: a fast JSON parser and generator optimized for statistical data and the web readxl: read.xls and .xlsx files without need for dependency packages SO_doc rvest: rvest helps you scrape information from web pages SO_doc xml2: for XML
And modelling:
modelr: provides functions that help you create elegant pipelines when modelling broom: easily extract the models into tidy data
Finally, tidyverse suggest the use of:
knitr: the amazing general-purpose literate programming engine, with lightweight API's designed to give users full control of the output without heavy coding work. SO_docs: one, two
 rmarkdown: Rstudio's package for reproducible programming. SO_docs: one, two, three, four
 Section 35.2: Creating tbl_df's
A tbl_df (pronounced tibble diff) is a variation of a data frame that is often used in tidyverse packages. It is implemented in the tibble package.
Use the as_data_frame function to turn a data frame into a tbl_df:

One of the most notable differences between data.frames and tbl_dfs is how they print:
# A tibble: 32 x 11      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb *  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
1 21.0     6 160.0   110  3.90 2.620 16.46     0     1     4     4
2 21.0     6 160.0   110  3.90 2.875 17.02     0     1     4     4
3 22.8     4 108.0    93  3.85 2.320 18.61     1     1     4     1
4 21.4     6 258.0   110  3.08 3.215 19.44     1     0     3     1
5 18.7     8 360.0   175  3.15 3.440 17.02     0     0     3     2
6 18.1     6 225.0   105  2.76 3.460 20.22     1     0     3     1
7 14.3     8 360.0   245  3.21 3.570 15.84     0     0     3     4
8 24.4     4 146.7    62  3.69 3.190 20.00     1     0     4     2
9 22.8     4 140.8    95  3.92 3.150 22.90     1     0     4     2
10 19.2     6 167.6   123  3.92 3.440 18.30     1     0     4     4
# ... with 22 more rows
32 x 11The printed output includes a summary of the dimensions of the table ()
It includes the type of each column (dbl)
numberoptions(tibble.print_max It prints a limited number of rows. (To change this use = [])).
group_byMany functions in the dplyr package work naturally with tbl_dfs, such as ().
Chapter 36: Rcpp
Section 36.1: Extending Rcpp with Plugins
Within C++, one can set different compilation flags using:

List of the built-in plugins:

Section 36.2: Inline Code Compile
cppFunction()evalCpp(). A third function called sourceCpp()Rcpp features two functions that enable code compilation inline and exportation directly into R:  and  exists to read in C++ code in a separate file though can be used akin
cppFunction()to .
Below is an example of compiling a C++ function within R. Note the use of "" to surround the source.

To quickly understand a C++ expression use:

Section 36.3: Rcpp Attributes
Rcpp Attributes makes the process of working with R and C++ straightforward. The form of attributes take:

The use of attributes is typically associated with:

sourceCppthat is placed directly above a declared function header when reading in a C++ file via ().
Below is an example of an external C++ file that uses attributes.

To use this external C++ file within R, we do the following:


Section 36.4: Specifying Additional Build Dependencies
Rcpp.hTo use additional packages within the Rcpp ecosystem, the correct header file may not be  but
Rcpp<PACKAGE>.h (as e.g. for RcppArmadillo). It typically needs to be imported and then the dependency is stated
within

Examples:

Chapter 37: Random Numbers Generator
Section 37.1: Random permutations
To generate random permutation of 5 numbers:

To generate random permutation of any vector:


Section 37.2: Generating random numbers using various density functions
Below are examples of generating 5 random numbers using various probability distributions.
Uniform distribution between 0 and 10 runif(5, min=0, max=10)
Normal distribution with 0 mean and standard deviation of 1

Binomial distribution with 10 trials and success probability of 0.5 rbinom(5, size=10, prob=0.5)
Geometric distribution with 0.2 success probability

Hypergeometric distribution with 3 white balls, 10 black balls and 5 draws

Negative Binomial distribution with 10 trials and success probability of 0.8

Poisson distribution with mean and variance (lambda) of 2

Exponential distribution with the rate of 1.5


Chi-squared distribution with 15 degrees of freedom



Cauchy distribution with 0 location and scale of 1 rcauchy(5, location=0, scale=1)
Log-normal distribution with 0 mean and standard deviation of 1 (on log scale)


Wilcoxon distribution with 10 observations in the first sample and 20 in second.

Multinomial distribution with 5 object and 3 boxes using the specified probabilities

Section 37.3: Random number generator's reproducibility
set.seedWhen expecting someone to reproduce an R code that has random elements in it, the () function becomes very handy. For example, these two lines will always produce different output (because that is the whole point of random number generators):

These two will also produce different outputs:

However, if we set the seed to something identical in both cases (most people use 1 for simplicity), we get two identical samples:


Chapter 38: Parallel processing
Section 38.1: Parallel processing with parallel package
The base package parallel allows parallel computation through forking, sockets, and random-number generation.
Detect the number of cores present on the localhost:

Create a cluster of the cores on the localhost:

First, a function appropriate for parallelization must be created. Consider the mtcars dataset. A regression on mpg could be improved by creating a separate regression model for each level of cyl.

Create a function that can loop through all the possible iterations of zlevels. This is still in serial, but is an important step as it determines the exact process that will be parallelized.

Curry this function:

Parallel computing using parallel cannot access the global environment. Luckily, each function creates a local environment parallel can access. Creation of a wrapper function allows for parallelization. The function to be applied also needs to be placed within the environment.


Now create a cluster and run the wrapper function.
 parallelcluster <- parallel::makeCluster(parallel::detectCores()) models <- parallel::parLapply(parallelcluster,zlevels,                               wrapper(datax, datay, dataz)) Always stop the cluster when finished.

applyThe parallel package includes the entire () family, prefixed with par.
Section 38.2: Parallel processing with foreach package
The foreach package brings the power of parallel processing to R. But before you want to use multi core CPUs you have to assign a multi core cluster. The doSNOW package is one possibility.
A simple use of the foreach loop is to calculate the sum of the square root and the square of all numbers from 1 to 100000.

combineThe structure of the output of foreach is controlled by the . argument. The default output structure is a list. In the code above, c is used to return a vector instead. Note that a calculation function (or operator) such as "+" may also be used to perform a calculation and return a further processed object.
It is important to mention that the result of each foreach-loop is the last call. Thus, in this example k will be added to the result.
Parameter	Details
combine Function. Determines how the results of the loop are combined. Possible values are c, cbind,
.combine rbind, "+", "*"...
if TRUE the result is ordered according to the order of the iteration vairable (here i). If FALSE the result
.inorder is not ordered. This can have postive effects on computation time. for functions which are provided by any package except base, like e.g. mass, randomForest or else, you
"mass", "randomForest".packages
           have to provide these packages with c()
      Section 38.3: Random Number Generation
set.seedA major problem with parallelization is the used of RNG as seeds. Random numbers by the number are iterated by the number of operations from either the start of the session or the most recent (). Since parallel processes arise from the same function, it can use the same seed, possibly causing identical results! Calls will run in serial on the different cores, provide no advantage.
A set of seeds must be generated and sent to each parallel process. This is automatically done in some packages (parallel, snow, etc.), but must be explicitly addressed in others.

Seeds can be also be set for reproducibility.

Section 38.4: mcparallelDo
The mcparallelDo package allows for the evaluation of R code asynchronously on Unix-alike (e.g. Linux and MacOSX) operating systems. The underlying philosophy of the package is aligned with the needs of exploratory data analysis rather than coding. For coding asynchrony, consider the future package.
Example
Create data

Trigger mcparallelDo to perform analysis on a fork
 mcparallelDo({glm(len ~ supp * dose, data=ToothGrowth)},"interactionPredictorModel") Do other things, e.g.
binaryPredictorModel <- glm(len ~ supp, data=ToothGrowth) gaussianPredictorModel <- glm(len ~ dose, data=ToothGrowth)
The result from mcparallelDo returns in your targetEnvironment, e.g. .GlobalEnv, when it is complete with a message (by default)

Other Examples



Chapter 39: Subsetting
Given an R object, we may require separate analysis for one or more parts of the data contained in it. The process of obtaining these parts of the data from a given object is called subsetting.
Section 39.1: Data frames
Subsetting a data frame into a smaller data frame can be accomplished the same as subsetting a list.

Subsetting a dataframe into a column vector can be accomplished using double brackets [[ ]] or the dollar sign operator $.

Subsetting a data as a two dimensional matrix can be accomplished using i and j terms.


Note: Subsetting by j (column) alone simplifies to the variable's own type, but subsetting by i alone returns a
data.frame, as the different variables may have different types and classes. Setting the drop parameter to FALSE
keeps the data frame.

Section 39.2: Atomic vectors
Atomic vectors (which excludes lists and expressions, which are also vectors) are subset using the [ operator:

The [ operator can also take a vector as the argument. For example, to select the first and third elements:

v1v1Some times we may require to omit a particular value from the vector. This can be achieved using a negative sign(-) before the index of that value. For example, to omit to omit the first value from v1, use [-1]. This can be extended to more than one value in a straight forward way. For example, [-c(1,3)].

On some occasions, we would like to know, especially, when the length of the vector is large, index of a particular value, if it exists:

If the atomic vector has names (a names attribute), it can be subset using a character vector of names:

The [[ operator can also be used to index atomic vectors, with differences in that it accepts a indexing vector with a length of one and strips any names present:

Vectors can also be subset using a logical vector. In contrast to subsetting with numeric and character vectors, the logical vector used to subset has to be equal to the length of the vector whose elements are extracted, so if a logical vector y is used to subset x, i.e. x[y], if 
Section 39.3: Matrices
i, jFor each dimension of an object, the [ operator takes one argument. Vectors have one dimension and take one argument. Matrices and data frames have two dimensions and take two arguments, given as [] where i is the row and j is the column. Indexing starts at 1.


mat[i,j] is the element in the i-th row, j-th column of the matrix mat. For example, an i value of 2 and a j value of 1 gives the number in the second row and the first column of the matrix. Omitting i or j returns all values in that dimension.

When the matrix has row or column names (not required), these can be used for subsetting:

drop = FALSEBy default, the result of a subset will be simplified if possible. If the subset only has one dimension, as in the examples above, the result will be a one-dimensional vector rather than a two-dimensional matrix. This default can be overridden with the  argument to [:

Of course, dimensions cannot be dropped if the selection itself has two dimensions:

Selecting individual matrix entries by their positions
1st row, 1st column), (1st row, 3rdIt is also possible to use a Nx2 matrix to select N individual elements from a matrix (like how a coordinate system works). If you wanted to extract, in a vector, the entries of a matrix in the (
column), (2nd row, 3rd column), (2nd row, 1st column) this can be done easily by creating a index matrix with those coordinates and using that to subset the matrix:


In the above example, the 1st column of the ind matrix refers to rows in mat, the 2nd column of ind refers to columns in mat.
Section 39.4: Lists

l1Note the result of [2] is still a list, as the [ operator selects elements of a list, returning a smaller list. The [[ operator extracts list elements, returning an object of the type of the list element.


Compared to:

which is equivalent to:

The $ operator allows you to select list elements solely by name, but unlike [ and [[, does not require quotes. As an infix operator, $ can only take a single name:

Also, the $ operator allows for partial matching by default:

in contrast with [[ where it needs to be specified whether partial matching is allowed:

options(warnPartialMatchDollar = TRUESetting ), a "warning" is given when partial matching happens with $:

Section 39.5: Vector indexing
For this example, we will use the vector:

R vectors are 1-indexed, so for example x[1] will return 11. We can also extract a sub-vector of x by passing a vector of indices to the bracket operator:

If we pass a vector of negative indices, R will return a sub-vector with the specified indices excluded:

We can also pass a boolean vector to the bracket operator, in which case it returns a sub-vector corresponding to the coordinates where the indexing vector is TRUE:

If the indexing vector is shorter than the length of the array, then it will be repeated, as in:

Section 39.6: Other objects
The [ and [[ operators are primitive functions that are generic. This means that any object in R (specifically
isTRUE(is.object(x)) --i.e. has an explicit "class" attribute) can have its own specified behaviour when subsetted; i.e. has its own methods for [ and/or [[.
is.object(iris)) objects where [.data.frame and [[.data.framedata.frameFor example, this is the case with "data.frame" ( methods are defined and they are made to exhibit both "matrix"-like and "list"-like subsetting. With forcing an error when subsetting a "data.frame", we see that, actually, a function [. was called when we -just- used [.

Without further details on the current topic, an example[ method:

subset (and .subset2We can overcome the method dispatching of [ by using the equivalent non-generic . for [[).
unclassThis is especially useful and efficient when programming our own "class"es and want to avoid work-arounds (like (x)) when computing on our "class"es efficiently (avoiding method dispatch and copying objects):

Section 39.7: Elementwise Matrix Operations
Let A and B be two matrices of same dimension. The operators +,-,/,*,^ when used with matrices of same dimension perform the required operations on the corresponding elements of the matrices and return a new matrix of the same dimension. These operations are usually referred to as element-wise operations.
Operator A op B	Meaning
+	A + B	Addition of corresponding elements of A and B
-	A - B	Subtracts the elements of B from the corresponding elements of A
/	A / B	Divides the elements of A by the corresponding elements of B
*	A * B	Multiplies the elements of A by the corresponding elements of B
^	A^(-1) For example, gives a matrix whose elements are reciprocals of A
A %*%For "true" matrix multiplication, as seen in Linear Algebra, use %*%. For example, multiplication of A with B is: 
ncol() of A be the same as nrowB. The dimensional requirements are that the () of B
Some Functions used with Matrices Function	Example	Purpose
nrow()	nrow(A)	determines the number of rows of A ncol()	ncol(A)	determines the number of columns of A
rownames() rownames(A) prints out the row names of the matrix A colnames()	colnames(A)	prints out the column names of the matrix A rowMeans() rowMeans(A) computes means of each row of the matrix A
colMeans()colMeans(A)computes means of each column of the matrix Aupper.tri()upper.tri(A)returns a vector whose elements are the upper triangular matrix of square matrix Alower.tri()lower.tri(A)returns a vector whose elements are the lower triangular matrix of square matrix Adet()det(A)results in the determinant of the matrix Asolve()solve(A)results in the inverse of the non-singular matrix Adiag()diag(A)returns a diagonal matrix whose off-diagnal elemts are zeros and diagonals are the same as that of the square matrix At()t(A)returns the the transpose of the matrix Aeigen()eigen(A)retuens the eigenvalues and eigenvectors of the matrix Ais.matrix()is.matrix(A)returns TRUE or FALSE depending on whether A is a matrix or not.as.matrix()as.matrix(x)creates a matrix out of the vector xChapter 40: Debugging
Section 40.1: Using debug
You can set any function for debugging with debug.

All subsequent calls to the function will enter debugging mode. You can disable this behavior with undebug.

If you know you only want to enter the debugging mode of a function once, consider the use of debugonce.

Section 40.2: Using browser
The browser function can be used like a breakpoint: code execution will pause at the point it is called. Then user can then inspect variable values, execute arbitrary R code and step through the code line by line.
browserOnce () is hit in the code the interactive interpreter will start. Any R code can be run as normal, and in addition the following commands are present,
Command	Meaning
c	Exit browser and continue program f	Finish current loop or function \ n	Step Over (evaluate next statement, stepping over function calls) s	Step Into (evaluate next statement, stepping into function calls) where	Print stack trace r	Invoke "resume" restart Q	Exit browser and quit
For example we might have a script like,

When running the above script we initially see something like,

We could then interact with the prompt as so,



Chapter 41: Installing packages
repos = NULLParameter	Details pkgs	character vector of the names of packages. If , a character vector of file paths. lib	character vector giving the library directories where to install the packages.
 repos	character vector, the base URL(s) of the repositories to use, can be NULL to install from local files method	download method
repos = NULL   destdir	directory where downloaded packages are stored logical indicating whether to also install uninstalled packages which these packages depend on/link dependencies
to/import/suggest (and so on recursively). Not used if .
Arguments to be passed to 'download.file' or to the functions for binary installs on OS X and
... Windows.
Section 41.1: Install packages from GitHub
To install packages directly from GitHub use the devtools package:

To install ggplot2 from github:

The above command will install the version of ggplot2 that corresponds to the master branch. To install from a different branch of a repository use the ref argument to provide the name of the branch. For example, the following command will install the dev_general branch of the googleway package. devtools::install_github("SymbolixAU/googleway", ref = "dev_general")
Another option is to use the ghit package. It provides a lightweight alternative for installing packages from github:

To install a package that is in a private repository on Github, generate a personal access token at http://www.github.com/settings/tokens/ (See ?install_github for documentation on the same). Follow these steps:



This prevents the common error: "Peer certificate cannot be authenticated with given CA certificates"
6. Finally, use the following command to install your package seamlessly

Alternatively, set an environment variable GITHUB_PAT, using

RprofileThe PAT generated in Github is only visible once, i.e., when created initially, so its prudent to save that token in .. This is also helpful if the organisation has many private repositories.
Section 41.2: Download and install packages from repositories
Packages are collections of R functions, data, and compiled code in a well-defined format. Public (and private) repositories are used to host collections of R packages. The largest collection of R packages is available from CRAN.
Using CRAN
A package can be installed from CRAN using following code:

Where "dplyr" is referred to as a character vector.
More than one packages can be installed in one go by using the combine function c() and passing a series of character vector of package names:

install.packagesgetOption("repos"In some cases,  may prompt for a CRAN mirror or fail, depending on the value of ). To prevent this, specify a CRAN mirror as repos argument:

install.packagesUsing the repos argument it is also possible to install from other repositories. For complete information about all the available options, run ?.
data.tableMost packages require functions, which were implemented in other packages (e.g. the package ). In order to install a package (or multiple packages) with all the packages, which are used by this given package, the argument dependencies should be set to TRUE):

Using Bioconductor
Bioconductor hosts a substantial collection of packages related to Bioinformatics. They provide their own package management centred around the biocLite function:

By default this installs a subset of packages that provide the most commonly used functionality. Specific packages can be installed by passing a vector of package names. For example, to install RImmPort from Bioconductor:

Section 41.3: Install package from local source
To install package from local source file:

Here, path_to_source is absolute path of local source file.
Another command that opens a window to choose downloaded zip or tar.gz source files is:

Another possible way is using the GUI based RStudio:
Step 1: Go to Tools.
Step 2: Go to Install Packages.
Step 3: In the Install From set it as Package Archive File (.zip; .tar.gz)
Step 4: Then Browse find your package file (say crayon_1.3.1.zip) and after some time (after it shows the Package path and file name in the Package Archive tab)
install_localAnother way to install R package from local source is using () function from devtools package.

Section 41.4: Install local development version of a package
While working on the development of an R package it is often necessary to install the latest version of the package. This can be achieved by first building a source distribution of the package (on the command line)

and then installing it in R. Any running R sessions with previous version of the package loaded will need to reload it.

A more convenient approach uses the devtools package to simplify the process. In an R session with the working directory set to the package directory

will build, install and reload the package.
Section 41.5: Using a CLI package manager -- basic pacman usage
pacman is a simple package manager for R.
pacman allows a user to compactly load all desired packages, installing any which are missing (and their dependencies), with a single command, p_load. pacman does not require the user to type quotation marks around a package name. Basic usage is as follows:

install.packagesThe only package requiring a library, require, or  statement with this approach is pacman itself:

or, equally valid:

In addition to saving time by requiring less code to manage packages, pacman also facilitates the construction of reproducible code by installing any needed packages if and only if they are not already installed.
Since you may not be sure if pacman is installed in the library of a user who will use your code (or by yourself in future uses of your own code) a best practice is to include a conditional statement to install pacman if it is not already loaded:

Chapter 42: Inspecting packages
Packages build on base R. This document explains how to inspect installed packages and their functionality. Related Docs: Installing packages
Section 42.1: View Package Version
Conditions: package should be at least installed. If not loaded in the current session, not a problem.

Section 42.2: View Loaded packages in Current Session
To check the list of loaded packages

OR

Section 42.3: View package information
To retrieve information about dplyr package and its functions' descriptions:

No need to load the package first.
Section 42.4: View package's built-in data sets
To see built-in data sets from package dplyr

No need to load the package first.
Section 42.5: List a package's exported functions
To get the list of functions within package dplyr, we first must load the package:

Chapter 43: Creating packages with devtools
This topic will cover the creation of R packages from scratch with the devtools package.
Section 43.1: Creating and distributing packages
This is a compact guide about how to quickly create an R package from your code. Exhaustive documentations will be linked when available and should be read if you want a deeper knowledge of the situation. See Remarks for more resources.
The directory where your code stands will be referred as ./, and all the commands are meant to be executed from a R prompt in this folder.
Creation of the documentation
The documentation for your code has to be in a format which is very similar to LaTeX.
However, we will use a tool named roxygen in order to simplify the process:

The full man page for roxygen is available here. It is very similar to doxygen.
Here is a practical sample about how to document a function with roxygen:

And here will be the result.
It is also recommanded to create a vignette (see the topic Creating vignettes), which is a full guide about your package.
Construction of the package skeleton
script1.R and ./script2.Assuming that your code is written for instance in files ./R, launch the following command in order to create the file tree of your package:
package.skeleton(name="MyPackage", code_files=c("script1.R","script2.R"))
MyPackage/manThen delete all the files in .//. You have now to compile the documentation:

You should also generate a reference manual from your documentation using R CMD Rd2pdf MyPackage from a command prompt started in ./.
Edition of the package properties 1. Package description
MyPackage/DESCRIPTIONModify ./ according to your needs. The fields Package, Version, License, Description,
Title, Author and Maintainer are mandatory, the other are optional.
If your package depends on others packages, specify them in a field named Depends (R version < 3.2.0) or Imports (R version > 3.2.0).
2. Optional folders
MyPackage/ only had R/ and manOnce you launched the skeleton build, .// subfolders. However, it can have some others:
dataRData extension, and you can load it at runtime with data() and loadtestssrcexecmisc/: here you can place the data that your library needs and that isn't code. It must be saved as dataset with the .()
/: all the code files in this folder will be ran at install time. If there is any error, the installation will fail.
/: for C/C++/Fortran source files you need (using Rcpp...).
/: for other executables.
/: for barely everything else.
Finalization and build
MyPackage/Read-and-delete-meYou can delete ./.
As it is now, your package is ready to be installed.
devtools::install("MyPackage"You can install it with ).
To build your package as a source tarball, you need to execute the following command, from a command prompt in
./ : R CMD build MyPackage
Distribution of your package Through Github
MyPackageSimply create a new repository called MyPackage and upload everything in / to the master branch. Here is an example.
Then anyone can install your package from github with devtools:

Through CRAN
Your package needs to comply to the CRAN Repository Policy. Including but not limited to: your package must be cross-platforms (except some very special cases), it should pass the R CMD check test.
Here is the submission form. You must upload the source tarball.
Section 43.2: Creating vignettes
A vignette is a long-form guide to your package. Function documentation is great if you know the name of the function you need, but it's useless otherwise. A vignette is like a book chapter or an academic paper: it can describe the problem that your package is designed to solve, and then show the reader how to solve it.
Vignettes will be created entirely in markdown.

vignettes/MyVignette.RmdYou can now edit your vignette at ./.
The text in your vignette is formatted as Markdown.
The only addition to the original Markdown, is a tag that takes R code, runs it, captures the output, and translates it into formatted Markdown:

Will display as:

DESCRIPTIONThus, all the packages you will use in your vignettes must be listed as dependencies in ./.
Chapter 44: Using pipe assignment in your own package %<>%: How to ?
In order to use the pipe in a user-created package, it must be listed in the NAMESPACE like any other function you choose to import.
Section 44.1: Putting the pipe in a utility-functions file
zzz.R or utils.ROne option for doing this is to export the pipe from within the package itself. This may be done in the 'traditional'  files that many packages utilise for useful little functions that are not exported as part of the
package. For example, putting:


Chapter 45: Arima Models
Section 45.1: Modeling an AR1 Process with Arima
We will model the process



We will fit an Arima model with autoregressive order 1, 0 degrees of differencing, and an MA order of 0.

Notice that our coefficient is close to the true value from the generated data







Chapter 46: Distribution Functions
DistributionsR has many built-in functions to work with probability distributions, with official docs starting at ?.
Section 46.1: Normal distribution
normLet's use * as an example. From the documentation:

So if I wanted to know the value of a standard normal distribution at 0, I would do

Which gives us 0.3989423, a reasonable answer.
pnormqnorm(.5In the same way (0) gives .5. Again, this makes sense, because half of the distribution is to the left of 0. qnorm will essentially do the opposite of pnorm. ) gives 0.
Finally, there's the rnorm function:

Will generate 10 samples from standard normal.
If you want to change the parameters of a given distribution, simply change them like so
rnorm(10, mean=4, sd= 3)
Section 46.2: Binomial Distribution
We now illustrate the functions dbinom,pbinom,qbinom and rbinom defined for Binomial distribution.
dbinomThe () function gives the probabilities for various values of the binomial variable. Minimally it requires three arguments. The first argument for this function must be a vector of quantiles(the possible values of the random variable X). The second and third arguments are the defining parameters of the distribution, namely, n(the number of independent trials) and p(the probability of success in each trial). For example, for a binomial distribution with = 5, 
The binomial probability distribution plot can be displayed as in the following figure:

p = 0.5Note that the binomial distribution is symmetric when . To demonstrate that the binomial distribution is negatively skewed when p is larger than 0.5, consider the following example:

When p is smaller than 0.5 the binomial distribution is positively skewed as shown below.

pbinom X <= x We will now illustrate the usage of the cumulative distribution function (). This function can be used to calculate probabilities such as P(). The first argument to this function is a vector of quantiles(values of x).

The above probability can also be obtained as follows:


Presenting the binomial distribution in the form of a table:


rbinomThe () is used to generate random samples of specified sizes with a given parameter values.

Chapter 47: Shiny
Section 47.1: Create an app
Shiny is an R package developed by RStudio that allows the creation of web pages to interactively display the results of an analysis in R.
There are two simple ways to create a Shiny app:
ui.R and server.in one .R file, or in two files: R.



Section 47.2: Checkbox Group
Create a group of checkboxes that can be used to toggle multiple choices independently. The server will receive the input as a character vector of the selected values.


It's possible to change the settings :
label : title choices : selected values selected : The initially selected value (NULL for no selection) inline : horizontal or vertical width
It is also possible to add HTML.
Section 47.3: Radio Button
You can create a set of radio buttons used to select an item from a list.
It's possible to change the settings :
selected : The initially selected value (character(0) for no selection) inline : horizontal or vertical width
It is also possible to add HTML.


Section 47.4: Debugging
debug() and debugonce() won't work well in the context of most Shiny debugging. However, browserbrowser() statements inserted in critical places can give you a lot of insight into how your Shiny code is (not) working. See also: Debugging using ()
Showcase mode
Showcase mode displays your app alongside the code that generates it and highlights lines of code in server.R as it runs them.
There are two ways to enable Showcase mode:
runApp("MyApp", display.mode "showcase"DisplayMode: ShowcaseLaunch Shiny app with the argument display.mode = "showcase", e.g., =
).
Create file called DESCRIPTION in your Shiny app folder and add this line in it: .
Reactive Log Visualizer
Reactive Log Visualizer provides an interactive browser-based tool for visualizing reactive dependencies and
options(shiny.reactlog=TRUEexecution in your application. To enable Reactive Log Visualizer, execute ) in R
console and or add that line of code in your server.R file. To start Reactive Log Visualizer, hit Ctrl+F3 on Windows or Command+F3 on Mac when your app is running. Use left and right arrow keys to navigate in Reactive Log Visualizer.
Section 47.5: Select box
Create a select list that can be used to choose a single or multiple items from a list of values.


It's possible to change the settings :
label : title choices : selected values selected : The initially selected value (NULL for no selection) multiple : TRUE or FALSE width size
selectize: TRUE or FALSE (for use or not selectize.js, change the display)
It is also possible to add HTML.
Section 47.6: Launch a Shiny app
ui.R and server.You can launch an application in several ways, depending on how you create you app. If your app is divided in two files R or if all of your app is in one file.
1. Two files app
ui.R and server.shinyAppYour two files Rhave to be in the same folder. You could then launch your app by running in the console the () function and by passing the path of the directory that contains the Shiny app.

ui.R or server.You can also launch the app directly from Rstudio by pressing the Run App button that appear on Rstudio when you an R file open.

runAppOr you can simply write () on the console if your working directory is Shiny App directory.
2. One file app
shinyAppIf you create your in one R file you can also launch it with the () function.
 inside of your code :

Section 47.7: Control widgets
	Function	Widget
actionButton	Action Button
checkboxGroupInput A group of check boxes
checkboxInputA single check boxdateInputA calendar to aid date selectiondateRangeInputA pair of calendars for selecting a date rangefileInputA file upload control wizardhelpTextHelp text that can be added to an input formnumericInputA field to enter numbersradioButtonsA set of radio buttonsselectInputA box with choices to select fromsliderInputA slider barsubmitButtonA submit buttontextInputA field to enter text

Chapter 48: spatial analysis
Section 48.1: Create spatial points from XY data set
When it comes to geographic data, R shows to be a powerful tool for data handling, analysis and visualisation.
Often, spatial data is avaliable as an XY coordinate data set in tabular form. This example will show how to create a spatial data set from an XY data set.
Spatial*DataFrameThe packages rgdal and sp provide powerful functions. Spatial data in R can be stored as (where * can be Points, Lines or Polygons).
This example uses data which can be downloaded at OpenGeocode.
At first, the working directory has to be set to the folder of the downloaded CSV data set. Furthermore, the package rgdal has to be loaded.

data.frameAfterwards, the CSV file storing cities and their geographical coordinates is loaded into R as a 

Often, it is useful to get a glimpse of the data and its structure (e.g. column names, data types etc.).

This shows that the latitude and longitude columns are interpreted as character values, since they hold entries like
SpatialPointsDataFrame"-33.532". Yet, the later used function () which creates the spatial data set requires the coordinate values to be of the data type numeric. Thus the two columns have to be converted.

Few of the values cannot be converted into numeric data and thus, NA values are created. They have to be removed.

Finally, the XY data set can be converted into a spatial data set. This requires the coordinates and the specification of the Coordinate Refrence System (CRS) in which the coordinates are stored.

The basic plot function can easily be used to sneak peak the produced spatial points.


Section 48.2: Importing a shape file (.shp)
rgdal
readOGRESRI shape files can easily be imported into R by using the function () from the rgdal package.

shpIt is important to know, that the dsn must not end with / and the layer does not allow the file ending (e.g. .) raster
Another possible way of importing shapefiles is via the raster library and the shapefile function:

Note how the path definition is different from the rgdal import statement.
rgdal::readORGtmap tmap package provides a nice wrapper for the  function.

Chapter 49: sqldf
Section 49.1: Basic Usage Examples
sqldf      () from the package sqldf allows the use of SQLite queries to select and manipulate data in R. SQL queries are entered as character strings.
To select the first 10 rows of the "diamonds" dataset from the package ggplot2, for example:
data("diamonds") head(diamonds)
# A tibble: 6 x 10   carat       cut color clarity depth table price     x     y     z   <dbl>     <ord> <ord>   <ord> <dbl> <dbl> <int> <dbl> <dbl> <dbl>
1 0.23     Ideal     E     SI2  61.5    55   326  3.95  3.98  2.43
2 0.21   Premium     E     SI1  59.8    61   326  3.89  3.84  2.31
3 0.23      Good     E     VS1  56.9    65   327  4.05  4.07  2.31
4 0.29   Premium     I     VS2  62.4    58   334  4.20  4.23  2.63
5 0.31      Good     J     SI2  63.3    58   335  4.34  4.35  2.75
6 0.24 Very Good     J    VVS2  62.8    57   336  3.94  3.96  2.48
require(sqldf) sqldf("select * from diamonds limit 10")
   carat       cut color clarity depth table price    x    y    z 1   0.23     Ideal     E     SI2  61.5    55   326 3.95 3.98 2.43
2 0.21   Premium     E     SI1  59.8    61   326 3.89 3.84 2.31
3 0.23      Good     E     VS1  56.9    65   327 4.05 4.07 2.31
4 0.29   Premium     I     VS2  62.4    58   334 4.20 4.23 2.63
5 0.31      Good     J     SI2  63.3    58   335 4.34 4.35 2.75
6 0.24 Very Good     J    VVS2  62.8    57   336 3.94 3.96 2.48
7 0.24 Very Good     I    VVS1  62.3    57   336 3.95 3.98 2.47
8 0.26 Very Good     H     SI1  61.9    55   337 4.07 4.11 2.53
9 0.22      Fair     E     VS2  65.1    61   337 3.87 3.78 2.49
10 0.23 Very Good     H     VS1  59.4    61   338 4.00 4.05 2.39
To select the first 10 rows where for the color "E":
sqldf("select * from diamonds where color = 'E' limit 10")
   carat       cut color clarity depth table price    x    y    z 1   0.23     Ideal     E     SI2  61.5    55   326 3.95 3.98 2.43
2 0.21   Premium     E     SI1  59.8    61   326 3.89 3.84 2.31
3 0.23      Good     E     VS1  56.9    65   327 4.05 4.07 2.31
4 0.22      Fair     E     VS2  65.1    61   337 3.87 3.78 2.49
5 0.20   Premium     E     SI2  60.2    62   345 3.79 3.75 2.27
6 0.32   Premium     E      I1  60.9    58   345 4.38 4.42 2.68
7 0.23 Very Good     E     VS2  63.8    55   352 3.85 3.92 2.48
8 0.23 Very Good     E     VS1  60.7    59   402 3.97 4.01 2.42
9 0.23 Very Good     E     VS1  59.5    58   402 4.01 4.06 2.40
10 0.23      Good     E     VS1  64.1    59   402 3.83 3.85 2.46
Notice in the example above that quoted strings within the SQL query are quoted using '' if the overall query is quoted with "" (this also works in reverse).
Suppose that we wish to add a new column to count the number of Premium cut diamonds over 1 carat:

Results of created values can also be returned as new columns:
sqldf("select *, count(*) as cnt_big_E_colored_stones from diamonds where carat > 1 and color = 'E' group by clarity")
  carat       cut color clarity depth table price    x    y    z cnt_big_E_colored_stones 1  1.30      Fair     E      I1  66.5    58  2571 6.79 6.75 4.50                       65
2 1.28     Ideal     E      IF  60.7    57 18700 7.09 6.99 4.27                       28
3 2.02 Very Good     E     SI1  59.8    59 18731 8.11 8.20 4.88                      499
4 2.03   Premium     E     SI2  61.5    59 18477 8.24 8.16 5.04                      666
5 1.51     Ideal     E     VS1  61.5    57 18729 7.34 7.40 4.53                      158
6 1.72 Very Good     E     VS2  63.4    56 18557 7.65 7.55 4.82                      318
7 1.20     Ideal     E    VVS1  61.8    56 16256 6.78 6.87 4.22                       52
8 1.55     Ideal     E    VVS2  62.5    55 18188 7.38 7.40 4.62                      106
If one would be interested what is the max price of the diamond according to the cut:

Chapter 50: Code profiling
Section 50.1: Benchmarking using microbenchmark
You can use the microbenchmark package to conduct "sub-millisecond accurate timing of expression evaluation".
data.tableIn this example we are comparing the speeds of six equivalent  expressions for updating elements in a group, based on a certain condition.

   times = 10L ## specify the number of times each expression is evaluated )
# Unit: milliseconds
#         expr         min          lq        mean      median         uq          max neval
# expression_1   11.646149   13.201670   16.808399   15.643384   18.78640    26.321346    10
# expression_2 8051.898126 8777.016935 9238.323459 8979.553856 9281.93377 12610.869058    10
# expression_3    3.208773    3.385841    4.207903    4.089515    4.70146     5.654702    10
# expression_4   15.758441   16.247833   20.677038   19.028982   21.04170    36.373153    10
# expression_5 7552.970295 8051.080753 8702.064620 8861.608629 9308.62842  9722.234921    10
# expression_6   18.403105   18.812785   22.427984   21.966764   24.66930    28.607064    10
The output shows that in this test expression_3 is the fastest. References data.table - Adding and modifying columns data.table - special grouping symbols in data.table
Section 50.2: proc.time()
proc.timeAt its simplest, () gives the total elapsed CPU time in seconds for the current process. Executing it in the console gives the following type of output:

This is particularly useful for benchmarking specific lines of code. For example:

This gives the following output:


system.time() is a wrapper for proc.time() that returns the elapsed time for a particular command/expression.

proc.timeNote that the returned object, of class , is slightly more complicated than it appears on the surface:

Section 50.3: Microbenchmark
Microbenchmark is useful for estimating the time taking for otherwise fast procedures. For example, consider estimating the time taken to print hello world.

system.time is essentially a wrapper function for proc.timeThis is because , which measures in seconds. As
printing "hello world" takes less than a second it appears that the time taken is less than a second, however this is not true. To see this we can use the package microbenchmark:

print("hello world"Here we can see after running ) 100 times, the average time taken was in fact 44
cat("hello world\nmicroseconds. (Note that running this code will print "hello world" 100 times onto the console.) We can compare this against an equivalent procedure, "), to see if it is faster than

cat() is almost twice as fast as printIn this case ().
Alternatively one can compare two procedures within the same microbenchmark call:

# print("hello world") 29.122 31.654 39.64255 34.5275 38.852 192.779   100
# cat("hello world\\n")  9.381 12.356 13.83820 12.9930 13.715  52.564   100
Section 50.4: System.time
System time gives you the CPU time required to execute a R expression, for example:

You can add larger pieces of code through use of braces:

Or use it to test functions:

Section 50.5: Line Profiling
auto.arimaOne package for line profiling is lineprof which is written and maintained by Hadley Wickham. Here is a quick demonstration of how it works with  in the forecast package:

This will provide you with a shiny app, which allows you to delve deeper into every function call. This enables you to see with ease what is causing your R code to slow down. There is a screenshot of the shiny app below:


Chapter 51: Control flow structures
Section 51.1: Optimal Construction of a For Loop
To illustrate the effect of good for loop construction, we will calculate the mean of each column in four different ways:
1. Using a poorly optimized for loop
2. Using a well optimized for for loop
apply3. Using an * family of functions
4. Using the colMeans function
Each of these options will be shown in code; a comparison of the computational time to execute each option will be shown; and lastly a discussion of the differences will be given.


vapply Function

colMeans Function

Efficiency comparison
The results of benchmarking these four approaches is shown below (code not displayed)
Unit: microseconds      expr     min       lq     mean   median       uq     max neval  cld      poor 240.986 262.0820 287.1125 275.8160 307.2485 442.609   100    d   optimal 220.313 237.4455 258.8426 247.0735 280.9130 362.469   100   c    vapply 107.042 109.7320 124.4715 113.4130 132.6695 202.473   100 a   colMeans 155.183 161.6955 180.2067 175.0045 194.2605 259.958   100  b
Notice that the optimized for loop edged out the poorly constructed for loop. The poorly constructed for loop is constantly increasing the length of the output object, and at each change of the length, R is reevaluating the class of the object.
Some of this overhead burden is removed by the optimized for loop by declaring the type of output object and its length before starting the loop.
In this example, however, the use of an vapply function doubles the computational efficiency, largely because we told R that the result had to be numeric (if any one result were not numeric, an error would be returned).
Use of the colMeans function is a touch slower than the vapply function. This difference is attributable to some
as.matrix conversion (because mtcars is a data.frameerror checks performed in colMeans and mainly to the ) that weren't performed in the vapply function.
Section 51.2: Basic For Loop Construction
In this example we will calculate the squared deviance for each column in a data frame, in this case the mtcars. Option A: integer index

squared_deviance is an 11 elements list, as expected.

Option B: character index

data.framedata.frameWhat if we want a  as a result? Well, there are many options for transforming a list into other objects. However, and maybe the simplest in this case, will be to store the for results in a .

The result will be the same event though we use the character option (B).
Section 51.3: The Other Looping Constructs: while and repeat
R provides two additional looping constructs, while and repeat, which are typically used in situations where the number of iterations required is indeterminate.
The while loop
The general form of a while loop is as follows,


where condition is evaluated prior to entering the loop body. If condition evaluates to TRUE, the code inside of the loop body is executed, and this process repeats until condition evaluates to FALSE (or a break statement is reached; see below). Unlike the for loop, if a while loop uses a variable to perform incremental iterations, the variable must be declared and initialized ahead of time, and must be updated within the loop body. For example, the following loops accomplish the same task:

Additionally, it is possible to terminate a while loop with a call to break from inside the loop body:

In this example, condition is always TRUE, so the only way to terminate the loop is with a call to break inside the body. Note that the final value of iter will depend on the state of your PRNG when this example is run, and should produce different results (essentially) each time the code is executed.
The repeat loop

The extra {} are not required, but the () are. Rewriting the previous example using repeat,

More on break
It's important to note that break will only terminate the immediately enclosing loop. That is, the following is an infinite loop:

With a little creativity, however, it is possible to break entirely from within a nested loop. As an example, consider the following expression, which, in its current state, will loop infinitely:

returnOne possibility is to recognize that, unlike break, the return expression does have the ability to return control across multiple levels of enclosing loops. However, since return is only valid when used within a function, we cannot simply replace break with () above, but also need to wrap the entire expression as an anonymous function:


Alternatively, we can create a dummy variable (exit) prior to the expression, and activate it via <<- from the inner loop when we are ready to terminate:


Chapter 52: Column wise operation
Section 52.1: sum of each column
Suppose we need to do the sum of each column in a dataset

There are many ways to do this. Using base R, the best option would be colSums

na.rm Here, we removed the first column as it is non-numeric and did the sum of each column, specifying the =
TRUE (in case there are any NAs in the dataset)
This also works with matrix

lapply/sapply/vapplyThis can be done in a loop with 

It should be noted that the output is a list. If we need a vector output

Or

data.tableThere are ways to do this with packages like dplyr or 

\\dHere, we are passing a regular expression to match the column names that we need to get the sum in summarise_at. The regex will match all columns that start with V followed by one or more numbers (+).
data.tableA  option is

setDT(df1We convert the 'data.frame' to 'data.table' ()), specified the columns to be applied the function in
SDcols and loop through the Subset of Data.table (.SD.) and get the sum.
If we need to use a group by operation, we can do this easily by specifying the group by column/columns

In cases where we need the sum of all the columns, summarise_each can be used instead of summarise_at

data.tableThe  option is


Chapter 53: JSON
Section 53.1: JSON to / from R objects
fromJSON() and toJSONdata.framesThe jsonlite package is a fast JSON parser and generator optimized for statistical data and the web. The two main functions used to read and write JSON are () respecitively, and are designed to work with vectors, matrices and , and streams of JSON from the web.
Create a JSON array from a vector, and vice versa

Create a named JSON array from a list, and vice versa

More complex list structures

data.frameCreate JSON from a , and vice versa


Read JSON direct from the internet
## Reading JSON from URL googleway_issues <- fromJSON("https://api.github.com/repos/SymbolixAU/googleway/issues")
googleway_issues$url # [1] "https://api.github.com/repos/SymbolixAU/googleway/issues/20"
"https://api.github.com/repos/SymbolixAU/googleway/issues/19"
# [3] "https://api.github.com/repos/SymbolixAU/googleway/issues/14"
"https://api.github.com/repos/SymbolixAU/googleway/issues/11"
# [5] "https://api.github.com/repos/SymbolixAU/googleway/issues/9"
 "https://api.github.com/repos/SymbolixAU/googleway/issues/5"
# [7] "https://api.github.com/repos/SymbolixAU/googleway/issues/2"

Chapter 54: RODBC
Section 54.1: Connecting to Excel Files via RODBC
While RODBC is restricted to Windows computers with compatible architecture between R and any target RDMS, one of its key flexibilities is to work with Excel files as if they were SQL databases.
require(RODBC) con = odbcConnectExcel("myfile.xlsx") # open a connection to the Excel file sqlTables(con)$TABLE_NAME # show all sheets df = sqlFetch(con, "Sheet1") # read a sheet df = sqlQuery(con, "select * from [Sheet1 $]") # read a sheet (alternative SQL syntax) close(con) # close the connection to the file
Section 54.2: SQL Server Management Database connection to get individual table
Another use of RODBC is in connecting with SQL Server Management Database. We need to specify the 'Driver' i.e. SQL Server here, the database name "Atilla" and then use the sqlQuery to extract either the full table or a fraction of it.

Section 54.3: Connecting to relational databases

This will connect to a SQL Server instance. For more information on what your connection string should look like, visit connectionstrings.com
Also, since there's no database specified, you should make sure you fully qualify the object you're wanting to query like this databasename.schema.objectname
Chapter 55: lubridate
Section 55.1: Parsing dates and datetimes from strings with lubridate
The lubridate package provides convenient functions to format date and datetime objects from character strings.
The functions are permutations of
	Letter	Element to parse Base R equivalent
y	year%y, %Ym (with y and d) month%m, %b, %h, %Bd	day%d, %eh	hour%H, %I%pm (with h and s) minute%Ms	seconds%Symdymd_hmse.g. () for parsing a date with the year followed by the month followed by the day, e.g. "2016-07-22", or () for parsing a datetime in the order year, month, day, hours, minutes, seconds, e.g. "2016-07-22 13:04:47".
The functions are able to recognize most separators (such as /, -, and whitespace) without additional arguments. They also work with inconsistent separators.
Dates

Utility functions
as.POSIXctDatetimes can be parsed using ymd_hms variants including ymd_hm and ymd_h. All datetime functions can accept a tz timezone argument akin to that of  or strptime, but which defaults to "UTC" instead of the local timezone.

as.POSIXctParser functions lubridate also includes three functions for parsing datetimes with a formatting string like  or strptime:
	Function	Output Class	Formatting strings accepted
Flexible. Will accept strptime-style with % or lubridate datetime
parse_date_time POSIXct	function name style, e.g "ymd hms". Will accept a vector of orders for heterogeneous data and guess which is appropriate.
Default POSIXct; if lt Strict. Accepts only strptime tokens (with or without %) from a limited
parse_date_time2
 TRUE	=, POSIXlt	set.
lt Default POSIXlt; if = Strict. Accepts only %-delimited strptime tokens with delimiters (-, /, :,
parse_date_time2 and fast_strptime use a fast C parser for efficiency.
parse_date_timeSee ? for formatting tokens.
Section 55.2: Dierence between period and duration
Unlike durations, periods can be used to accurately model clock times without knowing when events such as leap seconds, leap days, and DST changes occur.

Section 55.3: Instants
is.instantAn instant is a specific moment in time. Any date-time object that refers to a moment of time is recognized as an instant. To test if an object is an instant, use .


Section 55.4: Intervals, Durations and Periods
Intervals are simplest way of recording timespans in lubridate. An interval is a span of time that occurs between two specific instants.

Durations measure the exact amount of time that occurs between two instants.

Note: Units larger than weeks are not used due to their variability.
Durations can be created using dseconds, dminutes and other duration helper functions.
quick_durationsRun ? for complete list.

Durations can be subtracted and added to instants to get new instants.


Durations can be created from intervals.

Periods measure the change in clock time that occurs between two instants.
quick_periodsPeriods can be created using period function as well other helper functions like seconds, hours, etc. To get a complete list of period helper functions, Run ?.

is.period function can be used to check if an object is a period.

Section 55.5: Manipulating date and time in lubridate


Section 55.6: Time Zones
with_tz returns a date-time as it would appear in a different time zone.

force_tz returns a the date-time that has the same clock time as x in the new time zone.

Section 55.7: Parsing date and time in lubridate
ymd()Lubridate provides  series of functions for parsing character strings into dates. The letters y, m, and d correspond to the year, month, and day elements of a date-time.

Section 55.8: Rounding dates

round_date() takes a date-time object and rounds it to the nearest integer value of the specified time unit.


floor_date() takes a date-time object and rounds it down to the nearest integer value of the specified time unit.

ceiling_date() takes a date-time object and rounds it up to the nearest integer value of the specified time unit.


Chapter 56: Time Series and Forecasting
Section 56.1: Creating a ts object
Time series data can be stored as a ts object. ts objects contain information about seasonal frequency that is used by ARIMA functions. It also allows for calling of elements in the series by date using the window command.


Section 56.2: Exploratory Data Analysis with time-series data

In the spirit of Exploratory Data Analysis (EDA) a good first step is to look at a plot of your time-series data:
plot(AirPassengers) # plot the raw data
abline(reg=lm(AirPassengers~time(AirPassengers))) # fit a trend line

For further EDA we examine cycles across years:
cycle(AirPassengers)
     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
1949 1   2   3   4   5   6   7   8   9  10  11  12
1950 1   2   3   4   5   6   7   8   9  10  11  12
1951 1   2   3   4   5   6   7   8   9  10  11  12
1952 1   2   3   4   5   6   7   8   9  10  11  12
1953 1   2   3   4   5   6   7   8   9  10  11  12
1954 1   2   3   4   5   6   7   8   9  10  11  12
1955 1   2   3   4   5   6   7   8   9  10  11  12
1956 1   2   3   4   5   6   7   8   9  10  11  12
1957 1   2   3   4   5   6   7   8   9  10  11  12
1958 1   2   3   4   5   6   7   8   9  10  11  12
1959 1   2   3   4   5   6   7   8   9  10  11  12
1960 1   2   3   4   5   6   7   8   9  10  11  12
boxplot(AirPassengers~cycle(AirPassengers)) #Box plot across months to explore seasonal effects

Chapter 57: strsplit function
Section 57.1: Introduction
strsplit is a useful function for breaking up a vector into an list on some character pattern. With typical R tools, the whole list can be reincorporated to a data.frame or part of the list might be used in a graphing exercise.
Here is a common usage of strsplit: break a character vector along a comma separator:

As hinted above, the split argument is not limited to characters, but may follow a pattern dictated by a regular expression. For example, temp2 is identical to temp above except that the separators have been altered for each item. We can take advantage of the fact that the split argument accepts regular expressions to alleviate the irregularity in the vector.

Notes:
1. breaking down the regular expression syntax is out of scope for this example.
2. Sometimes matching regular expressions can slow down a process. As with many R functions that allow the use of regular expressions, the fixed argument is available to tell R to match on the split characters literally.
Chapter 58: Web scraping and parsing
Section 58.1: Basic scraping with rvest
rvest is a package for web scraping and parsing by Hadley Wickham inspired by Python's Beautiful Soup. It leverages Hadley's xml2 package's libxml2 bindings for HTML parsing.
As part of the tidyverse, rvest is piped. It uses
 to scrape the HTML of a webpage,
xml2::read_htmlwhich can then be subset with its html_node and html_nodes functions using CSS or XPath selectors, and parsed to R objects with functions like html_text and html_table.
To scrape the table of milestones from the Wikipedia page on R, the code would look like
library(rvest) url <- 'https://en.wikipedia.org/wiki/R_(programming_language)'
        # scrape HTML from website url %>% read_html() %>%     # select HTML tag with class="wikitable"     html_node(css = '.wikitable') %>%     # parse table into data.frame     html_table() %>%     # trim for printing     dplyr::mutate(Description = substr(Description, 1, 70))
##    Release       Date                                                  Description
## 1     0.16            This is the last alpha version developed primarily by Ihaka
## 2     0.49 1997-04-23 This is the oldest source release which is currently availab ## 3     0.60 1997-12-05 R becomes an official part of the GNU Project. The code is h ## 4   0.65.1 1999-10-07 First versions of update.packages and install.packages funct ## 5      1.0 2000-02-29 Considered by its developers stable enough for production us ## 6      1.4 2001-12-19 S4 methods are introduced and the first version for Mac OS X
## 7      2.0 2004-10-04 Introduced lazy loading, which enables fast loading of data
## 8      2.1 2005-04-18 Support for UTF-8 encoding, and the beginnings of internatio ## 9     2.11 2010-04-22                          Support for Windows 64 bit systems. ## 10    2.13 2011-04-14 Adding a new compiler function that allows speeding up funct ## 11    2.14 2011-10-31 Added mandatory namespaces for packages. Added a new paralle ## 12    2.15 2012-03-30 New load balancing functions. Improved serialization speed f ## 13     3.0 2013-04-03 Support for numeric index values 231 and larger on 64 bit sy
While this returns a data.frame, note that as is typical for scraped data, there is still further data cleaning to be done: here, formatting dates, inserting NAs, and so on.
Note that data in a less consistently rectangular format may take looping or other further munging to successfully parse. If the website makes use of jQuery or other means to insert content, read_html may be insufficient to scrape, and a more robust scraper like RSelenium may be necessary.
Section 58.2: Using rvest when login is required
I common problem encounter when scrapping a web is how to enter a userid and password to log into a web site.
In this example which I created to track my answers posted here to stack overflow. The overall flow is to login, go to a web page collect information, add it a dataframe and then move to the next page.

The loop in this case is limited to only 5 pages, this needs to change to fit your application. I replaced the user specific values with ******, hopefully this will provide some guidance for you problem.
Chapter 59: Generalized linear models
Section 59.1: Logistic regression on Titanic dataset
Logistic regression is a particular case of the generalized linear model, used to model dichotomous outcomes (probit and complementary log-log models are closely related).
The name comes from the link function used, the logit or log-odds function. The inverse function of the logit is called the logistic function and is given by:

This function takes a value between ]-Inf;+Inf[ and returns a value between 0 and 1; i.e the logistic function takes a linear predictor and returns a probability.
family = binomialLogistic regression can be performed using the glm function with the option  (shortcut for
family = binomial(link="logit"); the logit being the default link function for the binomial family).
In this example, we try to predict the fate of the passengers aboard the RMS Titanic.
Read the data:
url <- "http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt" titanic <- read.csv(file = url, stringsAsFactors = FALSE)
Clean the missing values:
In that case, we replace the missing values by an approximation, the average. titanic$age[is.na(titanic$age)] <- mean(titanic$age, na.rm = TRUE) Train the model:
 titanic.train <- glm(survived ~ pclass + sex + age,                          family = binomial, data = titanic) Summary of the model:

The output:

Next we see the deviance residuals, which are a measure of model fit. This part of output shows the distribution of the deviance residuals for individual cases used in the model.
 The next part of the output shows the coefficients, their standard errors, the z-statistic (sometimes called a Wald z-statistic), and the associated p-values.
The qualitative variables are "dummified". A modality is considered as the reference. The reference modality can be change with I in the formula.
All four predictors are statistically significant at a 0.1 % level.
The logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variable.
To see the odds ratio (multiplicative change in the odds of survival per unit increase in a predictor variable), exponentiate the parameter.
To see the confidence interval (CI) of the parameter, use confint.
 Below the table of coefficients are fit indices, including the null and deviance residuals and the Akaike Information Criterion (AIC), which can be used for comparing model performance.
When comparing models fitted by maximum likelihood to the same data, the smaller the AIC, the better the fit.
One measure of model fit is the significance of the overall model. This test asks whether the model with predictors fits significantly better than a model with just an intercept (i.e., a null model).
Example of odds ratios:

With this model, compared to the first class, the 3rd class passengers have about a tenth of the odds of survival.
Example of confidence interval for the parameters:


Exemple of calculating the significance of the overall model:
The test statistic is distributed chi-squared with degrees of freedom equal to the differences in degrees of freedom between the current and the null model (i.e., the number of predictor variables in the model).

The p-value is near 0, showing a strongly significant model.

Chapter 60: Reshaping data between long and wide forms
In R, tabular data is stored in data frames. This topic covers the various ways of transforming a single table.
Section 60.1: Reshaping data
Often data comes in tables. Generally one can divide this tabular data in wide and long formats. In a wide format, each variable has its own column.
Person Height [cm] Age [yr]
Alison	178	20
Bob	174	45
Carl	182	31
However, sometimes it is more convenient to have a long format, in which all variables are in one column and the values are in a second column.
Person Variable Value Alison Height [cm] 178
Bob	Height [cm] 174
Carl	Height [cm] 182
Alison	Age [yr]	20
Bob	Age [yr]	45
Carl	Age [yr]	31
Base R, as well as third party packages can be used to simplify this process. For each of the options, the mtcars dataset will be used. By default, this dataset is in a long format. In order for the packages to work, we will insert the row names as the first column.

Base R
stackunstackThere are two functions in base R that can be used to convert between wide and long format: () and ().

However, these functions can become very complex for more advanced use cases. Luckily, there are other options using third party packages. The tidyr package
gather() to convert from wide to long and spreadThis package uses () to convert from long to wide.
library(tidyr) long <- gather(data, variable, value, 2:12) # where variable is the name of the
# variable column, value indicates the name of the value column and 2:12 refers to

meltdcastThe data.table package extends the reshape2 functions and uses the function () to go from wide to long and () to go from long to wide.

Section 60.2: The reshape function
reshapeThe most flexible base R function for reshaping data is reshape. See ? for its syntax.
# create unbalanced longitudinal (panel) data set set.seed(1234) df <- data.frame(identifier=rep(1:5, each=3),                  location=rep(c("up", "down", "left", "up", "center"), each=3),                  period=rep(1:3, 5), counts=sample(35, 15, replace=TRUE),                  values=runif(15, 5, 10))[-c(4,8,11),] df
   identifier location period counts   values 1           1       up      1      4 9.186478
2 1       up      2     22 6.431116
3 1       up      3     22 6.334104
5 2     down      2     31 6.161130
6 2     down      3     23 6.583062 7           3     left      1      1 6.513467
9           3     left      3     24 5.199980 10          4       up      1     18 6.093998
12 4       up      3     20 7.628488
13 5   center      1     10 9.573291
14 5   center      2     33 9.156725
15 5   center      3     11 5.228851
Note that the data.frame is unbalanced, that is, unit 2 is missing an observation in the first period, while units 3 and 4 are missing observations in the second period. Also, note that there are two variables that vary over the periods: counts and values, and two that do not vary: identifier and location.
Long to Wide
To reshape the data.frame to wide format,
# reshape wide on time variable df.wide <- reshape(df, idvar="identifier", timevar="period",
                   v.names=c("values", "counts"), direction="wide") df.wide    identifier location values.1 counts.1 values.2 counts.2 values.3 counts.3 1           1       up 9.186478        4 6.431116       22 6.334104       22
5           2     down       NA       NA 6.161130       31 6.583062       23 7           3     left 6.513467        1       NA       NA 5.199980       24 10          4       up 6.093998       18       NA       NA 7.628488       20 13          5   center 9.573291       10 9.156725       33 5.228851       11
Notice that the missing time periods are filled in with NAs.
In reshaping wide, the "v.names" argument specifies the columns that vary over time. If the location variable is not necessary, it can be dropped prior to reshaping with the "drop" argument. In dropping the only non-varying / nonid column from the data.frame, the v.names argument becomes unnecessary.

Wide to Long
To reshape long with the current df.wide, a minimal syntax is

However, this is typically trickier:
# remove "." separator in df.wide names for counts and values names(df.wide)[grep("\\.", names(df.wide))] <-
              gsub("\\.", "", names(df.wide)[grep("\\.", names(df.wide))])
Now the simple syntax will produce an error about undefined columns.
With column names that are more difficult for the reshape function to automatically parse, it is sometimes necessary to add the "varying" argument which tells reshape to group particular variables in wide format for the transformation into long format. This argument takes a list of vectors of variable names or indices.

In reshaping long, the "v.names" argument can be provided to rename the resulting varying variables.
Sometimes the specification of "varying" can be avoided by use of the "sep" argument which tells reshape what part of the variable name specifies the value argument and which specifies the time argument.
Chapter 61: RMarkdown and knitr presentation
Parameter	definition
title	the title of the document author	The author of the document
r format(Sys.time(), '%d %B, %Y'date	The date of the document: Can be ")" author	The author of the document
           The output format of the document: at least 10 format available. For html document, html_output. For output
PDF document, pdf_document, ..
Section 61.1: Adding a footer to an ioslides presentation
Adding a footer is not natively possible. Luckily, we can make use of jQuery and CSS to add a footer to the slides of an ioslides presentation rendered with knitr. First of all we have to include the jQuery plugin. This is done by the line
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.2/jquery.min.js"></script>
Now we can use jQuery to alter the DOM (document object model) of our presentation. In other words: we alter the
 ...document).ready(functionHTML structure of the document. As soon as the presentation is loaded ($(() {
title-slide, .backdrop, or .segue})), we select all slides, that do not have the class attributes . and add the tag
<footer></footer> right before each slide is 'closed' (so before </slide>). The attribute label carries the content that will be displayed later on.
All we have to do now is to layout our footer with CSS:
<footer> (footer::afterAfter each ):
display the content of the attribute label use font size 12 position the footer (20 pixels from the bottom of the slide and 60 pxs from the left)
(the other properties can be ignored but might have to be modified if the presentation uses a different style template).


The result will look like this:

Section 61.2: Rstudio example
This is a script saved as .Rmd, on the contrary of r scripts saved as .R.
To knit the script, either use the render function or use the shortcut button in Rstudio.



Chapter 62: Scope of variables
Section 62.1: Environments and Functions
Variables declared inside a function only exist (unless passed) inside that function.

Variables passed into a function and then reassigned are overwritten, but only inside the function.

Variables assigned in a higher environment than a function exist within that function, without being passed.

Section 62.2: Function Exit
on.exit()The  function is handy for variable clean up if global variables must be assigned.
Some parameters, especially those for graphics, can only be set globally. This small function is common when creating more specialized plots.

Section 62.3: Sub functions
Functions called within a function (ie subfunctions) must be defined within that function to access any variables defined in the local environment without being passed.


Section 62.4: Global Assignment
barVariables can be assigned globally from any environment using <<-. () can now access y.


Global assignment is highly discouraged. Use of a wrapper function or explicitly calling variables from another local environment is greatly preferred.
Section 62.5: Explicit Assignment of Environments and Variables
Environments in R can be explicitly call and named. Variables can be explicitly assigned and call to or from those environments.
package:base or a subenvironment within package:baseA commonly created environment is one which encloses .

Variables can be explicitly assigned and call to or from those environments.

Since e2 inherits from e1, a is 3 in both e1 and e2. However, assigning a within e2 does not change the value of a in

Chapter 63: Performing a Permutation Test
Section 63.1: A fairly general function
We will use the built in tooth growth dataset. We are interested in whether there is a statistically significant difference in tooth growth when the guinea pigs are given vitamin C vs orange juice.
Here's the full example:

After we read in the CSV, we define the function

This function takes two vectors, and shuffles their contents together, then performs the function testStat on the shuffled vectors. The result of teststat is added to trials, which is the return value.
N = 10It does this ^5 times. Note that the value N could very well have been a parameter to the function.
This leaves us with a new set of data, trials, the set of means that might result if there truly is no relationship between the two variables.
Now to define our test statistic:

Perform the test:

Calculate our actual observed mean difference:

Let's see what our observation looks like on a histogram of our test statistic.

It doesn't look like our observed result is very likely to occur by random chance...
We want to calculate the p-value, the likeliehood of the original observed result if their is no relationship between the two variables.

Let's break that down a bit:

Will create a boolean vector, like:

With TRUE every time the value of result is greater than or equal to the observedMean.
The function mean will interpret this vector as 1 for TRUE and 0 for FALSE, and give us the percentage of 1's in the mix, ie the number of times our shuffled vector mean difference surpassed or equalled what we observed.
Finally, we multiply by 2 because the distribution of our test statistic is highly symmetric, and we really want to know which results are "more extreme" than our observed result.
All that's left is to output the p-value, which turns out to be 0.06093939. Interpretation of this value is subjective, but I would say that it looks like Vitamin C promotes tooth growth quite a lot more than Orange Juice does.

Chapter 64: xgboost
Section 64.1: Cross Validation and Tuning with xgboost


Chapter 65: R code vectorization best practices
Section 65.1: By row operations
The key in vectorizing R code, is to reduce or eliminate "by row operations" or method dispatching of R functions.
That means that when approaching a problem that at first glance requires "by row operations", such as calculating the means of each row, one needs to ask themselves:
What are the classes of the data sets I'm dealing with?
Is there an existing compiled code that can achieve this without the need of repetitive evaluation of R functions?
If not, can I do these operation by columns instead by row?
Finally, is it worth spending a lot of time on developing complicated vectorized code instead of just running a simple apply loop? In other words, is the data big/sophisticated enough that R can't handle it efficiently using a simple loop?
Putting aside the memory pre-allocation issue and growing object in loops, we will focus in this example on how to possibly avoid apply loops, method dispatching or re-evaluating R functions within loops.
A standard/easy way of calculating mean by row would be:
apply(mtcars, 1, mean)           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive   Hornet Sportabout
            Valiant          Duster 360
           29.90727            29.98136            23.59818            38.73955            53.66455            35.04909            59.72000           Merc 240D            Merc 230            Merc 280           Merc 280C          Merc 450SE
         Merc 450SL         Merc 450SLC
           24.63455            27.23364            31.86000            31.78727            46.43091            46.50000            46.35000  Cadillac Fleetwood Lincoln Continental   Chrysler Imperial            Fiat 128         Honda Civic
     Toyota Corolla       Toyota Corona
           66.23273            66.05855            65.97227            19.44091            17.74227
           18.81409            24.88864
   Dodge Challenger         AMC Javelin          Camaro Z28    Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa
           47.24091            46.00773            58.75273            57.37955            18.92864
           24.77909            24.88027      Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E
           60.97182            34.50818            63.15545            26.26273
But can we do better? Lets's see what happened here:
data.frame1. First, we converted a  to a matrix. (Note that his happens within the apply function.) This is both inefficient and dangerous. a matrix can't hold several column types at a time. Hence, such conversion will
apply(iris, 2, classstr(iris) or with sapply(iris, classprobably lead to loss of information and some times to misleading results (compare ) with )).
nrow(mtcars2. Second of all, we performed an operation repetitively, one time for each row. Meaning, we had to evaluate some R function ) times. In this specific case, mean is not a computationally expensive function, hence R could likely easily handle it even for a big data set, but what would happen if we need to calculate the standard deviation by row (which involves an expensive square root operation)? Which brings us to the next point:
3. We evaluated the R function many times, but maybe there already is a compiled version of this operation?
Indeed we could simply do:
rowMeans(mtcars)           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive   Hornet Sportabout
            Valiant          Duster 360
           29.90727            29.98136            23.59818            38.73955            53.66455            35.04909            59.72000           Merc 240D            Merc 230            Merc 280           Merc 280C          Merc 450SE
         Merc 450SL         Merc 450SLC
           24.63455            27.23364            31.86000            31.78727            46.43091            46.50000            46.35000  Cadillac Fleetwood Lincoln Continental   Chrysler Imperial            Fiat 128         Honda Civic
     Toyota Corolla       Toyota Corona
           66.23273            66.05855            65.97227            19.44091            17.74227
           18.81409            24.88864
   Dodge Challenger         AMC Javelin          Camaro Z28    Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa
           47.24091            46.00773            58.75273            57.37955            18.92864
           24.77909            24.88027      Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E
           60.97182            34.50818            63.15545            26.26273
data.frameThis involves no by row operations and therefore no repetitive evaluation of R functions. However, we still converted a  to a matrix. Though rowMeans has an error handling mechanism and it won't run on a data set that it can't handle, it's still has an efficiency cost.

data.frameBut still, can we do better? We could try instead of a matrix conversion with error handling, a different method that will allow us to use mtcars as a vector (because a  is essentially a list and a list is a vector).
Reduce(`+`, mtcars)/ncol(mtcars)
 [1] 29.90727 29.98136 23.59818 38.73955 53.66455 35.04909 59.72000 24.63455 27.23364 31.86000
31.78727 46.43091 46.50000 46.35000 66.23273 66.05855 [17] 65.97227 19.44091 17.74227 18.81409 24.88864 47.24091 46.00773 58.75273 57.37955 18.92864
24.77909 24.88027 60.97182 34.50818 63.15545 26.26273
Now for possible speed gain, we lost column names and error handling (including NA handling).
Another example would be calculating mean by group, using base R we could try
aggregate(. ~ cyl, mtcars, mean) cyl      mpg     disp        hp     drat       wt     qsec        vs        am     gear     carb 1   4 26.66364 105.1364  82.63636 4.070909 2.285727 19.13727 0.9090909 0.7272727 4.090909 1.545455
2 6 19.74286 183.3143 122.28571 3.585714 3.117143 17.97714 0.5714286 0.4285714 3.857143 3.428571
3 8 15.10000 353.1000 209.21429 3.229286 3.999214 16.77214 0.0000000 0.1428571 3.285714 3.500000
Still, we are basically evaluating an R function in a loop, but the loop is now hidden in an internal C function (it matters little whether it is a C or an R loop).
Could we avoid it? Well there is a compiled function in R called rowsum, hence we could do:
rowsum(mtcars[-2], mtcars$cyl)/table(mtcars$cyl) mpg     disp        hp     drat       wt     qsec        vs        am     gear     carb
4 26.66364 105.1364  82.63636 4.070909 2.285727 19.13727 0.9090909 0.7272727 4.090909 1.545455
6 19.74286 183.3143 122.28571 3.585714 3.117143 17.97714 0.5714286 0.4285714 3.857143 3.428571
8 15.10000 353.1000 209.21429 3.229286 3.999214 16.77214 0.0000000 0.1428571 3.285714 3.500000
Though we had to convert to a matrix first too.
data.frameA this point we may question whether our current data structure is the most appropriate one. Is a  is the best practice? Or should one just switch to a matrix data structure in order to gain efficiency?
By row operations will get more and more expensive (even in matrices) as we start to evaluate expensive functions each time. Lets us consider a variance calculation by row example.
Lets say we have a matrix m:
set.seed(100) m <- matrix(sample(1e2), 10) m       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
 [1,]    8   33   39   86   71  100   81   68   89    84
 [2,]   12   16   57   80   32   82   69   11   41    92
 [3,]   62   91   53   13   42   31   60   70   98    79
 [4,]   66   94   29   67   45   59   20   96   64     1
 [5,]   36   63   76    6   10   48   85   75   99     2
 [6,]   18    4   27   19   44   56   37   95   26    40
 [7,]    3   24   21   25   52   51   83   28   49    17
 [8,]   46    5   22   43   47   74   35   97   77    65
 [9,]   55   54   78   34   50   90   30   61   14    58
 [10,]   88   73   38   15    9   72    7   93   23    87 One could simply do:

On the other hand, one could also completely vectorize this operation by following the formula of variance

Chapter 66: Missing values
When we don't know the value a variable takes, we say its value is missing, indicated by NA.
Section 66.1: Examining missing data
is.naanyNA reports whether any missing values are present; while  reports missing values elementwise:

ìs.na returns a logical vector that is coerced to integer values under arithmetic operations (with FALSE=0, TRUE=1). We can use this to find out how many missing values there are:

is.naExtending this approach, we can use colSums and  on a data frame to count NAs per column:

The naniar package (currently on github but not CRAN) offers further tools for exploring missing values.
Section 66.2: Reading and writing data with NA values
read.When reading tabular datasets with the * functions, R automatically looks for missing values that look like
"NA". However, missing values are not always represented by NA. Sometimes a dot (.), a hyphen(-) or a character-
na.strings parameter of the read.value (e.g.: empty) indicates that a value is NA. The * function can be used to tell
R which symbols/characters need to be treated as NA values:

It is also possible to indicate that more than one symbol needs to be read as NA:

write.csvSimilarly, NAs can be written with customized strings using the na argument to . Other tools for reading and writing tables have similar options.
Section 66.3: Using NAs of dierent classes
The symbol NA is for a logical missing value:

This is convenient, since it can easily be coerced to other atomic vector types, and is therefore usually the only NA you will need:

If you do need a single NA value of another type, use NA_character_, NA_integer_, NA_real_ or NA_complex_. For missing values of fancy classes, subsetting with NA_integer_ usually works; for example, to get a missing-value Date:

Section 66.4: TRUE/FALSE and/or NA
NA is a logical type and a logical operator with an NA will return NA if the outcome is ambiguous. Below, NA OR TRUE evaluates to TRUE because at least one side evaluates to TRUE, however NA OR FALSE returns NA because we do not know whether NA would have been TRUE or FALSE

These properties are helpful if you want to subset a data set based on some columns that contain NA.

Chapter 67: Hierarchical Linear Modeling
Section 67.1: basic model fitting
apologies: since I don't know of a channel for discussing/providing feedback on requests for improvement, I'm going to put my question here. Please feel free to point out a better place for this! @DataTx states that this is "completely unclear, incomplete, or has severe formatting problems". Since I don't see any big formatting problems (:-) ), a little bit more guidance about what's expected here for improving clarity or completeness, and why what's here is unsalvageable, would be useful.
The primary packages for fitting hierarchical (alternatively "mixed" or "multilevel") linear models in R are nlme
(older) and lme4 (newer). These packages differ in many minor ways but should generally result in very similar fitted models.

Differences to consider:
vignette("lmer",package="lme4"formula syntax is slightly different nlme is (still) somewhat better documented (e.g. Pinheiro and Bates 2000 Mixed-effects models in S-PLUS;
however, see Bates et al. 2015 Journal of Statistical Software/) for lme4) lme4 is faster and allows easier fitting of crossed random effects
nlme provides p-values for linear mixed models out of the box, lme4 requires add-on packages such as lmerTest or afex nlme allows modeling of heteroscedasticity or residual correlations (in space/time/phylogeny)
The unofficial GLMM FAQ provides more information, although it is focused on generalized linear mixed models (GLMMs).
Chapter 68: *apply family of functions (functionals)
Section 68.1: Using built-in functionals
Built-in functionals: lapply(), sapply(), and mapply()
R comes with built-in functionals, of which perhaps the most well-known are the apply family of functions. Here is a description of some of the most common apply functions:
lapplysapply() = the same as lapplyvapply() = a variant of sapplymapply() = like lapply() = takes a list as an argument and applies the specified function to the list.
() but attempts to simplify the output to a vector or a matrix.
() in which the output object's type must be specified.
() but can pass multiple vectors as input to the specified function. Can be simplified
sapplylike ().
Map() is an alias to mapply() with SIMPLIFY = FALSE		.
lapply()

sapply()
sapply() will attempt to resolve its output to either a vector or a matrix.
# Two examples to show the different outputs of sapply() sapply(letters, print)  ## produces a vector x <- list(a = 1:10, beta = exp(-3:3), logic = c(TRUE,FALSE,FALSE,TRUE)) sapply(x, quantile)  ## produces a matrix
mapply()
mapply() works much like lapply() except it can take multiple vectors as input (hence the m for multivariate).

Section 68.2: Combining multiple `data.frames` (`lapply`, `mapply`)
In this exercise, we will generate four bootstrap linear regression models and combine the summaries of these models into a single data frame.

At this point, we can take two approaches to inserting the names into the data.frame.

If you're a fan of magrittr style pipes, you can accomplish the entire task in a single chain (though it may not be prudent to do so if you need any of the intermediary objects, such as the model objects themselves):

Section 68.3: Bulk File Loading
for a large number of files which may need to be operated on in a similar process and with well structured file names.
firstly a vector of the file names to be accessed must be created, there are multiple options for this:

 files <- list.files("./", pattern = "\\.rds$", full.names = TRUE)
rdswhere X is a vector of part of the files naming format used. lapply will output each response as element of a list. readRDS is specific to . files and will change depending on the application of the process.

This is not necessarily faster than a for loop from testing but allows all files to be an element of a list without assigning them explicitly.
libraryFinally, we often need to load multiple packages at once. This trick can do it quite easily by applying () to all libraries that we wish to import: lapply(c("jsonlite","stringr","igraph"),library,character.only=TRUE)
Section 68.4: Using user-defined functionals
User-defined functionals
Users can create their own functionals to varying degrees of complexity. The following examples are from Functionals by Hadley Wickham:

set.seedIn the first case, randomise accepts a single argument f, and calls it on a sample of Uniform random variables. To demonstrate equivalence, we call  below:


base::lapplyna.rmThe second example is a re-implementation of , which uses functionals to apply an operation (f) to each element in a list (x). The ... parameter allows the user to pass additional arguments to f, such as the option in the mean function:

Chapter 69: Text mining
Section 69.1: Scraping Data to build N-gram Word Clouds
The following example utilizes the tm text mining package to scrape and mine text data from the web to build word clouds with symbolic shading and ordering.
require(RWeka) require(tau) require(tm) require(tm.plugin.webmining) require(wordcloud)
# Scrape Google Finance --------------------------------------------------googlefinance <- WebCorpus(GoogleFinanceSource("NASDAQ:LFVN"))  
# Scrape Google News -----------------------------------------------------lv.googlenews <- WebCorpus(GoogleNewsSource("LifeVantage"))
p.googlenews  <- WebCorpus(GoogleNewsSource("Protandim")) ts.googlenews <- WebCorpus(GoogleNewsSource("TrueScience"))
 
# Scrape NYTimes ---------------------------------------------------------lv.nytimes <- WebCorpus(NYTimesSource(query = "LifeVantage", appid = nytimes_appid))
p.nytimes  <- WebCorpus(NYTimesSource("Protandim", appid = nytimes_appid)) ts.nytimes <- WebCorpus(NYTimesSource("TrueScience", appid = nytimes_appid))  
# Scrape Reuters ---------------------------------------------------------lv.reutersnews <- WebCorpus(ReutersNewsSource("LifeVantage"))
p.reutersnews  <- WebCorpus(ReutersNewsSource("Protandim")) ts.reutersnews <- WebCorpus(ReutersNewsSource("TrueScience"))
 
# Scrape Yahoo! Finance --------------------------------------------------lv.yahoofinance <- WebCorpus(YahooFinanceSource("LFVN"))
 
# Scrape Yahoo! News -----------------------------------------------------lv.yahoonews <- WebCorpus(YahooNewsSource("LifeVantage"))
p.yahoonews  <- WebCorpus(YahooNewsSource("Protandim")) ts.yahoonews <- WebCorpus(YahooNewsSource("TrueScience"))
 
# Scrape Yahoo! Inplay ---------------------------------------------------lv.yahooinplay <- WebCorpus(YahooInplaySource("LifeVantage"))
# Text Mining the Results ------------------------------------------------corpus <- c(googlefinance, lv.googlenews, p.googlenews, ts.googlenews, lv.yahoofinance, lv.yahoonews, p.yahoonews, ts.yahoonews, lv.yahooinplay) #lv.nytimes, p.nytimes, ts.nytimes,lv.reutersnews, p.reutersnews, ts.reutersnews,  inspect(corpus) wordlist <- c("lfvn", "lifevantage", "protandim", "truescience", "company", "fiscal", "nasdaq")
 ds0.1g <- tm_map(corpus, content_transformer(tolower))
ds1.1g <- tm_map(ds0.1g, content_transformer(removeWords), wordlist)
ds1.1g <- tm_map(ds1.1g, content_transformer(removeWords), stopwords("english")) ds2.1g <- tm_map(ds1.1g, stripWhitespace) ds3.1g <- tm_map(ds2.1g, removePunctuation) ds4.1g <- tm_map(ds3.1g, stemDocument)
 tdm.1g <- TermDocumentMatrix(ds4.1g) dtm.1g <- DocumentTermMatrix(ds4.1g)


random.orderNote the use of  and a sequential pallet from RColorBrewer, which allows the programmer to capture more information in the cloud by assigning meaning to the order and coloring of terms.
Above is the 1-gram case.
We can make a major leap to n-gram word clouds and in doing so we'll see how to make almost any text-mining analysis flexible enough to handle n-grams by transforming our TDM.
The initial difficulty you run into with n-grams in R is that tm, the most popular package for text mining, does not inherently support tokenization of bi-grams or n-grams. Tokenization is the process of representing a word, part of a word, or group of words (or symbols) as a single data element called a token.
textcntFortunately, we have some hacks which allow us to continue using tm with an upgraded tokenizer. There's more than one way to achieve this. We can write our own simple tokenizer using the () function from tau:
tokenize_ngrams <- function(x, n=3)
 return(rownames(as.data.frame(unclass(textcnt(x,method="string",n=n))))) or we can invoke RWeka's tokenizer within tm:

From this point you can proceed much as in the 1-gram case:


The example above is reproduced with permission from Hack-R's data science blog. Additional commentary may be found in the original article.
Chapter 70: ANOVA
Section 70.1: Basic usage of aov()
Analysis of Variance (aov) is used to determine if the means of two or more groups differ significantly from each other. Responses are assumed to be independent of each other, Normally distributed (within each group), and the within-group variances are assumed equal.
aovlmaovIn order to complete the analysis data must be in long format (see reshaping data topic). () is a wrapper around the () function, using Wilkinson-Rogers formula notation y~f where y is the response (independent) variable and f is a factor (categorical) variable representing group membership. If f is numeric rather than a factor variable, () will report the results of a linear regression in ANOVA format, which may surprise inexperienced users.
aovThe () function uses Type I (sequential) Sum of Squares. This type of Sum of Squares tests all of the (main and interaction) effects sequentially. The result is that the first effect tested is also assigned shared variance between it and other effects in the model. For the results from such a model to be reliable, data should be balanced (all groups are of the same size).
When the assumptions for Type I Sum of Squares do not hold, Type II or Type III Sum of Squares may be applicable. Type II Sum of Squares test each main effect after every other main effect, and thus controls for any overlapping variance. However, Type II Sum of Squares assumes no interaction between the main effects.
Lastly, Type III Sum of Squares tests each main effect after every other main effect and every interaction. This makes Type III Sum of Squares a necessity when an interaction is present.
AnovaType II and Type III Sums of Squares are implemented in the () function.
Using the mtcars data set as an example.

To view summary of ANOVA model:

lmOne can also extract the coefficients of the underlying () model:

Section 70.2: Basic usage of Anova()
AnovaWhen dealing with an unbalanced design and/or non-orthogonal contrasts, Type II or Type III Sum of Squares are necessary. The () function from the car package implements these. Type II Sum of Squares assumes no interaction between main effects. If interactions are assumed, Type III Sum of Squares is appropriate.
Anova() function wraps around the lmThe () function.
Using the mtcars data sets as an example, demonstrating the difference between Type II and Type III when an interaction is tested.



Chapter 71: Raster and Image Analysis
See also I/O for Raster Images
Section 71.1: Calculating GLCM Texture
Gray Level Co-Occurrence Matrix (Haralick et al. 1973) texture is a powerful image feature for image analysis. The glcm package provides a easy-to-use function to calculate such texutral features for RasterLayer objects in R.

Calculating GLCM textures in one direction


Calculation rotation-invariant texture features
The textural features can also be calculated in all 4 directions (0°, 45°, 90° and 135°) and then combined to one rotation-invariant texture. The key for this is the shift parameter:


Section 71.2: Mathematical Morphologies
The package mmand provides functions for the calculation of Mathematical Morphologies for n-dimensional arrays. With a little workaround, these can also be calculated for raster images.

At first, a kernel (moving window) has to be set with a size (e.g. 9x9) and a shape type (e.g. disc, box or diamond)

erodeAfterwards, the raster layer has to be converted into an array wich is used as input for the () function.

erode(), also the morphological functions dilate(), opening() and closingBesides () can be applied like this.

Chapter 72: Survival analysis
Section 72.1: Random Forest Survival Analysis with randomForestSRC
Just as the random forest algorithm may be applied to regression and classification tasks, it can also be extended to survival analysis.
In the example below a survival model is fit and used for prediction, scoring, and performance analysis using the package randomForestSRC from CRAN.

Section 72.2: Introduction - basic fitting and plotting of parametric survival models with the survival package
survreg()survfit()survival is the most commonly used package for survival analysis in R. Using the built-in lung dataset we can get started with Survival Analysis by fitting a regression model with the  function, creating a curve with , and plotting predicted survival curves by calling the predict method for this package with new data.
In the example below we plot 2 predicted curves and vary sex between the 2 sets of new data, to visualize its effect:


Section 72.3: Kaplan Meier estimates of survival curves and risk set tables with survminer
Base plot


More advanced
ggsurvplot(    fit,                     # survfit object with calculated statistics.    risk.table = TRUE,       # show risk table.    pval = TRUE,             # show p-value of log-rank test.    conf.int = TRUE,         # show confidence intervals for                             # point estimaes of survival curves.
   xlim = c(0,2000),        # present narrower X axis, but not affect
                            # survival estimates.    break.time.by = 500,     # break X axis in time intervals by 500.
   ggtheme = theme_RTCGA(), # customize plot and risk table with a theme.  risk.table.y.text.col = T, # colour risk table text annotations.
  risk.table.y.text = FALSE # show bars instead of names in text annotations
                            # in legend of risk table
)

Based on http://r-addict.com/2016/05/23/Informative-Survival-Plots.html
Chapter 73: Fault-tolerant/resilient code
	Parameter	Details
In case the "try part" was completed successfully tryCatch will return the last evaluated expression. Hence, the actual value being returned in case everything went well and there is no
expr	condition (i.e. a warning or an error) is the return value of readLines. Note that you don't need to
explicilty state the return value via return as code in the "try part" is not wrapped insided a function environment (unlike that for the condition handlers for warnings and error below)
Provide/define a handler function for all the conditions that you want to handle explicitly. AFAIU, you can provide handlers for any type of conditions (not just warnings and errors, but also
warning/error/etc custom conditions; see simpleCondition and friends for that) as long as the name of the respective handler function matches the class of the respective condition (see the Details part of the doc for tryCatch).
                 Here goes everything that should be executed at the very end, regardless if the expression in the "try part" succeeded or if there was any condition. If you want more than one expression to finally
expressionfinally be executed, then you need to wrap them in curly brackets, otherwise you could just have written = <> (i.e. the same logic as for "try part".
Section 73.1: Using tryCatch()
We're defining a robust version of a function that reads the HTML code from a given URL. Robust in the sense that we want it to handle situations where something either goes wrong (error) or not quite the way we planned it to
(warning). The umbrella term for errors and warnings is condition


Testing things out
Let's define a vector of URLs where one element isn't a valid URL

And pass this as input to the function we defined above

Investigating the output
length(y) # [1] 3
head(y[[1]])
# [1] "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\">"      # [2] "<html><head><title>R: Functions to Manipulate Connections</title>"      # [3] "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">" # [4] "<link rel=\"stylesheet\" type=\"text/css\" href=\"R.css\">"            
# [5] "</head><body>"                                                          


Chapter 74: Reproducible R
With 'Reproducibility' we mean that someone else (perhaps you in the future) can repeat the steps you performed and get the same result. See the Reproducible Research Task View.
Section 74.1: Data reproducibility
dput() and dget()
dputThe easiest way to share a (preferable small) data frame is to use a basic function (). It will export an R object in a plain text form.
getwdsetwdNote: Before making the example data below, make sure you're in an empty folder you can write to. Run () and read ? if you need to change folders.

dgetThen, anyone can load the precise R object to their GlobalEnvironment using the () function.

For larger R objects, there are a number of ways of saving them reproducibly. See Input and output .
Section 74.2: Package reproducibility
Package reproducibility is a very common issue in reproducing some R code. When various packages get updated, some interconnections between them may break. The ideal solution for the problem is to reproduce the image of the R code writer's machine on your computer at the date when the code was written. And here comes checkpoint package.
Starting from 2014-09-17, the authors of the package make daily copies of the whole CRAN package repository to their own mirror repository -- Microsoft R Archived Network. So, to avoid package reproduciblity issues when creating a reproducible R project, all you need is to:
1. Make sure that all your packages (and R version) are up-to-date.
checkpoint::checkpoint('YYYY-MM-DD'2. Include ) line in your code.
checkpointlibrary() or requirecheckpoint will create a directory . in your R_home directory ("~/"). To this technical directory it will install all the packages, that are used in your project. That means, checkpoint looks through all the .R files in your project directory to pick up all the () calls and install all the required packages in the form they existed at CRAN on the specified date.
PRO You are freed from the package reproducibility issue.
CONTRA For each specified date you have to download and install all the packages that are used in a certain project that you aim to reproduce. That may take quite a while.
Chapter 75: Fourier Series and Transformations
The Fourier transform decomposes a function of time (a signal) into the frequencies that make it up, similarly to how a musical chord can be expressed as the amplitude (or loudness) of its constituent notes. The Fourier transform of a function of time itself is a complex-valued function of frequency, whose absolute value represents the amount of that frequency present in the original function, and whose complex argument is the phase offset of the basic sinusoid in that frequency.
The Fourier transform is called the frequency domain representation of the original signal. The term Fourier transform refers to both the frequency domain representation and the mathematical operation that associates the frequency domain representation to a function of time. The Fourier transform is not limited to functions of time, but in order to have a unified language, the domain of the original function is commonly referred to as the time domain. For many functions of practical interest one can define an operation that reverses this: the inverse Fourier transformation, also called Fourier synthesis, of a frequency domain representation combines the contributions of all the different frequencies to recover the original function of time.
Linear operations performed in one domain (time or frequency) have corresponding operations in the other domain, which are sometimes easier to perform. The operation of differentiation in the time domain corresponds to multiplication by the frequency, so some differential equations are easier to analyze in the frequency domain. Also, convolution in the time domain corresponds to ordinary multiplication in the frequency domain. Concretely, this means that any linear time-invariant system, such as an electronic filter applied to a signal, can be expressed relatively simply as an operation on frequencies. So significant simplification is often achieved by transforming time functions to the frequency domain, performing the desired operations, and transforming the result back to time.
Harmonic analysis is the systematic study of the relationship between the frequency and time domains, including the kinds of functions or operations that are "simpler" in one or the other, and has deep connections to almost all areas of modern mathematics.
Functions that are localized in the time domain have Fourier transforms that are spread out across the frequency domain and vice versa. The critical case is the Gaussian function, of substantial importance in probability theory and statistics as well as in the study of physical phenomena exhibiting normal distribution (e.g., diffusion), which with appropriate normalizations goes to itself under the Fourier transform. Joseph Fourier introduced the transform in his study of heat transfer, where Gaussian functions appear as solutions of the heat equation.
The Fourier transform can be formally defined as an improper Riemann integral, making it an integral transform, although this definition is not suitable for many applications requiring a more sophisticated integration theory.
For example, many relatively simple applications use the Dirac delta function, which can be treated formally as if it were a function, but the justification requires a mathematically more sophisticated viewpoint. The Fourier transform can also be generalized to functions of several variables on Euclidean space, sending a function of 3dimensional space to a function of 3-dimensional momentum (or a function of space and time to a function of 4momentum).
This idea makes the spatial Fourier transform very natural in the study of waves, as well as in quantum mechanics, where it is important to be able to represent wave solutions either as functions either of space or momentum and sometimes both. In general, functions to which Fourier methods are applicable are complex-valued, and possibly vector-valued. Still further generalization is possible to functions on groups, which, besides the original Fourier transform on ℝ or ℝn (viewed as groups under addition), notably includes the discrete-time Fourier transform (DTFT, group = ℤ), the discrete Fourier transform (DFT, group = ℤ mod N) and the Fourier series or circular Fourier transform (group = S1, the unit circle ≈ closed finite interval with endpoints identified). The latter is routinely employed to handle periodic functions. The Fast Fourier transform (FFT) is an algorithm for computing the DFT.
Section 75.1: Fourier Series
Joseph Fourier showed that any periodic wave can be represented by a sum of simple sine waves. This sum is called the Fourier Series. The Fourier Series only holds while the system is linear. If there is, eg, some overflow effect (a threshold where the output remains the same no matter how much input is given), a non-linear effect enters the picture, breaking the sinusoidal wave and the superposition principle.





Also, the Fourier Series only holds if the waves are periodic, ie, they have a repeating pattern (non periodic waves are dealt by the Fourier Transform, see below). A periodic wave has a frequency f and a wavelength λ (a wavelength is the distance in the medium between the beginning and end of a cycle, λ=v/f0, where v is the wave velocity) that are defined by the repeating pattern. A non-periodic wave does not have a frequency or wavelength.
Some concepts:
The fundamental period, T, is the period of all the samples taken, the time between the first sample and the last
The sampling rate, sr, is the number of samples taken over a time period (aka acquisition frequency). For simplicity we will make the time interval between samples equal. This time interval is called the sample interval, si, which is the fundamental period time divided by the number of samples N. So, si=TN
 The fundamental frequency, f0, which is 1T. The fundamental frequency is the frequency of the repeating pattern or how long the wavelength is. In the previous waves, the fundamental frequency was 12π. The frequencies of the wave components must be integer multiples of the fundamental frequency. f0 is called the first harmonic, the second harmonic is 2∗f0, the third is 3∗f0, etc.


Here's a R function for plotting trajectories given a fourier series:

                        w <- 2*pi*f.0 trajectory <- sapply(ts, function(t) fourier.series(t,w))                         plot(ts, trajectory, type="l", xlab="time", ylab="f(t)");                         abline(h=0,lty=3)}

Chapter 76: .Rprofile
Section 76.1: .Rprofile - the first chunk of code executed
Rprofile. is a file containing R code that is executed when you launch R from the directory containing the
Rprofile file. The similarly named Rprofile.siteRprofile.site and to a greater extend .Rprofile., located in R's home directory, is executed by default every time you load R from any directory.  can be used to initialize an R session with personal preferences and various utility functions that you have defined.
Rprofile		Important note: if you use RStudio, you can have a separate .	 in every RStudio project directory.
Here are some examples of code that you might include in an .Rprofile file.
Setting your R home directory

Setting page size options
options(papersize="a4") options(editor="notepad") options(pager="internal")
set the default help type options(help_type="html") set a site library
.Library.site <- file.path(chartr("\\", "/", R.home()), "site-library")

This will allow you to not have to install all the packages again with each R version update.

Custom shortcuts or functions
Last.valueSometimes it is useful to have a shortcut for a long R expression. A common example of this setting an active binding to access the last top-level expression result without having to type out .:

Because .Rprofile is just an R file, it can contain any arbitrary R code.
Pre-loading the most useful packages
This is bad practice and should generally be avoided because it separates package loading code from the scripts where those packages are actually used.
See Also
help(StartupProfile.siteRHOME}/etcRenviron.siteRenvironSee ) for all the different startup scripts, and further aspects. In particular, two system-wide Profile files can be loaded as well. The first, Rprofile, may contain global settings, the other file  may contain local choices the system administrator can make for all users. Both files are found in the ${ directory of the R installation. This directory also contains global files Renviron and  which both can be completemented with a local file ~/. in the user's home directory.
Section 76.2: .Rprofile example
Startup

Options

Custom Functions

Chapter 77: dplyr
Section 77.1: dplyr's single table verbs
dplyr introduces a grammar of data manipulation in R. It provides a consistent interface to work with data no matter where it is stored: data.frame, data.table, or a database. The key pieces of dplyr are written using Rcpp, which makes it very fast for working with in-memory data.
dplyr's philosophy is to have small functions that do one thing well. The five simple functions (filter, arrange, SELECT, mutate, and summarise) can be used to reveal new ways to describe data. When combined with group_by, these functions can be used to calculate group wise summary statistics.
Syntax commonalities
All these functions have a similar syntax:
The first argument to all these functions is always a data frame
Columns can be referred directly using bare variable names (i.e., without using $)
These functions do not modify the original data itself, i.e., they don't have side effects. Hence, the results should always be saved to an object.
We will use the built-in mtcars dataset to explore dplyr's single table verbs. Before converting the type of mtcars to tbl_df (since it makes printing cleaner), we add the rownames of the dataset as a column using rownames_to_column function from the tibble package.
library(dplyr) # This documentation was written using version 0.5.0 mtcars_tbl <- as_data_frame(tibble::rownames_to_column(mtcars, "cars"))
# examine the structure of data head(mtcars_tbl)
# A tibble: 6 x 12
#               cars   mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
#              <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
#1         Mazda RX4  21.0     6   160   110  3.90 2.620 16.46     0     1     4     4
#2     Mazda RX4 Wag  21.0     6   160   110  3.90 2.875 17.02     0     1     4     4
#3        Datsun 710  22.8     4   108    93  3.85 2.320 18.61     1     1     4     1
#4    Hornet 4 Drive  21.4     6   258   110  3.08 3.215 19.44     1     0     3     1
#5 Hornet Sportabout  18.7     8   360   175  3.15 3.440 17.02     0     0     3     2
 #6           Valiant  18.1     6   225   105  2.76 3.460 20.22     1     0     3     1 filter
data.framefilter helps subset rows that match certain criteria. The first argument is the name of the  and the second (and subsequent) arguments are the criteria that filter the data (these criteria should evaluate to either TRUE or FALSE)
Subset all cars that have 4 cylinders - cyl:
filter(mtcars_tbl, cyl == 4)
# A tibble: 11 x 12
#             cars   mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
#            <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
#1      Datsun 710  22.8     4 108.0    93  3.85 2.320 18.61     1     1     4     1
#2       Merc 240D  24.4     4 146.7    62  3.69 3.190 20.00     1     0     4     2
#3        Merc 230  22.8     4 140.8    95  3.92 3.150 22.90     1     0     4     2
#4        Fiat 128  32.4     4  78.7    66  4.08 2.200 19.47     1     1     4     1 #5     Honda Civic  30.4     4  75.7    52  4.93 1.615 18.52     1     1     4     2
# ... with 6 more rows
We can pass multiple criteria separated by a comma. To subset the cars which have either 4 or 6 cylinders - cyl and have 5 gears - gear:
filter(mtcars_tbl, cyl == 4 | cyl == 6, gear == 5)
# A tibble: 3 x 12
#           cars   mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
#          <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #1 Porsche 914-2  26.0     4 120.3    91  4.43 2.140  16.7     0     1     5     2 #2  Lotus Europa  30.4     4  95.1   113  3.77 1.513  16.9     1     1     5     2
#3  Ferrari Dino  19.7     6 145.0   175  3.62 2.770  15.5     0     1     5     6
data.framefilter selects rows based on criteria, to select rows by position, use slice. slice takes only 2 arguments: the first one is a  and the second is integer row values.
To select rows 6 through 9:
slice(mtcars_tbl, 6:9)
# A tibble: 4 x 12
#        cars   mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
#       <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
#1    Valiant  18.1     6 225.0   105  2.76  3.46 20.22     1     0     3     1
#2 Duster 360  14.3     8 360.0   245  3.21  3.57 15.84     0     0     3     4
#3  Merc 240D  24.4     4 146.7    62  3.69  3.19 20.00     1     0     4     2
 #4   Merc 230  22.8     4 140.8    95  3.92  3.15 22.90     1     0     4     2 Or:

slice(mtcars_tbl, This results in the same output as 6:9)
n() represents the number of observations in the current group arrange
data.framearrange is used to sort the data by a specified variable(s). Just like the previous verb (and all other functions in dplyr), the first argument is a , and consequent arguments are used to sort the data. If more than one variable is passed, the data is first sorted by the first variable, and then by the second variable, and so on..
To order the data by horsepower - hp
arrange(mtcars_tbl, hp)
# A tibble: 32 x 12
#             cars   mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
#            <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
#1     Honda Civic  30.4     4  75.7    52  4.93 1.615 18.52     1     1     4     2
#2       Merc 240D  24.4     4 146.7    62  3.69 3.190 20.00     1     0     4     2
#3  Toyota Corolla  33.9     4  71.1    65  4.22 1.835 19.90     1     1     4     1
#4        Fiat 128  32.4     4  78.7    66  4.08 2.200 19.47     1     1     4     1 #5       Fiat X1-9  27.3     4  79.0    66  4.08 1.935 18.90     1     1     4     1 #6   Porsche 914-2  26.0     4 120.3    91  4.43 2.140 16.70     0     1     5     2 # ... with 26 more rows
To arrange the data by miles per gallon - mpg in descending order, followed by number of cylinders - cyl:
arrange(mtcars_tbl, desc(mpg), cyl)
# A tibble: 32 x 12
#             cars   mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
#            <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
#1  Toyota Corolla  33.9     4  71.1    65  4.22 1.835 19.90     1     1     4     1
#2        Fiat 128  32.4     4  78.7    66  4.08 2.200 19.47     1     1     4     1
#3     Honda Civic  30.4     4  75.7    52  4.93 1.615 18.52     1     1     4     2
#4    Lotus Europa  30.4     4  95.1   113  3.77 1.513 16.90     1     1     5     2 #5       Fiat X1-9  27.3     4  79.0    66  4.08 1.935 18.90     1     1     4     1 #6   Porsche 914-2  26.0     4 120.3    91  4.43 2.140 16.70     0     1     5     2
 # ... with 26 more rows select
SELECT is used to select only a subset of variables. To select only mpg, disp, wt, qsec, and vs from mtcars_tbl:

: notation can be used to select consecutive columns. To select columns from cars through disp and vs through carb:
SELECT(mtcars_tbl, cars:disp, vs:carb)
# A tibble: 32 x 8
#                cars   mpg   cyl  disp    vs    am  gear  carb
#               <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
#1          Mazda RX4  21.0     6 160.0     0     1     4     4
#2      Mazda RX4 Wag  21.0     6 160.0     0     1     4     4
#3         Datsun 710  22.8     4 108.0     1     1     4     1
#4     Hornet 4 Drive  21.4     6 258.0     1     0     3     1
#5  Hornet Sportabout  18.7     8 360.0     0     0     3     2
#6            Valiant  18.1     6 225.0     1     0     3     1 # ... WITH 26 more ROWS
hp:qsecSELECT(mtcars_tblor , -())
For datasets that contain several columns, it can be tedious to select several columns by name. To make life easier,
starts_with(), ends_with(), contains(), matchesthere are a number of helper functions (such as (),
num_range(), one_of(), and everything()) that can be used in SELECT. To learn more about how to use them, see
select_helpers and ?select?.
SELECTNote: While referring to columns directly in (), we use bare column names, but quotes should be used while referring to columns in helper functions.
To rename columns while selecting:

As expected, this drops all other variables.
To rename columns without dropping other variables, use rename: rename(mtcars_tbl, cylinders = cyl, displacement = disp)
# A tibble: 32 x 12
#                cars   mpg cylinders displacement    hp  drat    wt  qsec    vs
#               <chr> <dbl>     <dbl>        <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
#1          Mazda RX4  21.0         6        160.0   110  3.90 2.620 16.46     0
#2      Mazda RX4 Wag  21.0         6        160.0   110  3.90 2.875 17.02     0
#3         Datsun 710  22.8         4        108.0    93  3.85 2.320 18.61     1
#4     Hornet 4 Drive  21.4         6        258.0   110  3.08 3.215 19.44     1
#5  Hornet Sportabout  18.7         8        360.0   175  3.15 3.440 17.02     0
#6            Valiant  18.1         6        225.0   105  2.76 3.460 20.22     1
 # ... with 26 more rows, and 3 more variables: am <dbl>, gear <dbl>, carb <dbl> mutate
data.framemutate can be used to add new columns to the data. Like all other functions in dplyr, mutate doesn't add the newly created columns to the original data. Columns are added at the end of the .
mutate(mtcars_tbl, weight_ton = wt/2, weight_pounds = weight_ton * 2000)
# A tibble: 32 x 14 #                cars   mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb weight_ton weight_pounds #               <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>      <dbl>
        <dbl>
#1          Mazda RX4  21.0     6 160.0   110  3.90 2.620 16.46     0     1     4     4     1.3100
         2620
#2      Mazda RX4 Wag  21.0     6 160.0   110  3.90 2.875 17.02     0     1     4     4     1.4375
         2875
#3         Datsun 710  22.8     4 108.0    93  3.85 2.320 18.61     1     1     4     1     1.1600
         2320
#4     Hornet 4 Drive  21.4     6 258.0   110  3.08 3.215 19.44     1     0     3     1     1.6075
         3215
#5  Hornet Sportabout  18.7     8 360.0   175  3.15 3.440 17.02     0     0     3     2     1.7200
         3440
#6            Valiant  18.1     6 225.0   105  2.76 3.460 20.22     1     0     3     1     1.7300
         3460
# ... with 26 more rows
Note the use of weight_ton while creating weight_pounds. Unlike base R, mutate allows us to refer to columns that we just created to be used for a subsequent operation.
To retain only the newly created columns, use transmute instead of mutate:

summarise calculates summary statistics of variables by collapsing multiple values to a single value. It can calculate multiple statistics and we can name these summary columns in the same statement. To calculate the mean and standard deviation of mpg and disp of all cars in the dataset:

group_by can be used to perform group wise operations on data. When the verbs defined above are applied on this grouped data, they are automatically applied to each group separately.
To find mean and sd of mpg by cyl:

We select columns from cars through hp and gear, order the rows by cyl and from highest to lowest mpg, group the data by gear, and finally subset only those cars have mpg > 20 and hp > 75


Maybe we are not interested the intermediate results, we can achieve the same result as above by wrapping the function calls:

This can be a little difficult to read. So, dplyr operations can be chained using the pipe %>% operator. The above code transalates to:

summarise_alldplyr provides () to apply functions to all (non-grouping) columns.
To find the number of distinct values for each column:

To find the number of distinct values for each column by cyl:

#3     8    14    12    11     9    11    13    14     1     2     2     4
Note that we just had to add the group_by statement and the rest of the code is the same. The output now consists of three rows - one for each unique value of cyl.
To summarise specific multiple columns, use summarise_at

select_helpershelper functions (?) can be used in place of column names to select specific columns To apply multiple functions, either pass the function names as a character vector:

or wrap them inside funs:

Column names are now be appended with function names to keep them distinct. In order to change this, pass the name to be appended with the function:

#2     6 19.74286  183.3143 122.28571 1.453567 41.56246 24.26049
#3     8 15.10000  353.1000 209.21429 2.560048 67.77132 50.97689
To select columns conditionally, use summarise_if:
Take the mean of all columns that are numeric grouped by cyl:

However, some variables are discrete, and mean of these variables doesn't make sense.
To take the mean of only continuous variables by cyl:
mtcars_tbl %>%     group_by(cyl) %>%     summarise_if(function(x) is.numeric(x) & n_distinct(x) > 6, mean)
# A tibble: 3 x 7
#    cyl      mpg     disp        hp     drat       wt     qsec
#  <dbl>    <dbl>    <dbl>     <dbl>    <dbl>    <dbl>    <dbl>
#1     4 26.66364 105.1364  82.63636 4.070909 2.285727 19.13727
#2     6 19.74286 183.3143 122.28571 3.585714 3.117143 17.97714
#3     8 15.10000 353.1000 209.21429 3.229286 3.999214 16.77214
Section 77.2: Aggregating with %>% (pipe) operator
help("mtcars"The pipe (%>%) operator could be used in combination with dplyr functions. In this example we use the mtcars dataset (see ) for more information) to show how to sumarize a data frame, and to add variables to the data with the result of the application of a function.

1. Sumarize the data
To compute statistics we use summarize and the appropriate functions. In this case n() is used for counting the number of cases.


2. Compute statistics by group
It is possible to compute the statistics by groups of the data. In this case by Number of cylinders and Number of forward gears

Section 77.3: Subset Observation (Rows)
dplyr::filter() - Select a subset of rows in a data frame that meet a logical criteria:
dplyr::filter(iris,Sepal.Length>7) #       Sepal.Length Sepal.Width Petal.Length Petal.Width   Species
#    1           7.1         3.0          5.9         2.1 virginica
#    2           7.6         3.0          6.6         2.1 virginica
#    3           7.3         2.9          6.3         1.8 virginica
#    4           7.2         3.6          6.1         2.5 virginica
#    5           7.7         3.8          6.7         2.2 virginica
#    6           7.7         2.6          6.9         2.3 virginica
#    7           7.7         2.8          6.7         2.0 virginica
#    8           7.2         3.2          6.0         1.8 virginica
#    9           7.2         3.0          5.8         1.6 virginica
#    10          7.4         2.8          6.1         1.9 virginica
#    11          7.9         3.8          6.4         2.0 virginica #    12          7.7         3.0          6.1         2.3 virginica
dplyr::distinct() - Remove duplicate rows:
distinct(iris, Sepal.Length, .keep_all = TRUE) #       Sepal.Length Sepal.Width Petal.Length Petal.Width    Species
#    1           5.1         3.5          1.4         0.2     setosa
#    2           4.9         3.0          1.4         0.2     setosa
#    3           4.7         3.2          1.3         0.2     setosa
#    4           4.6         3.1          1.5         0.2     setosa
#    5           5.0         3.6          1.4         0.2     setosa
#    6           5.4         3.9          1.7         0.4     setosa
#    7           4.4         2.9          1.4         0.2     setosa
#    8           4.8         3.4          1.6         0.2     setosa
#    9           4.3         3.0          1.1         0.1     setosa
#   10          5.8         4.0          1.2         0.2     setosa
#   11          5.7         4.4          1.5         0.4     setosa
#   12          5.2         3.5          1.5         0.2     setosa
#   13          5.5         4.2          1.4         0.2     setosa
#   14          4.5         2.3          1.3         0.3     setosa
#   15          5.3         3.7          1.5         0.2     setosa
#   16          7.0         3.2          4.7         1.4 versicolor
#   17          6.4         3.2          4.5         1.5 versicolor
#   18          6.9         3.1          4.9         1.5 versicolor
#   19          6.5         2.8          4.6         1.5 versicolor
#   20          6.3         3.3          4.7         1.6 versicolor
#   21          6.6         2.9          4.6         1.3 versicolor
#   22          5.9         3.0          4.2         1.5 versicolor
#   23          6.0         2.2          4.0         1.0 versicolor
#   24          6.1         2.9          4.7         1.4 versicolor
#   25          5.6         2.9          3.6         1.3 versicolor
#   26          6.7         3.1          4.4         1.4 versicolor
#   27          6.2         2.2          4.5         1.5 versicolor
#   28          6.8         2.8          4.8         1.4 versicolor
#   29          7.1         3.0          5.9         2.1  virginica
#   30          7.6         3.0          6.6         2.1  virginica
#   31          7.3         2.9          6.3         1.8  virginica
#   32          7.2         3.6          6.1         2.5  virginica
#   33          7.7         3.8          6.7         2.2  virginica
#   34          7.4         2.8          6.1         1.9  virginica
#   35          7.9         3.8          6.4         2.0  virginica
Section 77.4: Examples of NSE and string variables in dpylr
dplyr uses Non-Standard Evaluation(NSE), which is why we normally can use the variable names without quotes. However, sometimes during the data pipeline, we need to get our variable names from other sources such as a
Shiny selection box. In case of functions like SELECT, we can just use select_ to use a string variable to select

But if we want to use other features such as summarize or filter we need to use interp function from lazyeval package

Chapter 78: caret
caret is an R package that aids in data processing needed for machine learning problems. It stands for classification and regression training. When building models for a real dataset, there are some tasks other than the actual learning algorithm that need to be performed, such as cleaning the data, dealing with incomplete observations, validating our model on a test set, and compare different models.
caret helps in these scenarios, independent of the actual learning algorithms used.
Section 78.1: Preprocessing
preProcesspreProcessPre-processing in caret is done through the () function. Given a matrix or data frame type object x, () applies transformations on the training data which can then be applied to testing data.
preProcessThe heart of the () function is the method argument. Method operations are applied in this order:
1. Zero-variance filter
2. Near-zero variance filter
3. Box-Cox/Yeo-Johnson/exponential transformation
4. Centering
5. Scaling
6. Range
7. Imputation
8. PCA
9. ICA
10. Spatial Sign
Below, we take the mtcars data set and perform centering, scaling, and a spatial sign transform.

Chapter 79: Extracting and Listing Files in Compressed Archives
Section 79.1: Extracting files from a .zip archive
Unzipping a zip archive is done with unzip function from the utils package (which is included in base R).

This will extract all files in "bar.zip" to the "foo" directory, which will be created if necessary. Tilde expansion is done automatically from your working directory. Alternatively, you can pass the whole path name to the zipfile.

Chapter 80: Probability Distributions with R
Section 80.1: PDF and PMF for dierent distributions in R
PMF FOR THE BINOMIAL DISTRIBUTION
Suppose that a fair die is rolled 10 times. What is the probability of throwing exactly two sixes?
You can answer the question using the dbinom function:

PMF FOR THE POISSON DISTRIBUTION
The number of sandwhich ordered in a restaurant on a given day is known to follow a Poisson distribution with a mean of 20. What is the probability that exactly eighteen sandwhich will be ordered tomorrow?
You can answer the question with the dpois function:

PDF FOR THE NORMAL DISTRIBUTION
To find the value of the pdf at x=2.5 for a normal distribution with a mean of 5 and a standard deviation of 2, use the command:

Chapter 81: R in LaTeX with knitr
Option	Details
echo	(TRUE/FALSE) - whether to include R source code in the output file message (TRUE/FALSE) - whether to include messages from the R source execution in the output file warning	(TRUE/FALSE) - whether to include warnings from the R source execution in the output file error	(TRUE/FALSE) - whether to include errors from the R source execution in the output file cache	(TRUE/FALSE) - whether to cache the results of the R source execution fig.width (numeric) - width of the plot generated by the R source execution fig.height (numeric) - height of the plot generated by the R source execution
Section 81.1: R in LaTeX with Knitr and Code Externalization
Knitr is an R package that allows us to intermingle R code with LaTeX code. One way to achieve this is external code chunks. External code chunks allow us to develop/test R Scripts in an R development environment and then include the results in a report. It is a powerful organizational technique. This approach is demonstrated below.

When using this approach we keep our code in a separate R file as shown below.

Section 81.2: R in LaTeX with Knitr and Inline Code Chunks
Knitr is an R package that allows us to intermingle R code with LaTeX code. One way to achieve this is inline code chunks. This apporach is demonstrated below.


Section 81.3: R in LaTex with Knitr and Internal Code Chunks
Knitr is an R package that allows us to intermingle R code with LaTeX code. One way to achieve this is internal code chunks. This apporach is demonstrated below.

Chapter 82: Web Crawling in R
Section 82.1: Standard scraping approach using the RCurl package
We try to extract imdb top chart movies and ratings


Chapter 83: Creating reports with RMarkdown
Section 83.1: Including bibliographies
bibliography:biblio-style:A bibtex catalogue cna easily be included with the YAML option . A certain style for the bibliography can be added with . The references are added at the end of the document.

Section 83.2: Including LaTeX Preample Commands
There are two possible ways of including LaTeX preamble commands (e.g. \usepackage) in a RMarkdown document.
header-includes1. Using the YAML option :


2. Including External Commands with includes, in_header

header-includesHere, the content of includes.tex are the same three commands we included with .
Writing a whole new template
A possible third option is to write your own LaTex template and include it with template. But this covers a lot more of the structure than only the preamble.

Section 83.3: Printing tables
There are several packages that allow the output of data structures in form of HTML or LaTeX tables. They mostly differ in flexibility.
Here I use the packages:
knitr xtable pander
For HTML documents

For PDF documents


How can I stop xtable printing the comment ahead of each table?
options(xtable.comment = FALSE)
Section 83.4: Basic R-markdown document structure
R-markdown code chunks
R-markdown is a markdown file with embedded blocks of R code called chunks. There are two types of R code chunks: inline and block.
Inline chunks are added using the following syntax:

They are evaluated and inserted their output answer in place.
Block chunks have a different syntax:

And they come with several possible options. Here are the main ones (but there are many others):
echo (boolean) controls wether the code inside chunk will be included in the document include (boolean) controls wether the output should be included in the document fig.width (numeric) sets the width of the output figures fig.height (numeric) sets the height of the output figures fig.cap (character) sets the figure captions
tag=valueThey are written in a simple  format like in the example above.
R-markdown document example
Below is a basic example of R-markdown file illustrating the way R code chunks are embedded inside r-markdown.

Converting R-markdown to other formats
The R knitr package can be used to evaluate R chunks inside R-markdown file and turn it into a regular markdown file.
The following steps are needed in order to turn R-markdown file into pdf/html:
1. Convert R-markdown file to markdown file using knitr.
2. Convert the obtained markdown file to pdf/html using specialized tools like pandoc.
knit2html() and knit2pdfIn addition to the above knitr package has wrapper functions () that can be used to produce the final document without the intermediate step of manually converting it to the markdown format:
income.RmdIf the above example file was saved as  it can be converted to a pdf file using the following R commands:

The final document will be similar to the one below.

Chapter 84: GPU-accelerated computing
Section 84.1: gpuR gpuMatrix objects
library(gpuR) # gpuMatrix objects X <- gpuMatrix(rnorm(100), 10, 10) Y <- gpuMatrix(rnorm(100), 10, 10) # transfer data to GPU when operation called # automatically copied back to CPU Z <- X %*% Y
Section 84.2: gpuR vclMatrix objects
library(gpuR) # vclMatrix objects X <- vclMatrix(rnorm(100), 10, 10) Y <- vclMatrix(rnorm(100), 10, 10) # data always on GPU # no data transfer Z <- X %*% Y

Chapter 85: heatmap and heatmap.2
Section 85.1: Examples from the ocial documentation
stats::heatmap Example 1 (Basic usage)

Example 2 (no column dendrogram (nor reordering) at all)
heatmap(x, Colv = NA, col = cm.colors(256), scale = "column",         RowSideColors = rc, margins = c(5,10),         xlab = "specification variables", ylab =  "Car Models",         main = "heatmap(<Mtcars data>, ..., scale = \"column\")")

Example 3 ("no nothing")


Example 4 (with reorder())
round(Ca <- cor(attitude), 2)
#            rating complaints privileges learning raises critical advance
# rating       1.00       0.83       0.43     0.62   0.59     0.16    0.16
# complaints   0.83       1.00       0.56     0.60   0.67     0.19    0.22
# privileges   0.43       0.56       1.00     0.49   0.45     0.15    0.34
# learning     0.62       0.60       0.49     1.00   0.64     0.12    0.53
# raises       0.59       0.67       0.45     0.64   1.00     0.38    0.57
# critical     0.16       0.19       0.15     0.12   0.38     1.00    0.28 # advance      0.16       0.22       0.34     0.53   0.57     0.28    1.00 symnum(Ca) # simple graphic #            rt cm p l rs cr a
# rating     1                
# complaints +  1            
# privileges .  .  1          
# learning   ,  .  . 1        
# raises     .  ,  . , 1      
# critical             .  1  
# advance          . . .     1
# attr(,"legend")
# [1] 0 ' ' 0.3 '.' 0.6 ',' 0.8 '+' 0.9 '*' 0.95 'B' 1 heatmap(Ca,               symm = TRUE, margins = c(6,6))

Example 5 (NO reorder())


Example 6 (slightly artificial with color bar, without ordering)




Example 8 (For variable clustering, rather use distance based on cor())


#            `--[dendrogram w/ 2 branches and 5 members at h = 0.0584]
#               |--leaf "RTEN"
#               `--[dendrogram w/ 2 branches and 4 members at h = 0.0187] #                  |--[dendrogram w/ 2 branches and 2 members at h = 0.00657]
#                  |  |--leaf "ORAL"
#                  |  `--leaf "WRIT"
#                  `--[dendrogram w/ 2 branches and 2 members at h = 0.0101]
#                     |--leaf "PREP"
#                     `--leaf "FAMI"
Section 85.2: Tuning parameters in heatmap.2
Given:

One can use heatmap.2 - a more recent optimized version of heatmap, by loading the following library:

To add a title, x- or y-label to your heatmap, you need to set the main, xlab and ylab:

If you wish to define your own color palette for your heatmap, you can set the col parameter by using the colorRampPalette function:

As you can notice, the labels on the y axis (the car names) don't fit in the figure. In order to fix this, the user can tune the margins parameter:
heatmap.2(x, trace="none", key=TRUE,col = colorRampPalette(c("darkblue","white","darkred"))(100), margins=c(5,8))

Further, we can change the dimensions of each section of our heatmap (the key histogram, the dendograms and the heatmap itself), by tuning lhei and lwid :

Colv=FALSE (or Rowv=FALSEIf we only want to show a row(or column) dendogram, we need to set ) and adjust the dendogram parameter:
heatmap.2(x, trace="none", key=TRUE, Colv=FALSE, dendrogram = "row", col = colorRampPalette(c("darkblue","white","darkred"))(100), margins=c(5,8), lwid = c(5,15), lhei = c(3,15))

cex.main, cex.lab, cex.axisFor changing the font size of the legend title,labels and axis, the user needs to set  in the par list:
par(cex.main=1, cex.lab=0.7, cex.axis=0.7) heatmap.2(x, trace="none", key=TRUE, Colv=FALSE, dendrogram = "row", col = colorRampPalette(c("darkblue","white","darkred"))(100), margins=c(5,8), lwid = c(5,15), lhei = c(5,15))


Chapter 86: Network analysis with the igraph package
Section 86.1: Simple Directed and Non-directed Network Graphing
The igraph package for R is a wonderful tool that can be used to model networks, both real and virtual, with simplicity. This example is meant to demonstrate how to create two simple network graphs using the igraph package within R v.3.2.3.
Non-Directed Network
The network is created with this piece of code:

Directed Network

This code will then generate a network with arrows:

Code example of how to make a double sided arrow:

Chapter 87: Functional programming
Section 87.1: Built-in Higher Order Functions
R has a set of built in higher order functions: Map, Reduce, Filter, Find, Position, Negate.
Map applies a given function to a list of values:

Reduce successively applies a binary function to a list of values in a recursive fashion.

Filter given a predicate function and a list of values returns a filtered list containing only values for whom predicate function is TRUE.

Find given a predicate function and a list of values returns the first value for which the predicate function is TRUE.

Position given a predicate function and a list of values returns the position of the first value in the list for which the predicate function is TRUE.

Negate inverts a predicate function making it return FALSE for values where it returned TRUE and vice versa.

Chapter 88: Get user input
Section 88.1: User input in R
Sometimes it can be interesting to have a cross-talk between the user and the program, one example being the swirl package that had been designed to teach R in R.
One can ask for user input using the readline command:

The user can then give any answer, such as a number, a character, vectors, and scanning the result is here to make sure that the user has given a proper answer. For example:

However, it is to be noted that this code be stuck in a never-ending loop, as user input is saved as a character.
as.numericWe have to coerce it to a number, using :


Chapter 89: Spark API (SparkR)
Section 89.1: Setup Spark context
Setup Spark context in R
To start working with Sparks distributed dataframes, you must connect your R program with an existing Spark Cluster.

Here are infos how to connect your IDE to a Spark cluster.
Get Spark Cluster
There is an Apache Spark introduction topic with install instructions. Basically, you can employ a Spark Cluster locally via java (see instructions) or use (non-free) cloud applications (e.g. Microsoft Azure [topic site], IBM).
Section 89.2: Cache data
What:
Caching can optimize computation in Spark. Caching stores data in memory and is a special case of persistence. Here is explained what happens when you cache an RDD in Spark.
Why:
Basically, caching saves an interim partial result - usually after transformations - of your original data. So, when you use the cached RDD, the already transformed data from memory is accessed without recomputing the earlier transformations.
How:
Here is an example how to quickly access large data (here 3 GB big csv) from in-memory storage when accessing it more then once:

Section 89.3: Create RDDs (Resilient Distributed Datasets)
From dataframe:

From csv:
For csv's, you need to add the csv package to the environment before initiating the Spark context:

Then, you can load the csv either by infering the data schema of the data in the columns:
train <- read.df(sqlContext, "/train.csv", header= "true", source = "com.databricks.spark.csv", inferSchema = "true")
Or by specifying the data schema beforehand:


Chapter 90: Meta: Documentation Guidelines
Section 90.1: Style
Prompts
If you want your code to be copy-pastable, remove prompts such as R>, >, or + at the beginning of each new line. Some Docs authors prefer to not make copy-pasting easy, and that is okay.
Console output
Console output should be clearly distinguished from code. Common approaches include:
Include prompts on input (as seen when using the console).
Comment out all output, with # or ## starting each line.
Print as-is, trusting the leading [1] to make the output stand out from the input. Add a blank line between code and console output.
Assignment
<-1 (ambiguous between x <- 1 and x = and <- are fine for assigning R objects. Use white space appropriately to avoid writing code that is difficult to parse, such as x< -1)
Code comments
Be sure to explain the purpose and function of the code itself. There isn't any hard-and-fast rule on whether this explanation should be in prose or in code comments. Prose may be more readable and allows for longer explanations, but code comments make for easier copy-pasting. Keep both options in mind.
Sections
Many examples are short enough to not need sections, but if you use them, start with H1.
Section 90.2: Making good examples
Most of the guidance for creating good examples for Q&A carries over into the documentation.

There are some additional considerations in the context of Docs:
data.frame Refer to built-in docs like ?	 whenever relevant. The SO Docs are not an attempt to replace the built-in docs. It is important to make sure new R users know that the built-in docs exist as well as how to find them.
 Move content that applies to multiple examples to the Remarks section.
Chapter 91: Input and output
Section 91.1: Reading and writing data frames
Data frames are R's tabular data structure. They can be written to or read from in a variety of ways.
This example illustrates a couple common situations. See the links at the end for other resources.
Writing
getwdsetwdBefore making the example data below, make sure you're in a folder you want to write to. Run () to verify the folder you're in and read ? if you need to change folders.

Now, we have three similarly-formatted CSV files on disk.
Reading
We have three similarly-formatted files (from the last section) to read in. Since these files are related, we should store them together after reading in, in a list:

str(file_contentsrbind or iterating over the list with ?lapplyTo work with this list of files, first examine the structure with ), then read about stacking the list with ?.
Further resources
read.table and ?write.tableCheck out ? to extend this example. Also:
R binary formats (for tables and other objects)
Plain-text table formats comma-delimited CSVs tab-delimited TSVs Fixed-width formats
Language-agnostic binary table formats Feather
Foreign table and spreadsheet formats
SAS
SPSS
Stata
Excel
Relational database table formats
MySQL
SQLite
PostgreSQL

Chapter 92: I/O for foreign tables (Excel, SAS, SPSS, Stata)
Section 92.1: Importing data with rio
A very simple way to import data from many common file formats is with rio. This package provides a function
importimport       () that wraps many commonly used data import functions, thereby providing a standard interface. It works simply by passing a file name or URL to ():

import       () can also read from compressed directories, URLs (HTTP or HTTPS), and the clipboard. A comprehensive list of all supported file formats is available on the rio package github repository.
importIt is even possible to specify some further parameters related to the specific file format you are trying to read, passing them directly within the () function:
import("example.csv", format = ",") #for csv file where comma is used as separator import("example.csv", format = ";") #for csv file where semicolon is used as separator
Section 92.2: Read and write Stata, SPSS and SAS files
The packages foreign and haven can be used to import and export files from a variety of other statistical packages like Stata, SPSS and SAS and related software. There is a read function for each of the supported data types to import the files.

Some examples for the most common data types:

The foreign package can read in stata (.dta) files for versions of Stata 7-12. According to the development page, the read.dta is more or less frozen and will not be updated for reading in versions 13+. For more recent versions of Stata, you can use either the readstata13 package or haven. For readstata13, the files are

For reading in SPSS and SAS files

read.fwfThe SAScii package provides functions that will accept SAS SET import code and construct a text file that can be processed with . It has proved very robust for import of large public-released datasets. Support is at https://github.com/ajdamico/SAScii
write.foreignTo export data frames to other statistical packages you can use the write functions (). This will write 2 files, one containing the data and one containing instructions the other package needs to read the data.

read.spssFile stored by the SPSS can also be read with  in this way:
 foreign::read.spss('data.sav', to.data.frame=TRUE, use.value.labels=FALSE,                      use.missings=TRUE, reencode='UTF-8')
# to.data.frame if TRUE: return a data frame
# use.value.labels if TRUE: convert variables with value labels into R factors with those levels
# use.missings if TRUE: information on user-defined missing values will used to set the corresponding values to NA.
# reencode character strings will be re-encoded to the current locale. The default, NA, means to do so in a UTF-8 locale, only.
Section 92.3: Importing Excel files
There are several R packages to read excel files, each of which using different languages or resources, as summarized in the following table:
R package Uses
xlsx	Java
XLconnect Java openxlsx	C++ readxl	C++
RODBC ODBC gdata Perl
For the packages that use Java or ODBC it is important to know details about your system because you may have compatibility issues depending on your R version and OS. For instance, if you are using R 64 bits then you also must have Java 64 bits to use xlsx or XLconnect.
package::functionSome examples of reading excel files with each package are provided below. Note that many of the packages have the same or very similar function names. Therefore, it is useful to state the package explicitly, like . The package openxlsx requires prior installation of RTools.
Reading excel files with the xlsx package

The index or name of the sheet is required to import. xlsx::read.xlsx("Book1.xlsx", sheetName="Sheet1")
Reading Excel files with the XLconnect package

Book1.xlsxBook1.xlsxXLConnect automatically imports the pre-defined Excel cell-styles embedded in . This is useful when you wish to format your workbook object and export a perfectly formatted Excel document. Firstly, you will need to create the desired cell formats in  and save them, for example, as myHeader, myBody and myPcts. Then, after loading the workbook in R (see above):
Headerstyle <- XLConnect::getCellStyle(wb, "myHeader")
Bodystyle <- XLConnect::getCellStyle(wb, "myBody")
Pctsstyle <- XLConnect::getCellStyle(wb, "myPcts")
The cell styles are now saved in your R environment. In order to assign the cell styles to certain ranges of your data, you need to define the range and then assign the style:
Headerrange <- expand.grid(row = 1, col = 1:8)
Bodyrange <- expand.grid(row = 2:6, col = c(1:5, 8))
Pctrange <- expand.grid(row = 2:6, col = c(6, 7))
XLConnect::setCellStyle(wb, sheet = "sheet1", row = Headerrange$row,              col = Headerrange$col, cellstyle = Headerstyle)
XLConnect::setCellStyle(wb, sheet = "sheet1", row = Bodyrange$row,
             col = Bodyrange$col, cellstyle = Bodystyle) XLConnect::setCellStyle(wb, sheet = "sheet1", row = Pctrange$row,              col = Pctrange$col, cellstyle = Pctsstyle)
Note that XLConnect is easy, but can become extremely slow in formatting. A much faster, but more cumbersome formatting option is offered by openxlsx.
Reading excel files with the openxlsx package

The sheet, which should be read into R can be selected either by providing its position in the sheet argument:

or by declaring its name:

Additionally, openxlsx can detect date columns in a read sheet. In order to allow automatic detection of dates, an argument detectDates should be set to TRUE: openxlsx::read.xlsx("spreadsheet1.xlsx", sheet = "Sheet1", detectDates= TRUE) Reading excel files with the readxl package
Excel files can be imported as a data frame into R using the readxl package.

xls and .xlsxIt can read both . files.

The sheet to be imported can be specified by number or name.

col_names = TRUEThe argument  sets the first row as the column names.  readxl::read_excel("spreadsheet.xls", sheet = 1, col_names = TRUE)
The argument col_types can be used to specify the column types in the data as a vector.
readxl::read_excel("spreadsheet.xls", sheet = 1, col_names = TRUE,                    col_types = c("text", "date", "numeric", "numeric"))
Reading excel files with the RODBC package
Excel files can be read using the ODBC Excel Driver that interfaces with Windows' Access Database Engine (ACE), formerly JET. With the RODBC package, R can connect to this driver and directly query workbooks. Worksheets are assumed to maintain column headers in first row with data in organized columns of similar types. NOTE: This approach is limited to only Windows/PC machines as JET/ACE are installed .dll files and not available on other operating systems.

Connecting with an SQL engine in this approach, Excel worksheets can be queried similar to database tables including JOIN and UNION operations. Syntax follows the JET/ACE SQL dialect. NOTE: Only data access DML statements, specifically SELECT can be run on workbooks, considered not updateable queries.
joindf <-  sqlQuery(xlconn, "SELECT t1.*, t2.* FROM [Sheet1$] t1
                             INNER JOIN [Sheet2$] t2                              ON t1.[ID] = t2.[ID]")
uniondf <-  sqlQuery(xlconn, "SELECT * FROM [Sheet1$]
                              UNION  
                              SELECT * FROM [Sheet2$]")
Even other workbooks can be queried from the same ODBC channel pointing to a current workbook:
otherwkbkdf <- sqlQuery(xlconn, "SELECT * FROM
                                 [Excel 12.0 Xml;HDR=Yes;
                                  Database=C:\\Path\\To\\Other\\Workbook.xlsx].[Sheet1$];") Reading excel files with the gdata package example here
Section 92.4: Import or Export of Feather file
Feather is an implementation of Apache Arrow designed to store data frames in a language agnostic manner while maintaining metadata (e.g. date classes), increasing interoperability between Python and R. Reading a feather file will produce a tibble, not a standard data.frame.

## 4  21.4     6   258   110  3.08 3.215 19.44     1     0     3     1
## 5  18.7     8   360   175  3.15 3.440 17.02     0     0     3     2 ## 6  18.1     6   225   105  2.76 3.460 20.22     1     0     3     1
head(df) ##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
 ## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 The current documentation contains this warning:
Note to users: Feather should be treated as alpha software. In particular, the file format is likely to evolve over the coming year. Do not use Feather for long-term data storage.

Chapter 93: I/O for database tables
Section 93.1: Reading Data from MySQL Databases
General
Using the package RMySQL we can easily query MySQL as well as MariaDB databases and store the result in an R dataframe:

Using limits
It is also possible to define a limit, e.g. getting only the first 100,000 rows. In order to do so, just change the SQL query regarding the desired limit. The mentioned package will consider these options. Example:

Section 93.2: Reading Data from MongoDB Databases
In order to load data from a MongoDB database into an R dataframe, use the library MongoLite:

The code connects to the server HOSTNAME as USERNAME with PASSWORD, tries to open the database TweetCollector and read the collection Tweets. The query tries to read the field i.e. column Text.
The results is a dataframe with columns as the yielded data set. In case of this example, the dataframe contains the column Text, e.g. documents$Text.
Chapter 94: I/O for geographic data (shapefiles, etc.)
See also Introduction to Geographical Maps and Input and Output
Section 94.1: Import and Export Shapefiles
map.shpWith the rgdal package it is possible to import and export shapfiles with R. The function readOGR can be used to imports shapfiles. If you want to import a file from e.g. ArcGIS the first argument dsn is the path to the folder which contains the shapefile. layer is the name of the shapefile without the file ending (just map and not ).

To export a shapefile use thewriteOGR function. The first argument is the spatial object produced in R. dsn and layer are the same as above. The obligatory 4. argument is the driver used to generate the shapefile. The function
ogrDrivers() lists all available drivers. If you want to export a shapfile to ArcGis or QGis you could use driver = "ESRI Shapefile".
writeOGR(Rmap, dsn = "path\to\the\folder\containing\the\shapefile", layer = "map",          driver = "ESRI Shapefile" )
read_shape(), which is a wrapper for rgdal::reagOGRtmap package has a very convenient function (). The
read_shape() function simplifies the process of importing a shapefile a lot. On the downside, tmap is quite heavy.

Chapter 95: I/O for raster images
See also Raster and Image Analysis and Input and Output
Section 95.1: Load a multilayer raster
The R-Logo is a multilayer raster file (red, green, blue)

The individual layers of the RasterStack object can be addressed by [[.



Chapter 96: I/O for R's binary format
Section 96.1: Rds and RData (Rda) files
rds and .Rdata (also known as .rdawrite.table.) files can be used to store R objects in a format native to R. There are multiple advantages of saving this way when contrasted with non-native storage approaches, e.g. :
It is faster to restore the data to R
It keeps R specific information encoded in the data (e.g., attributes, variable types, etc).
saveRDS/readRDS only handle a single R object. However, they are more flexible than the multi-object storage approach in that the object name of the restored object need not be the same as the object name when the object was stored.
Using an .rds file, for example, saving the iris dataset we would use:

To load it data back in:

save() and output as .RdataTo save a multiple objects we can use .
Example, to save 2 dataframes: iris and cars

To load:

Section 96.2: Enviromments
The functions save and load allow us to specify the environment where the object will be hosted:

Chapter 97: Recycling
Section 97.1: Recycling use in subsetting
Recycling can be used in a clever way to simplify code.
Subsetting
If we want to keep every third element of a vector we can do the following:

Here the logical expression was expanded to the length of the vector.
We can also perform comparisons using recycling:

Here "bar" gets recycled.

Chapter 98: Expression: parse + eval
Section 98.1: Execute code in string format
In this exemple, we want to execute code which is stored in a string format.


Chapter 99: Regular Expression Syntax in R
regexThis document introduces the basics of regular expressions as used in R. For more information about R's regular expression syntax, see ?. For a comprehensive list of regular expression operators, see this ICU guide on regular expressions.
Section 99.1: Use `grep` to find a string in a character vector

x|y means look for "x" or "y"

. is a special character in Regex. It means "match any character"

Be careful when trying to match dots!

To match a literal character, you have to escape the string with a backslash (\). However, R tries to look for escape characters when creating strings, so you actually need to escape the backslash itself (i.e. you need to double escape regular expression characters.)

If you want to match one of several characters, you can wrap those characters in brackets ([])


z0It may be useful to indicate character sequences. E.g. [0-4] will match 0, 1, 2, 3, or 4, [A-Z] will match any uppercase letter, [A-z] will match any uppercase or lowercase letter, and [A--9] will match any letter or number (i.e. all alphanumeric characters)

lower:] is short for a-z, [:upperR also has several shortcut classes that can be used in brackets. For instance, [::]
alpha:] is A-z, [:digit:] is 0-9, and [:alnum:] is A-z0is short for A-Z, [:-9. Note that these whole expressions
digitdigitmust be used inside brackets; for instance, to match a single digit, you can use [[::]] (note the double brackets). As another example, [@[::]/] will match the characters @, / or 0-9.

Brackets can also be used to negate a match with a carat (^). For instance, [^5] will match any character other than "5".


Chapter 100: Regular Expressions (regex)
regexRegular expressions (also called "regex" or "regexp") define patterns that can be matched against a string. Type ? for the official R documentation and see the Regex Docs for more details. The most important 'gotcha' that will not be learned in the SO regex/topics is that most R-regex functions need the use of paired backslashes to escape in a pattern parameter.
Section 100.1: Dierences between Perl and POSIX regex
There are two ever-so-slightly different engines of regular expressions implemented in R. The default is called
perl = TRUEPOSIX-consistent; all regex functions in R are also equipped with an option to turn on the latter type: .
Look-ahead/look-behind
perl = TRUE enables look-ahead and look-behind in regular expressions.
     "(?<=A)B" matches an appearance of the letter B only if it's preceded by A, i.e. "ABACADABRA" would be matched, but "abacadabra" and "aBacadabra" would not.
  Section 100.2: Validate a date in a "YYYYMMDD" format
It is a common practice to name files using the date as prefix in the following format: YYYYMMDD, for example:
20170101_results.csv. A date in such string format can be verified using the following regular expression:

0000-9999, months between: 01-12 and days 01-31The above expression considers dates from year: .
For example:

Note: It validates the date syntax, but we can have a wrong date with a valid syntax, for example: 20170229 (2017 it is not a leap year).

If you want to validate a date, it can be done via this user defined function: is.Date <- function(x) {return(!is.na(as.Date(as.character(x), format = '%Y%m%d')))}
Then

Section 100.3: Escaping characters in R regex patterns
\"Since both R and regex share the escape character ,", building correct patterns for grep, sub, gsub or any other function that accepts a pattern argument will often need pairing of backslashes. If you build a three item character vector in which one items has a linefeed, another a tab character and one neither, and hte desire is to turn either the linefeed or the tab into 4-spaces then a single backslash is need for the construction, but tpaired backslashes for matching:

Note that the pattern argument (which is optional if it appears first and only needs partial spelling) is the only argument to require this doubling or pairing. The replacement argument does not require the doubling of characters needing to be escaped. If you wanted all the linefeeds and 4-space occurrences replaces with tabs it would be:

Section 100.4: Validate US States postal abbreviations
The following regex includes 50 states and also Commonwealth/Territory (see www.50states.com):
regex <-
"(A[LKSZR])|(C[AOT])|(D[EC])|(F[ML])|(G[AU])|(HI)|(I[DLNA])|(K[SY])|(LA)|(M[EHDAINSOT])|(N[EVHJMYCD
])|(MP)|(O[HKR])|(P[WAR])|(RI)|(S[CD])|(T[NX])|(UT)|(V[TIA])|(W[AVIY])" For example:
> test <- c("AL", "AZ", "AR", "AJ", "AS", "DC", "FM", "GU","PW", "FL", "AJ", "AP")
> grepl(us.states.pattern, test)
 [1]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE
>
Note:
state.abbIf you want to verify only the 50 States, then we recommend to use the R-dataset:  from state, for example:

We get TRUE only for 50-States abbreviations: AL, AZ, AR, FL.
Section 100.5: Validate US phone numbers
The following regular expression:
us.phones.regex <- "^\\s*(\\+\\s*1(-?|\\s+))*[0-9]{3}\\s*-?\\s*[0-9]{3}\\s*-?\\s*[0-9]{4}$"
xxx-xxx-xxxxxxx-xxx-xx xxValidates a phone number in the form of: +1-, including optional leading/trailing blanks at the beginning/end of each group of numbers, but not in the middle, for example: +1- is not valid. The delimiter can be replaced by blanks: xxx xxx xxx or without delimiter: xxxxxxxxxx. The +1 prefix is optional.
Let's check it:
us.phones.regex <- "^\\s*(\\+\\s*1(-?|\\s+))*[0-9]{3}\\s*-?\\s*[0-9]{3}\\s*-?\\s*[0-9]{4}$"
phones.OK <- c("305-123-4567", "305 123 4567", "+1-786-123-4567",
    "+1 786 123 4567", "7861234567", "786 - 123   4567", "+ 1 786 - 123   4567")
 phones.NOK <- c("124-456-78901", "124-456-789", "124-456-78 90",     "124-45 6-7890", "12 4-456-7890") Valid cases:

Invalid cases:

Note:
 \\s Matches any space, tab or newline character
Chapter 101: Combinatorics
Section 101.1: Enumerating combinations of a specified length
Without replacement

expand.gridWith , each vector appears in a row:

For the special case of pairs, outer can be used, putting each vector into a cell:
# FUN here is used as a function executed on each resulting pair.
# in this case it's string concatenation. outer(LETTERS, LETTERS, FUN=paste0)
# Showing only first 10 rows and columns
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
 [1,] "AA" "AB" "AC" "AD" "AE" "AF" "AG" "AH" "AI" "AJ"
 [2,] "BA" "BB" "BC" "BD" "BE" "BF" "BG" "BH" "BI" "BJ"
 [3,] "CA" "CB" "CC" "CD" "CE" "CF" "CG" "CH" "CI" "CJ"
 [4,] "DA" "DB" "DC" "DD" "DE" "DF" "DG" "DH" "DI" "DJ"
 [5,] "EA" "EB" "EC" "ED" "EE" "EF" "EG" "EH" "EI" "EJ"
 [6,] "FA" "FB" "FC" "FD" "FE" "FF" "FG" "FH" "FI" "FJ"
 [7,] "GA" "GB" "GC" "GD" "GE" "GF" "GG" "GH" "GI" "GJ"
 [8,] "HA" "HB" "HC" "HD" "HE" "HF" "HG" "HH" "HI" "HJ"
 [9,] "IA" "IB" "IC" "ID" "IE" "IF" "IG" "IH" "II" "IJ"
[10,] "JA" "JB" "JC" "JD" "JE" "JF" "JG" "JH" "JI" "JJ"
Section 101.2: Counting combinations of a specified length
[1] 65780
With replacement
length(letters)^5
[1] 11881376

Chapter 102: Solving ODEs in R
Parameter Details y (named) numeric vector: the initial (state) values for the ODE system
	times	time sequence for which output is wanted; the first value of times must be the initial time
	func	name of the function that computes the values of the derivatives in the ODE system
	parms	(named) numeric vector: parameters passed to func
method	the integrator to use, by default: lsoda
Section 102.1: The Lorenz model
The Lorenz model describes the dynamics of three state variables, X, Y and Z. The model equations are:



The initial conditions are:

and a, b and c are three parameters with


Section 102.2: Lotka-Volterra or: Prey vs. predator


Section 102.3: ODEs in compiled languages - definition in R

Section 102.4: ODEs in compiled languages - definition in C


Section 102.5: ODEs in compiled languages - definition in fortran


Section 102.6: ODEs in compiled languages - a benchmark test
When you compiled and loaded the code in the three examples before (ODEs in compiled languages - definition in R, ODEs in compiled languages - definition in C and ODEs in compiled languages - definition in fortran) you are able to run a benchmark test.

Check if results are equal:

Make a benchmark (Note: On your machine the times are, of course, different):


We see clearly, that R is slow in contrast to the definition in C and fortran. For big models it's worth to translate the problem in a compiled language. The package cOde is one possibility to translate ODEs from R to C.
Chapter 103: Feature Selection in R -Removing Extraneous Features
Section 103.1: Removing features with zero or near-zero variance
A feature that has near zero variance is a good candidate for removal.
You can manually detect numerical variance below your own threshold:

Or, you can use the caret package to find near zero variance. An advantage here is that is defines near zero variance not in the numerical calculation of variance, but rather as a function of rarity:

Section 103.2: Removing features with high numbers of NA
If a feature is largely lacking data, it is a good candidate for removal:

In this case, we may want to remove NonD and Dream, which each have around 20% missing values (your cutoff may vary)
Section 103.3: Removing closely correlated features
Closely correlated features may add variance to your model, and removing one of a correlated pair might help reduce that. There are lots of ways to detect correlation. Here's one:


I'll want to look at what MPG is correlated to so strongly, and decide what to keep and what to toss. Same for cyl and disp. Alternatively, I might need to combine some strongly correlated features.

Chapter 104: Bibliography in RMD
Parameter in YAML header	Detail
toctable of contentsnumber_sectionsnumbering the sections automaticallybibliographypath to the bibliography filecslpath to the style fileSection 104.1: Specifying a bibliography and cite authors
The most important part of your RMD file is the YAML header. For writing an academic paper, I suggest to use PDF output, numbered sections and a table of content (toc).

bibliography.bibIn this example, our file  looks like this:

To cite an author mentioned in your .bib file write @ and the bibkey, e.g. Meyer2000.

rmarkdown::render("<path-to-your-RMD-file">)Rendering the RMD file via RStudio (Ctrl+Shift+K) or via console results in the following output:

Section 104.2: Inline references
If you have no *.bib file, you can use a references field in the document's YAML metadata. This should include an array of YAML-encoded references, for example:

Rendering this file results in the same output as in example "Specifying a bibliography".
Section 104.3: Citation styles
By default, pandoc will use a Chicago author-date format for citations and references. To use another style, you will need to specify a CSL 1.0 style file in the csl metadata field. In the following a often used citation style, the elsevier style, is presented (download at https://github.com/citation-style-language/styles ). The style-file has to be stored in the same directory as the RMD file OR the absolute path to the file has to be submitted.
To use another style then the default one, the following code is used:




Notice the differences to the output of example "Specifying a bibliography and cite authors"
Chapter 105: Writing functions in R
Section 105.1: Anonymous functions
applyAn anonymous function is, as the name implies, not assigned a name. This can be useful when the function is a part of a larger operation, but in itself does not take much place. One frequent use-case for anonymous functions is within the * family of Base functions.
data.frameCalculate the root mean square for each column in a :

Create a sequence of step-length one from the smallest to the largest value for each row in a matrix.

An anonymous function can also stand on its own:

is equivalent to

Section 105.2: RStudio code snippets
This is just a small hack for those who use self-defined functions often. Type "fun" RStudio IDE and hit TAB.

The result will be a skeleton of a new function.

One can easily define their own snippet template, i.e. like the one below

Global Options -> CodeThe option is Edit Snippets in the  menu.
Section 105.3: Named functions
R is full of functions, it is after all a functional programming language, but sometimes the precise function you need isn't provided in the Base resources. You could conceivably install a package containing the function, but maybe your requirements are just so specific that no pre-made function fits the bill? Then you're left with the option of making your own.
A function can be very simple, to the point of being being pretty much pointless. It doesn't even need to take an argument:
What's between the curly braces { } is the function proper. As long as you can fit everything on a single line they aren't strictly needed, but can be useful to keep things organized.
A function can be very simple, yet highly specific. This function takes as input a vector (vec in this example) and outputs the same vector with the vector's length (6 in this case) subtracted from each of the vector's elements.

lengthNotice that () is in itself a pre-supplied (i.e. Base) function. You can of course use a previously self-made function within another self-made function, as well as assign variables and perform other operations while spanning several lines:

multiplier=4 makes sure that 4 is the default value of the argument multiplier, if no value is given when calling
the function 4 is what will be used.
subtract.lengthThe above are all examples of named functions, so called simply because they have been given names (one, two,  etc.)

Chapter 106: Color schemes for graphics
Section 106.1: viridis - print and colorblind friendly palettes
Viridis (named after the chromis viridis fish) is a recently developed color scheme for the Python library matplotlib
(the video presentation by the link explains how the color scheme was developed and what are its main advantages). It is seamlessly ported to R.
There are 4 variants of color schemes: magma, plasma, inferno, and viridis (default). They are chosen with the option parameter and are coded as A, B, C, and D, correspondingly. To have an impression of the 4 color schemes, look at the maps:

(image souce)
The package can be installed from CRAN or github.
The vignette for viridis package is just brilliant.
scale_color_viridis() and scale_fill_viridisNice feature of the viridis color scheme is integration with ggplot2. Within the package two ggplot2-specific functions are defined: (). See the example below:



Section 106.2: A handy function to glimse a vector of colors
Quite often there is a need to glimpse the chosen color palette. One elegant solution is the following self defined function:

An example of use


Section 106.3: colorspace - click&drag interface for colors
choose_palette()The package colorspace provides GUI for selecting a palette. On the call of  function the following window pops-up:

When the palette is chosen, just hit OK and do not forget to store the output in a variable, e.g. pal.

The output is a function that takes n (number) as input and produces a color vector of length n according to the selected palette.

Section 106.4: Colorblind-friendly palettes
Even though colorblind people can recognize a wide range of colors, it might be hard to differentiate between certain colors.
RColorBrewer provides colorblind-friendly palettes:


The Color Universal Design from the University of Tokyo proposes the following palettes:

Section 106.5: RColorBrewer
ColorBrewer project is a very popular tool to select harmoniously matching color palettes. RColorBrewer is a port of the project for R and provides also colorblind-friendly palettes.
An example of use

RColorBrewer creates coloring options for ggplot2: scale_color_brewer and scale_fill_brewer.



Section 106.6: basic R color functions
colorsFunction () lists all the color names that are recognized by R. There is a nice PDF where one can actually see those colors.
colorRampPalette creates a function that interpolate a set of given colors to create new color palettes. This output function takes n (number) as input and produces a color vector of length n interpolating the initial colors.

rgbAny specific color may be produced with an () function:

produces green color.
Chapter 107: Hierarchical clustering with hclust
The stats package provides the hclust function to perform hierarchical clustering.
Section 107.1: Example 1 - Basic use of hclust, display of dendrogram, plot clusters
The cluster library contains the ruspini data - a standard set of data for illustrating cluster analysis.
    library(cluster)                ## to get the ruspini data     plot(ruspini, asp=1, pch=20)    ## take a look at the data

hclust expects a distance matrix, not the original data. We compute the tree using the default parameters and display it. The hang parameter lines up all of the leaves of the tree along the baseline.


Cut the tree to give four clusters and replot the data coloring the points by cluster. k is the desired number of clusters.


This clustering is a little odd. We can get a better clustering by scaling the data first.
    scaled_ruspini_hc_defaults = hclust(dist(scale(ruspini)))     srhc_def_4 = cutree(scaled_ruspini_hc_defaults,4)     plot(ruspini, pch=20, asp=1, col=srhc_def_4)

The default dissimilarity measure for comparing clusters is "complete". You can specify a different measure with the method parameter.

Section 107.2: Example 2 - hclust and outliers
With hierarchical clustering, outliers often show up as one-point clusters.
Generate three Gaussian distributions to illustrate the effect of outliers.


Build the cluster structure, split it into three cluster.

hclust found two outliers and put everything else into one big cluster. To get the "real" clusters, you may need to set k higher.


This StackOverflow post has some guidance on how to pick the number of clusters, but be aware of this behavior in hierarchical clustering.
Chapter 108: Random Forest Algorithm
RandomForest is an ensemble method for classification or regression that reduces the chance of overfitting the data. Details of the method can be found in the Wikipedia article on Random Forests. The main implementation for R is in the randomForest package, but there are other implementations. See the CRAN view on Machine Learning.
Section 108.1: Basic examples - Classification and Regression

This example tested on the training data, but illustrates that RF can make very good models.



Chapter 109: RESTful R Services
OpenCPU uses standard R packaging to develop, ship and deploy web applications.
Section 109.1: opencpu Apps
The official website contain good exemple of apps: https://www.opencpu.org/apps.html The following code is used to serve a R session:

After this code is executed, you can use URLs to access the functions of the R session. The result could be XML, html, JSON or some other defined formats.
For exemple, the previous R session can be accessed by a cURL call:
#curl uses http post method for -X POST or -d "arg=value" curl http://localhost:5936/ocpu/library/MASS/scripts/ch01.R -X POST curl http://localhost:5936/ocpu/library/stats/R/rnorm -d "n=10&mean=5"
The call is asynchronous, meaning that the R session is not blocked while waiting for the call to finish (contrary to shiny).
ocpu/tmpThe call result is kept in a temporary session stored in // An exemple of how to retrieve the temporary session:

x009f9e7630 is the name of the session.
ocpu/tmp/x009f9e7630/R/.val will return the value resulting of rnormPointing to /(5),
ocpu/tmp/x009f9e7630/R/console will return the content of the console of rnorm/(5), etc..
Chapter 110: Machine learning
Section 110.1: Creating a Random Forest model
One example of machine learning algorithms is the Random Forest alogrithm (Breiman, L. (2001). Random Forests. Machine Learning 45(5), p. 5-32). This algorithm is implemented in R according to Breiman's original Fortran implementation in the randomForest package.
Random Forest classifier objects can be created in R by preparing the class variable as factor, which is already apparent in the iris data set. Therefore we can easily create a Random Forest by:

x	a data frame holding the describing variables of the classes the classes of the individual obserbations. If this vector is factor, a classification model is created, if y
not a regression model is created.
ntree	The number of individual CART trees built do.trace	every ith step, the out-of-the-box errors overall and for each class are returned
Chapter 111: Using texreg to export models in a paper-ready way
The texreg package helps to export a model (or several models) in a neat paper-ready way. The result may be exported as HTML or .doc (MS Office Word).
Section 111.1: Printing linear regression results

The result looks like a table in a paper.

texreg::htmlreg()There are several additional handy parameters in  function. Here is a use case for the most helpful parameters.

Which result in a table like this


Chapter 112: Publishing
There are many ways of formatting R code, tables and graphs for publishing.
Section 112.1: Formatting tables
data.frameHere, "table" is meant broadly (covering , table,
Printing to plain text
Printing (as seen in the console) might suffice for a plain-text document to be viewed in monospaced font:
getwdsetwdNote: Before making the example data below, make sure you're in an empty folder you can write to. Run () and read ? if you need to change folders.

Printing delimited tables
Writing to CSV (or another common format) and then opening in a spreadsheet editor to apply finishing touches is another option:
getwdsetwdNote: Before making the example data below, make sure you're in an empty folder you can write to. Run () and read ? if you need to change folders.

Further resources
knitr::kabletables::tabularstargazer
 texreg xtable
 Section 112.2: Formatting entire documents
Sweave from the utils package allows for formatting code, prose, graphs and tables together in a LaTeX document.
Further Resources
 Knitr and RMarkdown
Chapter 113: Implement State Machine Pattern using S4 Class
Finite States Machine concepts are usually implemented under Object Oriented Programming (OOP) languages, for example using Java language, based on the State pattern defined in GOF (refers to the book: "Design Patterns").
R provides several mechanisms to simulate the OO paradigm, let's apply S4 Object System for implementing this pattern.
Section 113.1: Parsing Lines using State Machine
Let's apply the State Machine pattern for parsing lines with the specific pattern using S4 Class feature from R.
PROBLEM ENUNCIATION
Name;[Address;]PhoneWe need to parse a file where each line provides information about a person, using a delimiter (";"), but some information provided is optional, and instead of providing an empty field, it is missing. On each line we can have the following information: . Where the address information is optional, sometimes we have it and sometimes don't, for example:

The second line does not provide address information. Therefore the number of delimiters may be deferent like in this case with one delimiter and for the other lines two delimiters. Because the number of delimiters may vary, one way to atack this problem is to recognize the presence or not of a given field based on its pattern. In such case we can use a regular expression for identifying such patterns. For example:
"^([A-Z]'?\\s+)* *[A-Z]+(\\s+[A-Z]{1,2}\\.?,? +)*[A-Z]+((-|\\s+)[A-Z]+)*$"RAFAEL REAL, DAVID R. SMITH, ERNESTO PEREZ GONZALEZ, 0' CONNOR BROWN, LUIS PEREZ-MENA"^\\s[0-9]{1,4}(\\s+[A-Z]{1,2}[0-9]{1,2}[A-Z]{1,2}|[A-Z\\s0-9]+)$"Name: . For example:
, etc.
Address: . For example: 11020
87 SW 27THLE JEUNE ROAD, . For the sake of simplicity we don't include here the zipcode, city, state, but I can be included in this field or adding additional fields.
"^\\s*(\\+1(-|\\s+))*[0-9]{3}(-|\\s+)[0-9]{3}(-|\\s+)[0-9]{4}$"	 Phone: 	. For example:
305-123-4567, 305 123 4567, +1-786-123-4567.
Notes:
\"I am considering the most common pattern of US addresses and phones, it can be easy extended to consider more general situations.
In R the sign " has special meaning for character variables, therefore we need to escape it.
In order to simplify the process of defining regular expressions a good recommendation is to use the following web page: regex101.com, so you can play with it, with a given example, until you get the expected result for all possible combinations.
The idea is to identify each line field based on previously defined patterns. The State pattern define the following entities (classes) that collaborate to control the specific behavior (The State Pattern is a behavior pattern):

Let's describe each element considering the context of our problem:
handlehandle Context: Stores the context information of the parsing process, i.e. the current state and handles the entire State Machine Process. For each state, an action is executed (()), but the context delegates it, based on the state, on the action method defined for a particular state (() from State class). It defines the interface of interest to clients. Our Context class can be defined like this:
handleAttributes: state
Methods: (), ...
State: The abstract class that represents any state of the State Machine. It defines an interface for encapsulating the behavior associated with a particular state of the context. It can be defined like this:
doActionAttributes: name, pattern
Methods: (), isState (using pattern attribute verify whether the input argument belong to this state pattern or not), ...
 Concrete States (state sub-classes): Each subclass of the class State that implements a behavior associated with a state of the Context. Our sub-classes are: InitState, NameState, AddressState, PhoneState. Such classes just implements the generic method using the specific logic for such states. No additional attributes are required.
handle(), doActionNote: It is a matter of preference how to name the method that carries out the action, () or
goNext(). The method name doAction() can be the same for both classes (Stateor Context) we preferred to name
handleas () in the Context class for avoiding a confusion when defining two generic methods with the same input arguments, but different class.
PERSON CLASS
Using the S4 syntax we can define a Person class like this:

prototype, representationIt is a good recommendation to initialize the class attributes. The setClass documentation suggests using a generic method labeled as "initialize", instead of using deprecated attributes such as: .

Because the initialize method is already a standard generic method of package methods, we need to respect the original argument definition. We can verify it typing on R prompt:

It returns the entire function definition, you can see at the top who the function is defined like:

ObjectTherefore when we use setMethod we need to follow exaclty the same syntax (.).
toStringAnother existing generic method is show, it is equivalent () method from Java and it is a good idea to have a specific implementation for class domain:

toStringNote: We use the same convention as in the default () Java implementation.
Let's say we want to save the parsed information (a list of Person objects) into a dataset, then we should be able first to convert a list of objects to into something the R can transform (for example coerce the object as a list). We can define the following additional method (for more detail about this see the post)

R does not provide a sugar syntax for OO because the language was initially conceived to provide valuable functions for Statisticians. Therefore each user method requires two parts: 1) the Definition part (via setGeneric) and 2) the implementation part (via setMethod). Like in the above example. STATE CLASS
Following S4 syntax, let's define the abstract State class.

isStateEvery sub-class from State will have associated a name and pattern, but also a way to identify whether a given input belongs to this state or not (() method), and also implement the corresponding actions for this state
doAction(() method).
In order to understand the process, let's define the transition matrix for each state based on the input received:
Input/Current State	Init	Name Address Phone
Name	Name
Address	Address
Phone	Phone	Phone
End	End
row, col]=[i,jNote: The cell [] represents the destination state for the current state j, when it receives the input i.
It means that under the state Name it can receive two inputs: an address or a phone number. Another way to represents the transaction table is using the following UML State Machine diagram:

Let's implement each particular state as a sub-state of the class State
STATE SUB-CLASSES
Init State:
The initial state will be implemented via the following class:

In R to indicate a class is a sub-class of other class is using the attribute contains and indicating the class name of the parent class.
callNextMethodBecause the sub-classes just implement the generic methods, without adding additional attributes, then the show method, just call the equivalent method from the upper class (via method: ())
The initial state does not have associated a pattern, it just represents the beginning of the process, then we initialize the class with an NA value.
Now lets to implement the generic methods from the State class:

For this particular state (without pattern), the idea it just initializes the parsing process expecting the first field will be a name, otherwise it will be an error.

The doAction method provides the transition and updates the context with the information extracted. Here we are
operator. Instead, we can define get/setaccessing to context information via the @- methods, to encapsulate this
get-setprocess (as it is mandated in OO best practices: encapsulation), but that would add four more methods per without adding value for the purpose of this example.
It is a good recommendation in all doAction implementation, to add a safeguard when the input argument is not properly identified.
Name State
Here is the definition of this class definition:


We use the function grepl for verifying the input belongs to a given pattern.

Now we define the action to carry out for a given state:

Here we consider to possible transitions: one for Address state and the other one for Phone state. In all cases we update the context information:
The person information: address or phone with the input argument. The state of the process
isStateThe way to identify the state is to invoke the method: () for a particular state. We create a default specific states (addressState, phoneState) and then ask for a particular validation.
The logic for the other sub-classes (one per state) implementation is very similar.
Address State


Phone State

Here is where we add the person information into the list of persons of the context.


CONTEXT CLASS
Now the lets to explain the Context class implementation. We can define it considering the following attributes:

Where
state: The current state of the process person: The current person, it represents the information we have already parsed from the current line. persons: The list of parsed persons processed.
Note: Optionally, we can add a name to identify the context by name in case we are working with more than one parser type.

With such generic methods, we control the entire behavior of the parsing process:
() method of the current state.
handle(): Will invoke the particular doActionparseLineparseLinesas.dfaddPerson: Once we reach the end state, we need to add a person to the list of persons we have parsed.
(): Parse a single line
(): Parse multiple lines (an array of lines)
(): Extract the information from persons list into a data frame object.
Let's go on now with the corresponding implementations:

strsplithandleFirst, we split the original line in an array using the delimiter to identify each element via the R-function (), then iterate for each element as an input value for a given state. The () method returns again the context with the updated information (state, person, persons attribute).

Becuase R makes a copy of the input argument, we need to return the context (obj):

lapply() function. Then in the next invocation to lappy() function, now applies the data.frameThe attribute persons is a list of instance of S4 Person class. This something cannot be coerced to any standard type because R does not know of to treat an instance of a user defined class. The solution is to convert a Person into a list, using the as.list method previously defined. Then we can apply this function to each element of the list persons, via the 
persons.list into a data frame. Finally, the rbindfunction for converting each element of the () function is called
for adding each element converted as a new row of the data frame generated (for more detail about this see this post)

PUTTING ALL TOGETHER
Finally, lets to test the entire solution. Define the lines to parse where for the second line the address information is missing.


Finally obtain the corresponding dataset and print it:


And for some sub-state:
>show(new("PhoneState"))
PhoneState@[name='phone', pattern='^\s*(\+1(-|\s+))*[0-9]{3}(-|\s+)[0-9]{3}(-|\s+)[0-9]{4}$']
as.listFinally, test the () method:

CONCLUSION
object.setID("A1"setID(object, "A1"This example shows how to implement the State pattern, using one of the available mechanisms from R for using the OO paradigm. Nevertheless, the R OO solution is not user-friendly and differs so much from other OOP languages. You need to switch your mindset because the syntax is completely different, it reminds more the functional programming paradigm. For example instead of: ) as in Java/C#, for R you have to invoke the method in this way: ). Therefore you always have to include the object as an input argument to provide the context of the function. On the same way, there is no special this class attribute and either a "." notation for accessing methods or attributes of the given class. It is more error prompt because to refer a class or methods is done via attribute value ("Person", "isState", etc.).
if-elseif-elseSaid the above, S4 class solution, requires much more lines of codes than a traditional Java/C# languages for doing simple tasks. Anyway, the State Pattern is a good and generic solution for such kind of problems. It simplifies the process delegating the logic into a particular state. Instead of having a big  block for controlling all situations, we have smaller  blocks inside on each State sub-class implementation for implementing the action to carry out in each state.
Attachment: Here you can download the entire script.
Any suggestion is welcome.
Chapter 114: Reshape using tidyr
tidyr has two tools for reshaping data: gather (wide to long) and spread (long to wide).
See Reshaping data for other options.
Section 114.1: Reshape from long to wide format with spread()

We can "spread" the 'numbers' column, into separate columns:

Or spread the 'name' column into separate columns:

Section 114.2: Reshape from wide to long format with gather()


We can gather the columns together using 'numbers' as the key column:


Chapter 115: Modifying strings by substitution
sub and gsub are used to edit strings using patterns. See Pattern Matching and Replacement for more on related functions and Regular Expressions for how to build a pattern.
Section 115.1: Rearrange character strings using capture groups
If you want to change the order of a character strings you can use parentheses in the pattern to group parts of the string together. These groups can in the replacement argument be addresed using consecutive numbers.
The following example shows how you can reorder a vector of names of the form "surname, forename" into a vector of the form "forename surname".

If you only need the surname you could just address the first pairs of parentheses.
sub("^(.+),\\s(.+)", "\\1", strings) # [1] "Sigg"        "Holt"        "Ortega"      "De La Torre" "Perkins"  
Section 115.2: Eliminate duplicated consecutive elements
Let's say we want to eliminate duplicated subsequence element from a string (it can be more than one). For example:

and convert it into:

Using gsub, we can achieve it:

It works also for more than one different repetition, for example:

Let's explain the regular expression:
\\d1. (+): A group 1 delimited by () and finds any digit (at least one). Remember we need to use the double backslash (\\) here because for a character variable a backslash represents special escape character for literal string delimiters (\" or \'). \d\ is equivalent to: [0-9].
2. ,: A punctuation sign: , (we can include spaces or any other delimiter)
\\3.1: An identical string to the group 1, i.e.: the repeated number. If that doesn't happen, then the pattern doesn't match.
Let's try a similar situation: eliminate consecutive repeated words:

zA-Z0Then, just replace \d by \w, where \w matches any word character, including: any letter, digit or underscore. It is equivalent to [a--9_]:

Then, the above pattern includes as a particular case duplicated digits case.

Chapter 116: Non-standard evaluation and standard evaluation
Dplyr and many modern libraries in R use non-standard evaluation (NSE) for interactive programming and standard evaluation (SE) for programming1.
summarise() function use non-standard evaluation but relies on the summarise_()For instance, the  which uses standard evaluation.
The lazyeval library makes it easy to turn standard evaluation function into NSE functions.
Section 116.1: Examples with standard dplyr verbs
NSE functions should be used in interactive programming. However, when developping new functions in a new package, it's better to use SE version.
Load dplyr and lazyeval :

Filtering
NSE version

SE version (to be use when programming functions in a new package)

Summarise
NSE version

SE version
summarise_(mtcars, .dots = lazyeval::interp(~ mean(x), x = quote(disp))) summarise_(mtcars, .dots = setNames(list(lazyeval::interp(~ mean(x), x = quote(disp))), "mean_disp"))
summarise_(mtcars, .dots = list("mean_disp" = lazyeval::interp(~ mean(x), x = quote(disp))))
Mutate
NSE version

SE version


Chapter 117: Randomization
The R language is commonly used for statistical analysis. As such, it contains a robust set of options for randomization. For specific information on sampling from probability distributions, see the documentation for distribution functions.
Section 117.1: Random draws and permutations
The sample command can be used to simulate classic probability problems like drawing from an urn with and without replacement, or creating random permutations.
set.seedset.seedNote that throughout this example,  is used to ensure that the example code is reproducible. However, sample will work without explicitly calling .
Random permutation
In the simplest form, sample creates a random permutation of a vector of integers. This can be accomplished with:

When given no other arguments, sample returns a random permutation of the vector from 1 to x. This can be useful when trying to randomize the order of the rows in a data frame. This is a common task when creating randomization tables for trials, or when selecting a random subset of rows for analysis.
library(datasets) set.seed(1171) iris_rand <- iris[sample(x = 1:nrow(iris)),]
> head(iris)
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1 5.1         3.5          1.4         0.2  setosa
2 4.9         3.0          1.4         0.2  setosa
3 4.7         3.2          1.3         0.2  setosa
4 4.6         3.1          1.5         0.2  setosa
5 5.0         3.6          1.4         0.2  setosa
6 5.4         3.9          1.7         0.4  setosa
> head(iris_rand)
    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species
145          6.7         3.3          5.7         2.5  virginica
5            5.0         3.6          1.4         0.2     setosa
85           5.4         3.0          4.5         1.5 versicolor
137          6.3         3.4          5.6         2.4  virginica
128          6.1         3.0          4.9         1.8  virginica
105          6.5         3.0          5.8         2.2  virginica
Draws without Replacement
Using sample, we can also simulate drawing from a set with and without replacement. To sample without replacement (the default), you must provide sample with a set to be drawn from and the number of draws. The set to be drawn from is given as a vector.


Note that if the argument to size is the same as the length of the argument to x, you are creating a random permutation. Also note that you cannot specify a size greater than the length of x when doing sampling without replacement.

This brings us to drawing with replacement.
Draws with Replacement
To make random draws from a set with replacement, you use the replace argument to sample. By default, replace is FALSE. Setting it to TRUE means that each element of the set being drawn from may appear more than once in the final result.

Changing Draw Probabilities
By default, when you use sample, it assumes that the probability of picking each element is the same. Consider it as a basic "urn" problem. The code below is equivalent to drawing a colored marble out of an urn 20 times, writing down the color, and then putting the marble back in the urn. The urn contains one red, one blue, and one green marble, meaning that the probability of drawing each color is 1/3.

Suppose that, instead, we wanted to perform the same task, but our urn contains 2 red marbles, 1 blue marble, and 1 green marble. One option would be to change the argument we send to x to add an additional Red. However, a better choice is to use the prob argument to sample.
The prob argument accepts a vector with the probability of drawing each element. In our example above, the probability of drawing a red marble would be 1/2, while the probability of drawing a blue or a green marble would be 1/4.

Counter-intuitively, the argument given to prob does not need to sum to 1. R will always transform the given arguments into probabilities that total to 1. For instance, consider our above example of 2 Red, 1 Blue, and 1 Green.
You can achieve the same results as our previous code using those numbers:

The major restriction is that you cannot set all the probabilities to be zero, and none of them can be less than zero.
You can also utilize prob when replace is set to FALSE. In that situation, after each element is drawn, the proportions of the prob values for the remaining elements give the probability for the next draw. In this situation, you must have enough non-zero probabilities to reach the size of the sample you are drawing. For example:

In this example, Red is drawn in the first draw (as the first element). There was an 80% chance of Red being drawn, a 19% chance of Blue being drawn, and a 1% chance of Green being drawn.
For the next draw, Red is no longer in the urn. The total of the probabilities among the remaining items is 20% (19% for Blue and 1% for Green). For that draw, there is a 95% chance the item will be Blue (19/20) and a 5% chance it will be Green (1/20).
Section 117.2: Setting the seed
set.seedset.seedThe  function is used to set the random seed for all randomization functions. If you are using R to create a randomization that you want to be able to reproduce, you should use  first.

Note that parallel processing requires special treatment of the random seed, described more elsewhere.
Chapter 118: Object-Oriented Programming in R
This documentation page describes the four object systems in R and their high-level similarities and differences. Greater detail on each individual system can be found on its own topic page.
The four systems are: S3, S4, Reference Classes, and S6.
Section 118.1: S3
The S3 object system is a very simple OO system in R.
Every object has an S3 class. It can be get (got?) with the function class.

It can also be set with the function class:

It can also be set with the function attr:

An object can have many classes:

When using a generic function, R uses the first element of the class that has an available generic.
For example:

summary.bicycleBut if we now define a :

Chapter 119: Coercion
Coercion happens in R when the type of objects are changed during computation either implicitly or by using functions for explicit coercion (such as as.numeric, as.data.frame, etc.).
Section 119.1: Implicit Coercion
Coercion happens with data types in R, often implicitly, so that the data can accommodate all the values. For example,
Notice that at first, x is of type integer. But when we assigned x[2] = character as vectors in R can only hold data of single type.

Chapter 120: Standardize analyses by writing standalone R scripts
sourceIf you want to routinely apply an R analysis to a lot of separate data files, or provide a repeatable analysis method to other people, an executable R script is a user-friendly way to do so. Instead of you or your user having to call R and execute your script inside R via (.) or a function call, your user may simply call the script itself as if it was a program.
Section 120.1: The basic structure of standalone R program and how to call it
The first standalone R script
R.exeRscript.exeStandalone R scripts are not executed by the program R ( under Windows), but by a program called Rscript (), which is included in your R installation by default.
To hint at this fact, standalone R scripts start with a special line called Shebang line, which holds the following content: #!/usr/bin/env Rscript. Under Windows, an additional measure is needed, which is detailled later.
The following simple standalone R script saves a histogram under the file name "hist.png" from numbers it receives as input:

You can see several key elements of a standalone R script. In the first line, you see the Shebang line. Followed by
cat("....\n") is used to print a message to the user. Use file("stdin"that, ) whenever you want to specify "User
input on console" as a data origin. This can be used instead of a file name in several data reading functions (scan,
read.table, read.csv,...). After the user input is converted from strings to numbers, the plotting begins. There, it
png(.) and dev.offcan be seen, that plotting commands which are meant to be written to a file must be enclosed in two commands. These are in this case (). The first function depends on the desired output file format (other
jpeg(.) and pdf(.)). The second function, dev.offcommon choices being () is always required. It writes the plot to the file and ends the plotting process.
Preparing a standalone R script Linux/Mac
The standalone script's file must first be made executable. This can happen by right-clicking the file, opening
"Properties" in the opening menu and checking the "Executable" checkbox in the "Permissions" tab. Alternatively, the command

can be called in a Terminal.
Windows
For each standalone script, a batch file must be written with the following contents:

bat extension except a *.txtA batch file is a normal text file, but which has a *. extension. Create it using a text
editor like notepad (not Word) or similar and put the file name into quotation marks "FILENAME.bat") in the save dialog. To edit an existing batch file, right-click on it and select "Edit".
You have to adapt the code shown above everywhere XXX... is written:
Insert the correct folder where your R installation resides
Insert the correct name of your script and place it into the same directory as this batch file.
"C:\...\Rscript.exe"Rscript.exe program. The second part "%~dp0\XXX.R"~dp0Explanation of the elements in the code: The first part  tells Windows where to find the  tells Rscript to execute the R script you've written which resides in the same folder as the batch file (% stands for the batch file folder). Finally, %* forwards any command line arguments you give to the batch file to the R script.
If you double-click on the batch file, the R script is executed. If you drag files on the batch file, the corresponding file names are given to the R script as command line arguments.
Section 120.2: Using littler to execute R scripts
littler (pronounced little r) (cran) provides, besides other features, two possibilities to run R scripts from the command line with littler's r command (when one works with Linux or MacOS).
Installing littler From R:

The path of r is printed in the terminal, like
You could link to the 'r' binary installed in
'/home/*USER*/R/x86_64-pc-linux-gnu-library/3.4/littler/bin/r' from '/usr/local/bin' in order to use 'r' for scripting.
To be able to call r from the system's command line, a symlink is needed: ln -s /home/*USER*/R/x86_64-pc-linux-gnu-library/3.4/littler/bin/r /usr/local/bin/r Using apt-get (Debian, Ubuntu): sudo apt-get install littler
Using littler with standard .r scripts
With r from littler it is possible to execute standalone R scripts without any changes to the script. Example script:

hist.Note that no shebang is at the top of the scripts. When saved as for example r, it is directly callable from the system command:

Using littler on shebanged scripts
It is also possible to create executable R scripts with littler, with the use of the shebang

chmod +X /path/to/script.at the top of the script. The corresponding R script has to be made executable with r and is directly callable from the system terminal.
Chapter 121: Analyze tweets with R
(Optional) Every topic has a focus. Tell the readers what they will find here and let future contributors know what belongs.
Section 121.1: Download Tweets
The first think you need to do is to download tweets. You need to Setup your tweeter account. Much Information can be found in Internet on how to do it. The following two links were useful for my Setup (last checked in May 2017)
In particular I found the following two links useful (last checked in May 2017):
Link 1
Link 2
R Libraries
You will need the following R packages

Supposing you have your keys You have to run the following code

Change XXXXXXXXXXXXXXXXXXXXXX to your keys (if you have Setup your tweeter account you know which keys I mean).
Let's now suppose we want to download tweets on coffee. The following code will do it

You will get 1000 tweets on "coffee".
Section 121.2: Get text of tweets
Now we need to access the text of the tweets. So we do it in this way (we also need to clean up the tweets from special characters that for now we don't need, like emoticons with the sapply function.)

 coffee_tweets <- sapply(coffee_tweets,function(row) iconv(row, "latin1", "ASCII", sub="")) and you can check your tweets with the head function.


Chapter 122: Natural language processing
Natural language processing (NLP) is the field of computer sciences focused on retrieving information from textual input generated by human beings.
Section 122.1: Create a term frequency matrix
The simplest approach to the problem (and the most commonly used so far) is to split sentences into tokens. Simplifying, words have abstract and subjective meanings to the people using and receiving them, tokens have an objective interpretation: an ordered sequence of characters (or bytes). Once sentences are split, the order of the token is disregarded. This approach to the problem in known as bag of words model.
A term frequency is a dictionary, in which to each token is assigned a weight. In the first example, we construct a term frequency matrix from a corpus corpus (a collection of documents) with the R package tm.

In this example, we created a corpus of class Corpus defined by the package tm with two functions Corpus and VectorSource, which returns a VectorSource object from a character vector. The object tm_corpus is a list our documents with additional (and optional) metadata to describe each document.

Once we have a Corpus, we can proceed to preprocess the tokens contained in the Corpus to improve the quality of the final output (the term frequency matrix). To do this we use the tm function tm_map, which similarly to the apply family of functions, transform the documents in the corpus by applying a function to each document.
tm_corpus <- tm_map(tm_corpus, tolower) tm_corpus <- tm_map(tm_corpus, removeWords, stopwords("english")) tm_corpus <- tm_map(tm_corpus, removeNumbers) tm_corpus <- tm_map(tm_corpus, PlainTextDocument) tm_corpus <- tm_map(tm_corpus, stemDocument, language="english") tm_corpus <- tm_map(tm_corpus, stripWhitespace) tm_corpus <- tm_map(tm_corpus, PlainTextDocument)
Following these transformations, we finally create the term frequency matrix with

which gives a

that we can view by transforming it to a matrix
as.matrix(tdm)
           Docs Terms       character(0) character(0) character(0) character(0)   doctor               1            0            1            0   drug                 1            0            0            0   environ              0            1            0            1   healthcar            0            0            1            0   hospit               1            0            1            0   pollut               0            1            0            1   smog                 0            1            0            0   water                0            0            0            1
Each row represents the frequency of each token - that as you noticed have been stemmed (e.g. environment to environ) - in each document (4 documents, 4 columns).
In the previous lines, we have weighted each pair token/document with the absolute frequency (i.e. the number of instances of the token that appear in the document).
Chapter 123: R Markdown Notebooks (from RStudio)
An R Notebook is an R Markdown document with chunks that can be executed independently and interactively, with output visible immediately beneath the input. They are similar to R Markdown documents with the exception of results being displayed in the R Notebook creation/edit mode rather than in the rendered output. Note: R Notebooks are new feature of RStudio and are only available in version 1.0 or higher of RStudio.
Section 123.1: Creating a Notebook
You can create a new notebook in RStudio with the menu command File -> New File -> R Notebook
If you don't see the option for R Notebook, then you need to update your version of RStudio. For installation of RStudio follow this guide

Section 123.2: Inserting Chunks
Chunks are pieces of code that can be executed interactively. In-order to insert a new chunk by clicking on the insert button present on the notebook toolbar and select your desired code platform (R in this case, since we want to write R code). Alternatively we can use keyboard shortcuts to insert a new chunk Ctrl + Alt + I (OS X: Cmd +
Option + I)

Section 123.3: Executing Chunk Code
You can run the current chunk by clicking Run current Chunk (green play button) present on the right side of the chunk. Alternatively we can use keyboard shortcut Ctrl + Shift + Enter (OS X: Cmd + Shift + Enter)
The output from all the lines in the chunk will appear beneath the chunk.
Splitting Code into Chunks
Since a chunk produces its output beneath the chunk, when having multiple lines of code in a single chunk that produces multiples outputs it is often helpful to split into multiple chunks such that each chunk produces one output.
To do this, select the code to you want to split into a new chunk and press Ctrl + Alt + I (OS X: Cmd + Option + I)

Section 123.4: Execution Progress
When you execute code in a notebook, an indicator will appear in the gutter to show you execution progress. Lines of code which have been sent to R are marked with dark green; lines which have not yet been sent to R are marked with light green.
Executing Multiple Chunks
Running or Re-Running individual chunks by pressing Run for all the chunks present in a document can be painful. We can use Run All from the Insert menu in the toolbar to Run all the chunks present in the notebook. Keyboard shortcut is Ctrl + Alt + R (OS X: Cmd + Option + R)
There's also a option Restart R and Run All Chunks command (available in the Run menu on the editor toolbar), which gives you a fresh R session prior to running all the chunks.
We also have options like Run All Chunks Above and Run All Chunks Below to run chunks Above or Below from a selected chunk.

Section 123.5: Preview Output
Before rendering the final version of a notebook we can preview the output. Click on the Preview button on the toolbar and select the desired output format.
You can change the type of output by using the output options as "pdf_document" or "html_notebook"

Section 123.6: Saving and Sharing
Rmd is saved, an .nb.htmlWhen a notebook . file is created alongside it. This file is a self-contained HTML file which contains both a rendered copy of the notebook with all current chunk outputs (suitable for display on a website) and a copy of the notebook .Rmd itself.
More info can be found at RStudio docs

Chapter 124: Aggregating data frames
Aggregation is one of the most common uses for R. There are several ways to do so in R, which we will illustrate here.
Section 124.1: Aggregating with data.table
dt[i, j, by]list() is .().()Grouping with the data.table package is done using the syntax  Which can be read out loud as: "Take dt, subset rows using i, then calculate j, grouped by by." Within the dt statement, multiple calculations or groups should be put in a list. Since an alias for , both can be used interchangeably. In the examples below we use .
CODE:

OUTPUT:


Section 124.2: Aggregating with base R
For this, we will use the function aggregate, which can be used as follows:

The following code shows various ways of using the aggregate function.
CODE:

OUTPUT:


Section 124.3: Aggregating with dplyr
Aggregating with dplyr is easy! You can use the group_by() and the summarize() functions for this. Some examples are given below.
CODE:

OUTPUT:



Chapter 125: Data acquisition
Get data directly into an R session. One of the nice features of R is the ease of data acquisition. There are several ways data dissemination using R packages.
Section 125.1: Built-in datasets
Rhas a vast collection of built-in datasets. Usually, they are used for teaching purposes to create quick and easily reproducible examples. There is a nice web-page listing the built-in datasets: https://vincentarelbundock.github.io/Rdatasets/datasets.html
Example
Swiss Fertility and Socioeconomic Indicators (1888) Data. Let's check the difference in fertility based of rurality and domination of Catholic population.


Section 125.2: Packages to access open databases
Numerous packages are created specifically to access some databases. Using them can save a bunch of time on reading/formatting the data.
Eurostat
search_eurostatEven though eurostat package has a function (), it does not find all the relevant datasets available. This, it's more convenient to browse the code of a dataset manually at the Eurostat website: Countries Database, or Regional Database. If the automated download does not work, the data can be grabbed manually at via Bulk Download Facility.


Section 125.3: Packages to access restricted data
Human Mortality Database
Human Mortality Database is a project of the Max Planck Institute for Demographic Research that gathers and preprocess human mortality data for those countries, where more or less reliable statistics is available.

        cnt <- country[i]         exposures[[cnt]] <- readHMDweb(cnt, "Exposures_1x1", user_hmd, pass_hmd)
        # let's print the progress         paste(i,'out of',length(country)) } # this will take quite a lot of time
readHMDwebPlease note, the arguments user_hmd and pass_hmd are the login credentials at the website of Human Mortality Database. In order to access the data, one needs to create an account at http://www.mortality.org/ and provide their own credentials to the () function.


GoalKicker.com - R Notes for Professionals	450

Section 125.4: Datasets within packages
library(pkgdataThere are packages that include data or are created specifically to disseminate datasets. When such a package is loaded ()), the attached datasets become available either as R objects; or they need to be called with the () function.
Gapminder
A nice dataset on the development of countries.


World Population Prospects 2015 - United Nations Population Department
Let's see how the world has converged in male life expectancy at birth over 1950-2015.



Chapter 126: R memento by examples
This topic is meant to be a memento about the R language without any text, with self-explanatory examples.
Each example is meant to be as succint as possible.
Section 126.1: Plotting (using plot)

Result:

Section 126.2: Commonly used functions


Section 126.3: Data types
Vectors

Matrices

Dataframes





Chapter 127: Updating R version
Installing or Updating your Software will give access to new features and bug fixes. Updating your R installation can be done in a couple of ways. One Simple way is go to R website and download the latest version for your system.
Section 127.1: Installing from R Website
To get the latest release go to https://cran.r-project.org/ and download the file for your operating system. Open the downloaded file and follow the on-screen installation steps. All the settings can be left on default unless you want to change a certain behaviour.
Section 127.2: Updating from within R using installr Package
You can also update R from within R by using a handy package called installr.
Open R Console (NOT RStudio, this doesn't work from RStudio) and run the following code to install the package and initiate update.


Section 127.3: Deciding on the old packages
Once the installation is finished click the Finish button.
Now it asks if you want to copy your packages fro the older version of R to Newer version of R. Once you choose yes all the package are copied to the newer version of R.

After that you can choose if you still want to keep the old packages or delete.

You can even move your Rprofile.site from older version to keep all your customised settings.

Section 127.4: Updating Packages
You can update your installed packages once the updating of R is done.

Once its done Restart R and enjoy exploring.
Section 127.5: Check R Version
You can check R Version using the console

Credits
Thank you greatly to all the people from Stack Overflow Documentation who helped provide this content, more changes can be sent to web@petercv.com for new content to be published or updated
42Chapters 1, 2, 5, 6, 8, 17, 21, 26, 29, 39, 76, 92 and 100Aaghaz HussainChapter 20AbdouChapter 30abhiieorChapter 6AgriculturistChapter 39akrafChapters 1, 28 and 120akrunChapters 23, 52 and 54AkselAChapters 14, 33 and 105AleChapter 2AlexChapters 18, 28, 92, 94 and 115Alexandru PapiuChapter 21Alexey ShiklomanovChapters 26 and 99alexis_lazChapter 39Alihan ZıhnaChapter 77alistaireChapters 12, 13, 14, 20, 21, 33, 55 and 58alko989Chapter 41Allen WangChapter 23Andrea CirilloChapters 1, 28 and 92Andrea Ianni ௫Chapters 9 and 18Andrew BrēzaChapter 1Andrew BrykChapters 45 and 56AndreyAkinshinChapters 21 and 85AngeloChapter 55Artem KlevtsovChapters 36 and 38Arun BalakrishnanChapter 41AshishChapter 88AtishChapter 21AxemanChapters 2, 5, 21, 65 and 66baptisteChapter 28BarkleyBGChapter 9bartektartanusChapters 7, 11 and 37BatanichekChapters 3 and 18Ben BolkerChapters 22, 50, 59, 67 and 70BenjaminChapters 21, 29, 51, 68 and 100blmooreChapter 28Boysenb3rryChapter 86CarlChapter 22Carl WitthoftChapter 20Carlos CinelliChapters 18, 20, 21, 22, 27 and 92CarsonChapter 33catastrophicChapters 20 and 79CClaireChapter 47cdetermanChapter 84Christophe D.Chapters 18, 27 and 47CL.Chapter 41CMichaelChapter 21coatlessChapters 12, 13, 14 and 36
CptNemoChapter 122Craig VermeerChapter 2CropsChapter 92d.bChapters 1 and 119dash2Chapter 29DataTxChapters 18 and 70Dave2eChapter 58DaveRGPChapter 20DavidChapter 23David ArenburgChapters 18, 23, 30, 51 and 65David LealChapters 12, 100, 113 and 115David RobinsonChapters 18, 22 and 35Dawny33Chapter 20dayneChapters 4, 8, 9, 18 and 39Dean MacGregorChapters 12 and 23Derwin McGearyChapter 20DeveauPChapters 34 and 88Dirk EddelbuettelChapters 36 and 76dmailChapters 123 and 127dotancohenChapter 6DrPositronChapter 21EDiChapter 20egnhaChapter 32Eric LecoutreChapters 19, 20 and 66FisherDisinformationChapters 3, 7, 26, 34, 37 and 68FlorianChapter 124FoldedChromatinChapter 20FrankChapters 1, 2, 4, 6, 8, 12, 13, 14, 15, 16, 17, 20, 21, 23, 30, 31, 34, 41, 42, 46, 60, 66, 74, 90, 91, 92, 93, 94, 95, 96, 101, 107, 111, 112, 114 and 115G5WChapters 107 and 108Gavin SimpsonChapters 29, 39 and 68George BonebrightChapter 1Giorgos KChapter 26Glen MoutrieChapter 50GregorChapters 6, 17, 18, 21 and 39HackChapters 9, 18, 33, 41, 49, 54, 56, 64, 69, 72 and 75Hairizuan NoorazmanChapter 9herbamanChapters 20 and 120highBandWidthChapter 22ikashnitskyChapters 21, 28, 35, 48, 74, 76, 94, 96, 105, 106, 111 and 125JaapChapters 8, 11, 23, 28, 41, 66 and 92James ElderfieldChapter 40jameselmoreChapter 38JavChapter 50jcbChapter 68JeffChapter 79Jeromy AnglimChapters 92 and 114JHowIXChapters 81 and 93josliberChapters 19, 31 and 32JoyChapter 103JulioSergioChapter 29JvHChapter 47J_FChapters 102 and 104
K.DaiseyChapters 20, 26, 38 and 62kaksatChapters 18, 41, 92 and 105Karolis KoncevičiusChapters 27, 37, 76, 83, 87 and 101Karsten W.Chapter 11kartoffelsalatChapter 1Kay BrodersenChapter 21kdopenChapters 1 and 6Ken S.Chapter 92kitman0804Chapters 66 and 92kneijenhuijsChapters 8, 12, 29, 41 and 70L.V.RaoChapters 1, 2, 13, 17, 25, 27, 39 and 46leogamaChapter 23lmckeoghChapters 2 and 41lmoChapters 11, 38, 57, 60, 66 and 92lokiChapters 28, 38, 48, 71, 95 and 110LovyChapters 1, 43 and 126Mallick HossainChapter 23Marcin KosińskiChapter 72MarioChapter 96maRtinChapters 26 and 48Martin SchmelzerChapters 61 and 83Maximilian KohlChapter 89MichaelChiricoChapters 12, 13, 14 and 100micstrChapter 23MihaChapters 37, 49 and 92mripChapter 39munirbeChapter 106NanamiChapter 85narendraChapters 28, 41 and 55Nathan WerthChapters 23 and 29nrussellChapters 10, 51 and 68NWatersChapter 30omarChapters 29 and 68oshunChapter 23PACChapters 20, 100 and 116Pankaj SharmaChapters 80 and 82ParfaitChapter 92Peter HumburgChapters 9 and 41Pierre LafortuneChapter 22polkaChapters 1, 21 and 92Pragyaditya DasChapter 41PsidomChapter 31QaswedChapter 5Rahul SainiChapter 1Raj PadmanabhanChapter 41RappsterChapter 73rcortyChapter 118RetractedAndRetiredChapter 1RobertChapters 3, 11, 22, 26, 29, 33, 51 and 77RobertMcChapter 44Robin GertenbachChapter 11Roel HogervorstChapters 92 and 114russellpierceChapters 1, 3, 10, 38, 40, 47 and 96
Sam FirkeChapters 20 and 21SathishChapter 5scoaChapters 18 and 55seasmithChapters 21, 66 and 68Shawn MehanChapter 21smciChapter 11SommerEngineeringChapters 30 and 93Sowmya S. ManianChapter 42SpacedmanChapter 21stanekamChapter 23StedyChapter 76Steve_CorrinChapters 1, 8, 13, 15, 16, 17, 23, 29, 30, 45, 66, 78 and 90SumedhChapters 18, 23 and 77Sun BeeChapter 24SymbolixAUChapters 12, 14, 33, 41, 47, 50, 53, 60 and 114symbolrushChapters 26 and 33takjeChapters 11 and 60Tal GaliliChapter 107TARehmanChapter 117tenCupMaximumChapters 46 and 63TensibaiChapters 3, 19 and 30theArunChapters 1, 12, 28, 66 and 77thelatemailChapters 14 and 19ThomasChapters 6, 18, 76 and 92Tim CokerChapter 54TriskalJMChapter 21tuomastikChapter 47UmbertoChapter 121user2100721Chapters 18, 21 and 114user890739Chapter 41USER_1Chapters 22 and 97UweChapter 6VeddaChapter 22WAFChapter 22whileChapter 1YCRChapters 59, 61, 98 and 109Yun ChingChapter 28zacdavChapters 68 and 96zeliteChapter 20zx8754Chapters 41 and 96
You may also like

















GoalKicker.com - R Notes for Professionals	1

GoalKicker.com - R Notes for Professionals	1



GoalKicker.com - R Notes for Professionals	1





GoalKicker.com - R Notes for Professionals	1

GoalKicker.com - R Notes for Professionals	1

















